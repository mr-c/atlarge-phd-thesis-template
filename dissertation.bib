@article{suriadi_event_2017,
	title = {Event log imperfection patterns for process mining: {Towards} a systematic approach to cleaning event logs},
	volume = {64},
	issn = {0306-4379},
	shorttitle = {Event log imperfection patterns for process mining},
	url = {https://www.sciencedirect.com/science/article/pii/S0306437915301344},
	doi = {10.1016/j.is.2016.07.011},
	abstract = {Process-oriented data mining (process mining) uses algorithms and data (in the form of event logs) to construct models that aim to provide insights into organisational processes. The quality of the data (both form and content) presented to the modeling algorithms is critical to the success of the process mining exercise. Cleaning event logs to address quality issues prior to conducting a process mining analysis is a necessary, but generally tedious and ad hoc task. In this paper we describe a set of data quality issues, distilled from our experiences in conducting process mining analyses, commonly found in process mining event logs or encountered while preparing event logs from raw data sources. We show that patterns are used in a variety of domains as a means for describing commonly encountered problems and solutions. The main contributions of this article are in showing that a patterns-based approach is applicable to documenting commonly encountered event log quality issues, the formulation of a set of components for describing event log quality issues as patterns, and the description of a collection of 11 event log imperfection patterns distilled from our experiences in preparing event logs. We postulate that a systematic approach to using such a pattern repository to identify and repair event log quality issues benefits both the process of preparing an event log and the quality of the resulting event log. The relevance of the pattern-based approach is illustrated via application of the patterns in a case study and through an evaluation by researchers and practitioners in the field.},
	urldate = {2023-12-15},
	journal = {Information Systems},
	author = {Suriadi, S. and Andrews, R. and ter Hofstede, A. H. M. and Wynn, M. T.},
	month = mar,
	year = {2017},
	keywords = {Data mining, Data quality, Event log preparation, Event log quality, Patterns, Process mining, Systematic data pre-processing},
	pages = {132--150},
}

@techreport{russell_workflow_data_2004,
	address = {Brisbane},
	type = {{QUT} {Technical} report},
	title = {Workflow {Data} {Patterns}},
	abstract = {Workflow systems seek to provide an implementation vehicle for complex, recurring business processes. Notwithstanding this common objective, there are a
variety of distinct features offered by commercial workflow management systems.
These differences result in significant variations in the ability of distinct tools to represent and implement the plethora of requirements that may arise in contemporary business processes. Many of these requirements recur quite frequently during the requirements analysis activity for workflow systems and abstractions
of these requirements serve as a useful means of identifying the key components of workflow languages.
Previous work has identified a number of workflow control patterns which characterise the range of control flow constructs that might be encountered when modelling and analysing workflow. In this paper, we describe a series of workflow data patterns that aim to capture the various ways in which data is represented and utilised in workflows. By delineating these patterns in a form that is independent of specific workflow technologies and modelling languages, we are able to
provide a comprehensive treatment of the workflow data perspective and we subsequently use these patterns as the basis for a detailed comparison of a number
of commercially available workflow management systems and business process modelling languages.},
	number = {FIT-TR-2004-01},
	institution = {Queensland University of Technology},
	author = {Russell, Nick and ter Hofstede, Arthur H. M. and van der Aalst, Wil M. P.},
	year = {2004},
}

@techreport{russell_exception_2006,
	title = {Exception {Handling} {Patterns} in {Process}-{Aware} {Information} {Systems}.},
	url = {http://www.workflowpatterns.com/documentation/documents/BPM-06-04.pdf},
	abstract = {This paper presents a classification framework for exception handling in process-aware information systems (PAIS) based on patterns. This framework is independent of specific modelling approaches or technologies and as such provides an objective means of delineating the exception-handling capabilities of specific workflow and process-aware information systems. It is subsequently used to assess the level of exceptions support provided by eight commercial workflow systems and business process modelling and execution languages. On the basis of these investigations, we propose a graphical, tool-independent language for defining exception handling strategies in process-aware information systems.},
	number = {BPM-06-04},
	institution = {BPMcenter.org},
	author = {Russell, Nick and van der Aalst, Wil M. P. and ter Hofstede, Arthur H. M.},
	year = {2006},
}

@techreport{russell_workflow_resource_2004,
	address = {Eindhoven},
	title = {Workflow {Resource} {Patterns}},
	url = {http://www.workflowpatterns.com/documentation/documents/Resource%20Patterns%20BETA%20TR.pdf},
	abstract = {Workflow systems seek to provide an implementation vehicle for complex, re-
curring business processes. Notwithstanding this common objective, there are a
variety of distinct features offered by commercial workflow management systems.
These differences result in significant variations in the ability of distinct tools to
represent and implement the plethora of requirements that may arise in contem-
porary business processes. Many of these requirements recur quite frequently
during the requirements analysis activity for workflow systems and abstractions
of these requirements serve as a useful means of identifying the key components
of workflow languages.
Previous work has identified a number of Workflow Control Patterns and
Workflow Data Patterns, which characterize the range of control flow and data
constructs that might be encountered when modelling and analysing workflows.
In this paper, we describe a series of Workflow Resource Patterns that aim to cap-
ture the various ways in which resources are represented and utilized in workflows.
By delineating these Patterns in a form that is independent of specific workflow
technologies and modelling languages, we are able to provide a comprehensive
treatment of the resource perspective and we subsequently use these Patterns
as the basis for a detailed comparison of a number of commercially available
workflow management systems and business process modelling languages},
	number = {WP 127},
	institution = {Eindhoven University of Technology},
	author = {Russell, Nick and ter Hofstede, A.H.M. and Edmond, David},
	year = {2004},
}

@article{van_der_aalst_workflow_2003,
	title = {Workflow {Patterns}},
	volume = {14},
	issn = {1573-7578},
	url = {https://doi.org/10.1023/A:1022883727209},
	doi = {10.1023/A:1022883727209},
	abstract = {Differences in features supported by the various contemporary commercial workflow management systems point to different insights of suitability and different levels of expressive power. The challenge, which we undertake in this paper, is to systematically address workflow requirements, from basic to complex. Many of the more complex requirements identified, recur quite frequently in the analysis phases of workflow projects, however their implementation is uncertain in current products. Requirements for workflow languages are indicated through workflow patterns. In this context, patterns address business requirements in an imperative workflow style expression, but are removed from specific workflow languages. The paper describes a number of workflow patterns addressing what we believe identify comprehensive workflow functionality. These patterns provide the basis for an in-depth comparison of a number of commercially availablework flow management systems. As such, this paper can be seen as the academic response to evaluations made by prestigious consulting companies. Typically, these evaluations hardly consider the workflow modeling language and routing capabilities, and focus more on the purely technical and commercial aspects.},
	number = {1},
	journal = {Distributed and Parallel Databases},
	author = {van der Aalst, W.M.P. and ter Hofstede, A.H.M. and Kiepuszewski, B. and Barros, A.P.},
	month = jul,
	year = {2003},
	pages = {5--51},
}

@misc{leo2023recording,
      title={Recording provenance of workflow runs with RO-Crate},
      author={Simone Leo and Michael R. Crusoe and Laura Rodríguez-Navas and Raül Sirvent and Alexander Kanitz and Paul De Geest and Rudolf Wittner and Luca Pireddu and Daniel Garijo and José M. Fernández and Iacopo Colonnelli and Matej Gallo and Tazro Ohta and Hirotaka Suetake and Salvador Capella-Gutierrez and Renske de Wit and Bruno P. Kinoshita and Stian Soiland-Reyes},
      year={2023},
      eprint={2312.07852},
      archivePrefix={arXiv},
      primaryClass={cs.DL}
}

@article{crusoe-methods-2022,
	title = {Methods included: {Standardizing} computational reuse and portability with the {Common} {Workflow} {Language}},
	volume = {65},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/3486897},
	doi = {10.1145/3486897},
	abstract = {Standardizing computational reuse and portability with the Common Workflow Language.},
	number = {6},
	urldate = {2022-05-21},
	journal = {Communications of the {ACM}},
	author = {Crusoe, Michael R. and Abeln, Sanne and Iosup, Alexandru and Amstutz, Peter and Chilton, John and Tijanić, Nebojša and Ménager, Hervé and Soiland-Reyes, Stian and Gavrilović, Bogdan and Goble, Carole and Community, The {CWL}},
	month = may,
	year = {2022},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	pages = {54--63},
	annote = {Comment: 8 pages, 3 figures. For the {LaTex} source code of this paper, see https://github.com/mr-c/cwl\_methods\_included},
}

@article{kotliar_cwl-airflow_2019,
	title = {{CWL}-{Airflow}: a lightweight pipeline manager supporting {Common} {Workflow} {Language}},
	volume = {8},
	shorttitle = {{CWL}-{Airflow}},
	url = {http://academic.oup.com/gigascience/article/8/7/giz084/5535758},
	doi = {10.1093/gigascience/giz084},
	abstract = {AbstractBackground. Massive growth in the amount of research data and computational analysis has led to increased use of pipeline managers in biomedical computa},
	language = {en},
	number = {7},
	urldate = {2020-12-06},
	journal = {GigaScience},
	author = {Kotliar, Michael and Kartashov, Andrey V. and Barski, Artem},
	month = jul,
	year = {2019},
	note = {Publisher: Oxford Academic},
}

@inproceedings{crusoe_capturing_2018,
	title = {Capturing {Interoperable} {Reproducible} {Workflows} {With} {Common} {Workflow} {Language}},
	url = {http://doi.org/10.5281/ZENODO.1312622},
	doi = {10.5281/ZENODO.1312622},
	booktitle = {Zenodo},
	author = {Crusoe, Michael R. and Soiland-Reyes, Stian and Khan, Farah Zaib and Sinnott, Richard O. and Lonie, Andrew and Crusoe, Michael R. and Goble, Carole},
	year = {2018},
	note = {00000},
}

@article{crusoe_channeling_2016,
	title = {Channeling {Community} {Contributions} to {Scientific} {Software}: {A} {Sprint} {Experience}},
	volume = {4},
	url = {http://dx.doi.org/10.5334/jors.96},
	doi = {10.5334/jors.96},
	journal = {Journal of Open Research Software},
	author = {Crusoe, Michael R. and Brown, C. Titus},
	month = jul,
	year = {2016},
	note = {00001},
}

@article{de_la_garza_desktop_2016,
	title = {From the desktop to the grid: scalable bioinformatics via workflow conversion},
	volume = {17},
	issn = {1471-2105},
	shorttitle = {From the desktop to the grid},
	url = {https://doi.org/10.1186/s12859-016-0978-9},
	doi = {10.1186/s12859-016-0978-9},
	abstract = {Reproducibility is one of the tenets of the scientific method. Scientific experiments often comprise complex data flows, selection of adequate parameters, and analysis and visualization of intermediate and end results. Breaking down the complexity of such experiments into the joint collaboration of small, repeatable, well defined tasks, each with well defined inputs, parameters, and outputs, offers the immediate benefit of identifying bottlenecks, pinpoint sections which could benefit from parallelization, among others. Workflows rest upon the notion of splitting complex work into the joint effort of several manageable tasks.},
	number = {1},
	urldate = {2020-11-11},
	journal = {BMC Bioinformatics},
	author = {de la Garza, Luis and Veit, Johannes and Szolek, Andras and Röttig, Marc and Aiche, Stephan and Gesing, Sandra and Reinert, Knut and Kohlbacher, Oliver},
	month = mar,
	year = {2016},
	pages = {127},
}

@article{colonnelli_streamflow_2020,
	title = {{StreamFlow}: cross-breeding cloud with {HPC}},
	issn = {2168-6750},
	shorttitle = {{StreamFlow}},
	doi = {10.1109/TETC.2020.3019202},
	abstract = {Workflows are among the most commonly used tools in a variety of execution environments. Many of them target a specific environment; few of them make it possible to execute an entire workflow in different environments, e.g. Kubernetes and batch clusters. We present a novel approach to workflow execution, called StreamFlow, that complements the workflow graph with the declarative description of potentially complex execution environments, and that makes it possible the execution onto multiple sites not sharing a common data space. StreamFlow is then exemplified on a novel bioinformatics pipeline for single cell transcriptomic data analysis workflow.},
	journal = {IEEE Transactions on Emerging Topics in Computing},
	author = {Colonnelli, Iacopo and Cantalupo, Barbara and Merelli, Ivan and Aldinucci, Marco},
	year = {2020},
	note = {Conference Name: IEEE Transactions on Emerging Topics in Computing},
	keywords = {C.1.3, Cloud computing, Computer Science - Distributed, Parallel, and Cluster Computing, Computer architecture, Containers, D.1.3, D.3.2, DSL, Pipelines, Task analysis, Tools},
	pages = {1--1},
}

@article{oliver_workflow_2019,
	title = {Workflow {Automation} for {Cycling} {Systems}: {The} {Cylc} {Workflow} {Engine}},
	issn = {1521-9615},
	shorttitle = {Workflow {Automation} for {Cycling} {Systems}},
	doi = {10.1109/MCSE.2019.2906593},
	abstract = {Complex cycling workflows are fundamental to Numerical Weather Prediction (NWP) and related environmental forecasting systems. Large numbers of jobs are executed at regular intervals to process new data and generate new forecasts. Dependence between these forecast cycles creates a single never-ending workflow, but NWP workflow schedulers have traditionally ignored this-at the cost of efficiency when running "off the clock"-by enforcing a simpler non-overlapping sequence of single-cycle workflows. Cylc ("Silk") (1)(2)(3) is designed to manage infinite cycling workflows efficiently even after delays in real-time operation, or in historical runs, when cycles can typically interleave for much-increased throughput. Cylc is not actually specialized to environmental forecasting, however, and cycling workflows may also be useful in other contexts. In this article we describe the origins and major features of Cylc, future plans for the project, and our experience of Open Source development and community engagement.},
	journal = {Computing in Science Engineering},
	author = {Oliver, H. and Shin, M. and Matthews, D. and Sanders, O. and Bartholomew, S. and Clark, A. and Fitzpatrick, B. and Haren, R. v and Hut, R. and Drost, N.},
	year = {2019},
	note = {00000},
	keywords = {Computational modeling, Delays, Predictive models, Real-time systems, Task analysis, Weather forecasting},
	pages = {1--1},
}

@article{lau_cancer_2017,
	title = {The {Cancer} {Genomics} {Cloud}: {Collaborative}, {Reproducible}, and {Democratized}—{A} {New} {Paradigm} in {Large}-{Scale} {Computational} {Research}},
	volume = {77},
	issn = {0008-5472},
	url = {http://dx.doi.org/10.1158/0008-5472.can-17-0387},
	doi = {10.1158/0008-5472.can-17-0387},
	abstract = {The Seven Bridges Cancer Genomics Cloud (CGC; www.cancergenomicscloud.org) enables researchers to rapidly access and collaborate on massive public cancer genomic datasets, including The Cancer Genome Atlas. It provides secure on- demand access to data, analysis tools, and computing resources. Researchers from diverse backgrounds can easily visualize, query, and explore cancer genomic datasets visually or programmatically. Data of interest can be immediately analyzed in the cloud using more than 200 preinstalled, curated bioinformatics tools and workflows. Researchers can also extend the functionality of the platform by adding their own data and tools via an intuitive software development kit. By colocalizing these resources in the cloud, the CGC enables scalable, reproducible analyses. Researchers worldwide can use the CGC to investigate key questions in cancer genomics.},
	number = {21},
	journal = {Cancer Research},
	author = {Lau, Jessica W. and Lehnert, Erik and Sethi, Anurag and Malhotra, Raunaq and Kaushik, Gaurav and Onder, Zeynep and Groves-Kirkby, Nick and Mihajlovic, Aleksandar and DiGiovanna, Jack and Srdic, Mladen and Bajcic, Dragan and Radenkovic, Jelena and Mladenovic, Vladimir and Krstanovic, Damir and Arsenijevic, Vladan and Klisic, Djordje and Mitrovic, Milan and Bogicevic, Igor and Kural, Deniz and Davis-Dusenbery, Brandi},
	month = oct,
	year = {2017},
	pages = {e3--e6},
}

@techreport{noauthor_ieee_2020,
	title = {{IEEE} {Standard} for {Bioinformatics} {Analyses} {Generated} by {High}-{Throughput} {Sequencing} ({HTS}) to {Facilitate} {Communication}},
	url = {https://doi.org/10.1109/IEEESTD.2020.9094416},
	abstract = {A major goal of this standard is to improve communication of bioinformatics protocols and data in order to facilitate bioinformatics workflow related exchange and communication between regulatory agencies, pharmaceutical companies, bioinformatics platform providers and researchers. A detailed communication helps ensure responsibility, reproducibility, verify bioinformatics protocol, track provenance information and promote interoperability. In addition, this standard also defines the assurance program for evaluating and certifying products against those requirements.},
	number = {2791-2020},
	month = may,
	year = {2020},
	note = {Conference Name: IEEE Std 2791-2020},
	keywords = {Bioinformatics, HTS, IEEE 2791, IEEE Standards, IEEE standard, IEEE standards, MPS, NGS, Next generation networking, Sequential analysis, Throughput, analysis, bioinformatics, bioinformatics platform providers, bioinformatics protocol, detailed communication, genomics, high-throughput sequencing, interoperability, massively parallel sequencing, next generation sequencing, open systems, pharmaceutical companies, pharmaceutical industry, pipeline, provenance information tracking, regulatory, regulatory agencies, researchers, workflow},
	pages = {1--16},
}

@techreport{van_wezenbeek_nationaal_2017,
	title = {Nationaal plan open science},
	url = {http://dx.doi.org/10.4233/uuid:9e9fa82e-06c1-4d0d-9e20-5620259a6c65},
	abstract = {This National Plan Open Science sets out what the Dutch parties involved in creating this Plan are already doing and what they plan to do to grasp the opportunities and at the same time make science even more accessible to others. A major boost is required if these initiatives are to be coordinated and the great ambition realised. That is why this Plan lists the ambitions and provides details of the parties intending to take action, as well as the timeframes within which they believe they can realise their objectives. The key ambitions are: (1) Full open access to publications in 2020: Continue the Dutch approach for all Dutch research organisations and research areas whilst recognising their differences and similarities; (2) To make research data optimally suited for reuse: To set clear and agreed technical and policy-related preconditions to facilitate reuse of research data, including provision of the necessary expertise and support; (3) Recognition and rewards: To examine together how open science can be an element of the evaluation and reward system for researchers, research groups and research proposals; and (4) To promote and support: To establish a 'clearing house' for all information regarding all available research support. With the ambitions set out in this plan the Netherlands is responding to the Amsterdam Call for Action on Open Science published in 2016, the conclusions of the Competitiveness Council in May 2016, and to the in the letter to Parliament concerning open science confirmed question by the State Secretary for Education, Culture and Science (January 2017). Open access to publications and optimal reuse of research data are becoming the standard for all knowledge institutes and research areas. The motto here is as open as possible, as closed as necessary.},
	institution = {Ministerie van Onderwijs, Cultuur en Wetenschap},
	author = {van Wezenbeek, W. J. S. M. and Touwen, H. J. J. and Versteeg, A. M. C. and van Wesenbeeck, A. J. M.},
	month = feb,
	year = {2017},
	doi = {10.4233/uuid:9e9fa82e-06c1-4d0d-9e20-5620259a6c65},
}

@article{lee_rrnaselector_2011,
	title = {{rRNASelector}: {A} computer program for selecting ribosomal {RNA} encoding sequences from metagenomic and metatranscriptomic shotgun libraries},
	volume = {49},
	issn = {1976-3794},
	shorttitle = {{rRNASelector}},
	url = {https://doi.org/10.1007/s12275-011-1213-z},
	doi = {10.1007/s12275-011-1213-z},
	abstract = {Metagenomic and metatranscriptomic shotgun sequencing techniques are gaining popularity as more cost-effective next-generation sequencing technologies become commercially available. The initial stage of bioinfor-matic analysis generally involves the identification of phylogenetic markers such as ribosomal RNA genes. The sequencing reads that do not code for rRNA can then be used for protein-based analysis. Hidden Markov model is a well-known method for pattern recognition. Hidden Markov models that are trained on well-curated rRNA sequence databases have been successfully used to identify DNA sequence coding for rRNAs in pro-karyotes. Here, we introduce rRNASelector, which is a computer program for selecting rRNA genes from massive metagenomic and metatranscriptomic sequences using hidden Markov models. The program successfully identified prokaryotic 5S, 26S, and 23S rRNA genes from Roche 454 FLX Titanium-based metagenomic and metatranscriptomic libraries. The rRNASelector program is available at http://sw.ezbiocloud.net/rrnaselector.},
	language = {en},
	number = {4},
	urldate = {2021-07-24},
	journal = {The Journal of Microbiology},
	author = {Lee, Jae-Hak and Yi, Hana and Chun, Jongsik},
	month = sep,
	year = {2011},
	pages = {689},
}

@misc{guarracino_covid-19_2020,
	title = {{COVID}-19 {PubSeq}: {Public} {SARS}-{CoV}-2 {Sequence} {Resource}},
	url = {https://bcc2020.sched.com/event/coLw/covid-19-pubseq-public-sars-cov-2-sequence-resource},
	abstract = {As part of the COVID-19 Virtual Biohackathon 2020 we formed a working group to create a
COVID-19 Public Sequence Resource (COVID-19 PubSeq) for SARS-CoV-2 virus sequences. Our goal
was to create a repository that had a low barrier to entry for uploading and analyzing sequence
data. We followed FAIR data practices: data are published with public domain (CC0) or creative
commons 4.0 (CC-BY-4.0) license, structured metadata is validated against standard ontologies,
and, most importantly, reproducible workflows are executed after the upload in order to provide
up-to-date results rapidly and in standardized data formats.

Existing data repositories for viral data include GISAID, EBI ENA and NCBI. These repositories allow
for free sharing data, but do not enforce strict quality control on submitted data or metadata, and
do not add value in terms of running additional analysis. In addition, some databases have a
restricted license which prevents data from being used in online web services and on-the-fly
computation, hindering research.

We created a prototype sequence resource within one week by leveraging existing technologies,
such as the Arvados Cloud platform (http://arvados.org), Common Workflow Language (CWL)
(http://commonwl.org), and the many free and open source software packages that are available
for bioinformatics. Pipelines developed by several teams were combined into an omnibus
pangenome analysis workflow. Computing resources for this project were generously donated by
Amazon Web Services.},
	author = {Guarracino, Andrea and Amstutz, Peter and Liener, Thomas and Crusoe, Michael and Novak, Adam and Garrison, Erik and Ohta, Tazro and Munyoki, Bonface and Welter, Danielle and Wait Zaranek, Sarah and Wait Zaranek, Alexander (Sasha) and Prins, Pjotr},
	month = jul,
	year = {2020},
}

@techreport{kunze_bagit_2018,
	type = {{RFC}},
	title = {The {BagIt} {File} {Packaging} {Format} ({V1}.0)},
	url = {https://www.rfc-editor.org/info/rfc8493},
	number = {8493},
	institution = {RFC Editor},
	author = {Kunze, J. and Littman, J. and Madden, E. and Scancella, J. and Adams, C.},
	month = oct,
	year = {2018},
	note = {ISSN: 2070-1721},
}

@article{gryk_workflows_2017,
	title = {Workflows and {Provenance}: {Toward} {Information} {Science} {Solutions} for the {Natural} {Sciences}},
	volume = {65},
	issn = {0024-2594},
	shorttitle = {Workflows and {Provenance}},
	url = {https://muse.jhu.edu/article/669457},
	doi = {10.1353/lib.2017.0018},
	abstract = {The era of big data and ubiquitous computation has brought with it concerns about ensuring reproducibility in this new research environment. It is easy to assume computational methods self-document by their very nature of being exact, deterministic processes. However, similar to laboratory experiments, ensuring reproducibility in the computational realm requires the documentation of both the protocols used (workflows) as well as a detailed description of the computational environment: algorithms, implementations, software environments as well as the data ingested and execution logs of the computation. These two aspects of computational reproducibility (workflows and execution details) are discussed in the context of biomolecular Nuclear Magnetic Resonance spectroscopy (bioNMR) as well as the PRIMAD model for computational reproducibility.},
	number = {4},
	urldate = {2021-04-16},
	journal = {Library trends},
	author = {Gryk, Michael R. and Ludäscher, Bertram},
	year = {2017},
	pmid = {29375158},
	pmcid = {PMC5779102},
	pages = {555--562},
}

@article{cuevas-vicenttin_scientific_2012,
	title = {Scientific {Workflows} and {Provenance}: {Introduction} and {Research} {Opportunities}},
	volume = {12},
	issn = {1610-1995},
	shorttitle = {Scientific {Workflows} and {Provenance}},
	url = {https://doi.org/10.1007/s13222-012-0100-z},
	doi = {10.1007/s13222-012-0100-z},
	abstract = {Scientific workflows are becoming increasingly popular for compute-intensive and data-intensive scientific applications. The vision and promise of scientific workflows includes rapid, easy workflow design, reuse, scalable execution, and other advantages, e.g., to facilitate “reproducible science” through provenance (e.g., data lineage) support. However, as described in the paper, important research challenges remain. While the database community has studied (business) workflow technologies extensively in the past, most current work in scientific workflows seems to be done outside of the database community, e.g., by practitioners and researchers in the computational sciences and eScience. We provide a brief introduction to scientific workflows and provenance, and identify areas and problems that suggest new opportunities for database research.},
	language = {en},
	number = {3},
	urldate = {2020-07-03},
	journal = {Datenbank-Spektrum},
	author = {Cuevas-Vicenttín, Víctor and Dey, Saumen and Köhler, Sven and Riddle, Sean and Ludäscher, Bertram},
	month = nov,
	year = {2012},
	pages = {193--203},
}

@article{crusoe_walking_2015,
	title = {Walking the talk: adopting and adapting sustainable scientiﬁc software development processes in a small biology lab},
	url = {http://doi.org/10.6084/m9.figshare.791567},
	doi = {10.6084/m9.figshare.791567},
	author = {Crusoe, Michael R.},
	year = {2015},
}

@article{crusoe_walking_2016,
	title = {Walking the {Talk}: {Adopting} and {Adapting} {Sustainable} {Scientific} {Software} {Development} processes in a {Small} {Biology} {Lab}},
	volume = {4},
	url = {http://dx.doi.org/10.5334/jors.35},
	doi = {10.5334/jors.35},
	journal = {Journal of Open Research Software},
	author = {Crusoe, Michael R. and Brown, C. Titus},
	month = nov,
	year = {2016},
}

@techreport{jiang_tr-19-01_2019,
	type = {Technical {Report}},
	title = {{TR}-19-01: {A} {Cloud}-{Agnostic} {Framework} for {Geo}-{Distributed} {Data}-{Intensive} {Applications}},
	url = {https://renci.org/technical-reports/tr-19-01/},
	abstract = {As the demand for Cloud computing trends up, valuable datasets are stored in the Cloud across various geographical regions and Cloud platforms and providers. The distribution of data across cloud providers imposes three major challenges for data-driven analysis and applications: the heterogeneity of cloud resources across clouds; low network throughput over the widearea network; and high monetary cost resulting from moving data in/out cloud regions. In this work we propose a cloudagnostic framework named PIVOT that builds on open-source technologies and abstraction principles to create the illusion of one single computer for applications and users. We have deployed a prototype across AWS and GCP and investigated its effectiveness against synthetic workloads. Using a combination of advanced middleware techniques and data-locality and cost aware scheduling strategies we show that PIVOT is able to achieve up to 4x improvement in network throughput and reduce {\textgreater} 60\% monetary cost.},
	language = {en},
	number = {TR-19-01},
	institution = {RENCI, University of North Carolina at Chapel Hill},
	author = {Jiang, Fan and Castillo, Claris and Ahalt, Stan},
	month = feb,
	year = {2019},
	pages = {10},
}

@techreport{jiang_pivot_2019,
	type = {Technical {Report}},
	title = {{PIVOT}: {Cost}-{Aware} {Scheduling} of {Data}-{Intensive} {Applications} in a {Cloud}-{Agnostic} {System}},
	url = {https://renci.org/technical-reports/tr-19-02/},
	abstract = {We have witnessed a surge in big data applications being hosted by assorted cloud vendors, and the astronomical amount of data they produce and consume on a daily basis. Traditional cluster computing frameworks can hardly cope with the unprecedented data volume and the geo-distributed, cross-cloud data distribution due to their limited scalability and adaptability across the heterogeneous clouds. Moreover, running data-intensive applications across clouds at will is extremely cost-inefﬁcient and likely to incur outrageous expenses. Hence, we introduce our cloud-agnostic system PIVOT with the novel cost-aware scheduling algorithm, which enables dataintensive applications to run and scale across clouds instantly in a cost-efﬁcient manner. We evaluate our system and scheduling algorithm extensively with simulation, and real-world big data applications on a deployment across 11 regions on AWS and GCP. The experimental results show that PIVOT achieves over 55\% saving in expense for VM subscription and up to 92\% for egress network trafﬁc compared to the state-of-the-art baselines. Notably, the cost-aware scheduling also achieves up to a 10x speedup in data transfers for data-intensive applications.},
	language = {en},
	number = {TR-19-02},
	institution = {RENCI, University of North Carolina at Chapel Hill},
	author = {Jiang, Fan and Ferriter, Kyle and Castillo, Claris},
	month = feb,
	year = {2019},
	pages = {8},
}

@techreport{bell_web-based_2017,
	address = {Geneva, Switzerland},
	title = {Web-based {Analysis} {Services} {Report}},
	url = {http://cds.cern.ch/record/2315331/},
	abstract = {Web-based services (cloud services) is an important trend to innovate end-user services while optimising the service operational costs. CERN users are constantly proposing new approaches (inspired from services existing on the web, tools used in education or other science or based on their experience in using existing computing services). In addition, industry and open source communities have recently made available a large number of powerful and attractive tools and platforms that enable large scale data processing. “Big Data” software stacks notably provide solutions for scalable storage, distributed compute and data analysis engines, data streaming, web-based interfaces (notebooks). Some of those platforms and tools, typically available as open source products, are experiencing a very fast adoption in industry and science such that they are becoming “de facto” references in several areas of data engineering, data science and machine learning. In parallel to users' requests, WLCG is considering to consolidate its deployment model into a relatively reduced number of resource sites (while benefiting from potentially ephemeral resources like public cloud and large installations like experiment filter farms, HPC facilities, analysis centres). In this schema, analysis facilities could be provided as web-based services located in strategic WLCG centres or spawned on relatively short-lived farms},
	language = {eng},
	number = {CERN-IT-Note-2018-004},
	institution = {CERN},
	author = {Bell, Tim and Canali, Luca and Grancher, Eric and Lamanna, Massimo and McCance, Gavin and Mato Vila, Pere and Piparo, Danilo and Moscicki, Jakub and Pace, Alberto and Brito Da Rocha, Ricardo and Simko, Tibor and Smith, Tim and Tejedor Saavedra, Enric},
	collaborator = {CERN. Geneva. IT Department},
	month = nov,
	year = {2017},
	keywords = {CERNBOX, CVMFS, Computing and Computers, Particle Physics - Experiment, REANA, ROOT, SWAN},
}

@techreport{the_austin_group_posix1-2008_2008,
	title = {{POSIX}.1-2008 ({IEEE} {Std} 1003.1™-2008 and {The} {Open} {Group} {Technical} {Standard} {Base} {Specifications}, {Issue} 7)},
	url = {https://pubs.opengroup.org/onlinepubs/9699919799.2008edition/},
	institution = {Austin Group},
	author = {The Austin Group},
	year = {2008},
}

@incollection{hutchison_taverna_2010,
	address = {Berlin, Heidelberg},
	title = {Taverna, {Reloaded}},
	volume = {6187},
	isbn = {978-3-642-13817-1 978-3-642-13818-8},
	url = {http://link.springer.com/10.1007/978-3-642-13818-8_33},
	abstract = {The Taverna workﬂow management system is an open source project with a history of widespread adoption within multiple experimental science communities, and a longterm ambition of eﬀectively supporting the evolving need of those communities for complex, data-intensive, service-based experimental pipelines. This paper describes how the recently overhauled technical architecture of Taverna addresses issues of eﬃciency, scalability, and extensibility, and presents performance results based on a collection of synthetic workﬂows, as well as a concrete case study involving a production workﬂow in the area of cancer research.},
	language = {en},
	urldate = {2020-01-21},
	booktitle = {Scientific and {Statistical} {Database} {Management}},
	publisher = {Springer Berlin Heidelberg},
	author = {Missier, Paolo and Soiland-Reyes, Stian and Owen, Stuart and Tan, Wei and Nenadic, Alexandra and Dunlop, Ian and Williams, Alan and Oinn, Tom and Goble, Carole},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Gertz, Michael and Ludäscher, Bertram},
	year = {2010},
	doi = {10.1007/978-3-642-13818-8_33},
	keywords = {Concurrent Thread, Execution Model, Execution Time, Input Port, Memory Usage},
	pages = {471--481},
}

@article{brandies_ten_2021,
	title = {Ten simple rules for getting started with command-line bioinformatics},
	volume = {17},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008645},
	doi = {10.1371/journal.pcbi.1008645},
	language = {en},
	number = {2},
	urldate = {2021-03-02},
	journal = {PLOS Computational Biology},
	author = {Brandies, Parice A. and Hogg, Carolyn J.},
	month = feb,
	year = {2021},
	note = {Publisher: Public Library of Science},
	keywords = {Bioinformatics, Cloud computing, Computational pipelines, Computer software, Operating systems, Programming languages, Software tools, Source code},
	pages = {e1008645},
}

@article{mitchell_mgnify_2020,
	title = {{MGnify}: the microbiome analysis resource in 2020},
	volume = {48},
	issn = {0305-1048},
	shorttitle = {{MGnify}},
	url = {https://doi.org/10.1093/nar/gkz1035},
	doi = {10.1093/nar/gkz1035},
	abstract = {MGnify (http://www.ebi.ac.uk/metagenomics) provides a free to use platform for the assembly, analysis and archiving of microbiome data derived from sequencing microbial populations that are present in particular environments. Over the past 2 years, MGnify (formerly EBI Metagenomics) has more than doubled the number of publicly available analysed datasets held within the resource. Recently, an updated approach to data analysis has been unveiled (version 5.0), replacing the previous single pipeline with multiple analysis pipelines that are tailored according to the input data, and that are formally described using the Common Workflow Language, enabling greater provenance, reusability, and reproducibility. MGnify's new analysis pipelines offer additional approaches for taxonomic assertions based on ribosomal internal transcribed spacer regions (ITS1/2) and expanded protein functional annotations. Biochemical pathways and systems predictions have also been added for assembled contigs. MGnify's growing focus on the assembly of metagenomic data has also seen the number of datasets it has assembled and analysed increase six-fold. The non-redundant protein database constructed from the proteins encoded by these assemblies now exceeds 1 billion sequences. Meanwhile, a newly developed contig viewer provides fine-grained visualisation of the assembled contigs and their enriched annotations.},
	number = {D1},
	urldate = {2021-04-16},
	journal = {Nucleic Acids Research},
	author = {Mitchell, Alex L and Almeida, Alexandre and Beracochea, Martin and Boland, Miguel and Burgin, Josephine and Cochrane, Guy and Crusoe, Michael R and Kale, Varsha and Potter, Simon C and Richardson, Lorna J and Sakharova, Ekaterina and Scheremetjew, Maxim and Korobeynikov, Anton and Shlemov, Alex and Kunyavskaya, Olga and Lapidus, Alla and Finn, Robert D},
	month = jan,
	year = {2020},
	pages = {D570--D578},
}

@incollection{couvares_workflow_2007,
	address = {London},
	title = {Workflow {Management} in {Condor}},
	isbn = {978-1-84628-757-2},
	url = {https://doi.org/10.1007/978-1-84628-757-2_22},
	abstract = {The Condor project began in 1988 and has evolved into a feature-rich batch system that targets high-throughput computing; that is, Condor ([262], [414]) focuses on providing reliable access to computing over long periods of time instead of highly tuned, high-performance computing for short periods of time or a small number of applications.},
	language = {en},
	urldate = {2021-03-02},
	booktitle = {Workflows for e-{Science}: {Scientific} {Workflows} for {Grids}},
	publisher = {Springer},
	author = {Couvares, Peter and Kosar, Tevfik and Roy, Alain and Weber, Jeff and Wenger, Kent},
	editor = {Taylor, Ian J. and Deelman, Ewa and Gannon, Dennis B. and Shields, Matthew},
	year = {2007},
	doi = {10.1007/978-1-84628-757-2_22},
	keywords = {Basic Local Alignment Search Tool, Batch System, Data Placement, Grid Environment, State Diagram},
	pages = {357--375},
}

@article{seemann_ten_2013,
	title = {Ten recommendations for creating usable bioinformatics command line software},
	volume = {2},
	issn = {2047-217X},
	url = {https://doi.org/10.1186/2047-217X-2-15},
	doi = {10.1186/2047-217X-2-15},
	abstract = {Bioinformatics software varies greatly in quality. In terms of usability, the command line interface is the first experience a user will have of a tool. Unfortunately, this is often also the last time a tool will be used. Here I present ten recommendations for command line software author’s tools to follow, which I believe would greatly improve the uptake and usability of their products, waste less user’s time, and improve the quality of scientific analyses.},
	number = {2047-217X-2-15},
	urldate = {2021-03-02},
	journal = {GigaScience},
	author = {Seemann, Torsten},
	month = dec,
	year = {2013},
}

@article{deelman_pegasus_2015,
	title = {Pegasus, a workflow management system for science automation},
	volume = {46},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X14002015},
	doi = {10.1016/j.future.2014.10.008},
	abstract = {Modern science often requires the execution of large-scale, multi-stage simulation and data analysis pipelines to enable the study of complex systems. The amount of computation and data involved in these pipelines requires scalable workflow management systems that are able to reliably and efficiently coordinate and automate data movement and task execution on distributed computational resources: campus clusters, national cyberinfrastructures, and commercial and academic clouds. This paper describes the design, development and evolution of the Pegasus Workflow Management System, which maps abstract workflow descriptions onto distributed computing infrastructures. Pegasus has been used for more than twelve years by scientists in a wide variety of domains, including astronomy, seismology, bioinformatics, physics and others. This paper provides an integrated view of the Pegasus system, showing its capabilities that have been developed over time in response to application needs and to the evolution of the scientific computing platforms. The paper describes how Pegasus achieves reliable, scalable workflow execution across a wide variety of computing infrastructures.},
	language = {en},
	urldate = {2021-03-02},
	journal = {Future Generation Computer Systems},
	author = {Deelman, Ewa and Vahi, Karan and Juve, Gideon and Rynge, Mats and Callaghan, Scott and Maechling, Philip J. and Mayani, Rajiv and Chen, Weiwei and Ferreira da Silva, Rafael and Livny, Miron and Wenger, Kent},
	month = may,
	year = {2015},
	keywords = {Pegasus, Scientific workflows, Workflow management system},
	pages = {17--35},
}

@article{perkel_workflow_2019,
	title = {Workflow systems turn raw data into scientific knowledge},
	volume = {573},
	copyright = {2019 Nature},
	url = {http://www.nature.com/articles/d41586-019-02619-z},
	doi = {10.1038/d41586-019-02619-z},
	abstract = {How workflow tools can make your computational methods portable, maintainable, reproducible and shareable.},
	language = {en},
	urldate = {2019-09-03},
	journal = {Nature},
	author = {Perkel, Jeffrey M.},
	month = sep,
	year = {2019},
	pages = {149--150},
}

@article{afgan_galaxy_2018,
	title = {The {Galaxy} platform for accessible, reproducible and collaborative biomedical analyses: 2018 update},
	volume = {46},
	issn = {0305-1048},
	shorttitle = {The {Galaxy} platform for accessible, reproducible and collaborative biomedical analyses},
	url = {https://doi.org/10.1093/nar/gky379},
	doi = {10.1093/nar/gky379},
	abstract = {Galaxy (homepage: https://galaxyproject.org, main public server: https://usegalaxy.org) is a web-based scientific analysis platform used by tens of thousands of scientists across the world to analyze large biomedical datasets such as those found in genomics, proteomics, metabolomics and imaging. Started in 2005, Galaxy continues to focus on three key challenges of data-driven biomedical science: making analyses accessible to all researchers, ensuring analyses are completely reproducible, and making it simple to communicate analyses so that they can be reused and extended. During the last two years, the Galaxy team and the open-source community around Galaxy have made substantial improvements to Galaxy's core framework, user interface, tools, and training materials. Framework and user interface improvements now enable Galaxy to be used for analyzing tens of thousands of datasets, and \&gt;5500 tools are now available from the Galaxy ToolShed. The Galaxy community has led an effort to create numerous high-quality tutorials focused on common types of genomic analyses. The Galaxy developer and user communities continue to grow and be integral to Galaxy's development. The number of Galaxy public servers, developers contributing to the Galaxy framework and its tools, and users of the main Galaxy server have all increased substantially.},
	number = {W1},
	urldate = {2021-03-02},
	journal = {Nucleic Acids Research},
	author = {Afgan, Enis and Baker, Dannon and Batut, Bérénice and van den Beek, Marius and Bouvier, Dave and Čech, Martin and Chilton, John and Clements, Dave and Coraor, Nate and Grüning, Björn A and Guerler, Aysam and Hillman-Jackson, Jennifer and Hiltemann, Saskia and Jalili, Vahid and Rasche, Helena and Soranzo, Nicola and Goecks, Jeremy and Taylor, James and Nekrutenko, Anton and Blankenberg, Daniel},
	month = jul,
	year = {2018},
	pages = {W537--W544},
}

@techreport{goncalves_ogc_2020,
	type = {{OGC} {Public} {Engineering} {Report}},
	title = {{OGC} {Earth} {Observations} {Applications} {Pilot}: {Terradue} {Engineering} {Report}},
	url = {http://docs.opengeospatial.org/per/20-042.html},
	abstract = {The availability of a growing volume of environmental data from space represents a unique opportunity for science, general R\&D, and applications (apps), but it also poses a major challenge to achieve its full potential in terms of data exploitation. Firstly, because the emergence of large volumes of data (Petabytes era) raises new issues in terms of discovery, access, exploitation, and visualization of “Big Data”, with profound implications on how users do “data-intensive” Earth Science. Secondly, because the inherent growing diversity and complexity of data and users, whereby different communities – having different needs, skills, methods, languages and protocols – need to cooperate to make sense of a wealth of data of different nature (e.g. EO, in-situ, model), structure and format.

Responding to these technological and community challenges requires the development of new ways of working, capitalizing on Information and Communication Technology (ICT) developments to facilitate the exploitation, analysis, sharing, mining and visualization of massive EO data sets and high-level products within Europe and beyond. Evolution in information technology and the consequent shifts in user behavior and expectations provide new opportunities to provide more significant support to EO data exploitation.

Earth Observation Platforms provide customers with an environment allowing them to focus on their core business and outsource other aspects to a platform that supplies services to a large number of customers with similar needs. The success of platforms in the business world is based on their ability to minimize cost and time to market for their customers, thereby also reducing the risk of exploring unproven business cases. In recent years, Platforms for the Exploitation of Earth Observation data have been developed by public and private companies in order to foster the usage of EO data and expand the market of Earth Observation-derived information. The domain is composed of platform providers, service providers who use the platform to deliver a service to their users, and data providers. The availability of free and open data (e.g. Copernicus Sentinel), together with the availability of affordable computing resources, creates an opportunity for the wide adoption and use of Earth Observation data in a growing number of fields in our society.

OGC activities in Testbed-13, Testbed-14, and Testbed-15 initiated the development of an architecture to allow the ad-hoc deployment and execution of applications close to the physical location of the source data with the goal to minimize data transfer between data repositories and application processes.

The activity described in this Engineering Report responds to the invitation for Earth observation platform operators to implement the OGC Earth Observation Applications Pilot architecture as it has been defined in those previous OGC Innovation Program (IP) initiatives. The goal of the pilot is to evaluate the maturity of those specifications in a real-world environment with several Earth Observation applications brought by several application developers that work with Earth observation satellites. These developers brought different views and requirements in terms of data discovery, data loading, data processing, and result delivery to which the platform readiness is challenged and evolutions are proposed to the architecture.

This Engineering Report initiates by introducing the Earth Observation (EO) Platform architecture and documents the encoding and interfaces needed for defining, deployment and execution of EO Applications brought by the different application developers. The ER concludes with a summary of the main challenges found during the pilot activities and provides further recommendations to advance the architecture, integration and implementation strategies taking in consideration the viewpoints of both EO platforms and EO application developers.},
	number = {OGC 20-042},
	urldate = {2020-11-18},
	institution = {Open Geospatial Consortium},
	author = {Gonçalves, Pedro},
	month = oct,
	year = {2020},
}

@techreport{simonis_ogc_2020,
	type = {{OGC} {Public} {Engineering} {Report}},
	title = {{OGC} {Earth} {Observation} {Applications} {Pilot}: {Summary} {Engineering} {Report}},
	url = {https://docs.ogc.org/per/20-073.html},
	abstract = {This Engineering Report (ER) summarizes the main achievements of the OGC Innovation Program initiative OGC Earth Observation Applications Pilot, conducted between December 2019 and July 2020. The pilot explored an Earth Observation Application software architecture that was developed in OGC Testbeds 13-15. The architecture allows the deployment and execution of externally developed applications on Earth Observation (EO) data and processing platforms. The architecture is essentially based on three major components:

    Execution Management Service (EMS): This component provides a RESTful interface (defined using OpenAPI) to register applications and build workflows from registered applications. The EMS selects the appropriate ADES platform to execute the processes based on the runtime input parameters (close to the data).

    Application Deployment and Execution Service (ADES): This component allows deployment, discovery, and execution of applications or processing of quoting requests.

    Applications are delivered in the form of Docker images along with corresponding metadata called an Application Package (AP). The application package provides all information for deployment and execution of an application.

The pilot demonstrated that the interoperability arrangements developed and documented in OGC Innovation Program initiatives Testbed 13, 14 and 15 provide a solid starting point for maturity tests within operational platforms. Additional arrangements and more detailed definitions have been developed during the pilot that now allow deployment and execution of an application on various platforms with minimal adaptions.

The pilot produced very valuable results. It confirmed the general approach to use Docker for application packaging and HTTP Web APIs or Web Services for application handling and execution. It defined application patterns based on data inputs/outputs, confirmed the role of the Common Workflow Language (CWL) for application description, execution, and workflow building; and recommends the usage of the SpatioTemporal Asset Catalog (STAC) as a data manifest for application inputs and outputs.},
	number = {OGC 20-073},
	urldate = {2020-11-18},
	institution = {Open Geospatial Consortium},
	author = {Simonis, Ingo},
	month = oct,
	year = {2020},
}

@techreport{landry_ogc_2020,
	type = {{OGC} {Public} {Engineering} {Report}},
	title = {{OGC} {Earth} {Observation} {Applications} {Pilot}: {CRIM} {Engineering} {Report}},
	url = {http://docs.opengeospatial.org/per/20-045.html},
	abstract = {CRIM’s key findings are as follows:

    From the application developer’s perspective, it was easier to first develop and test locally, and then use an OGC Application Programming Interface (API) to deploy and execute remotely. This is partly due to a smoother learning curve from application to packaging, than from package to platform.

    Conformance classes for an Application Deployment and Execution Service (ADES) API should be standardized. An offset in the support of API elements subsists in participants' implementations.

    Application packaging, deployment and execution are appropriately defined by combination of Common Workflow Language (CWL) and Docker images.

    More tests are required to better support application workflows running in multiple platforms. In order to build a federated cloud, these tests have to run regularly, in a structured and systemic fashion.

    Participants' findings should offer feedback into EOEPCA, for example for platforms intending to support machine learning (ML) services. From the application developer’s perspective, ML apps are well defined and should be deployable like any other app. It is not clear with the current EOEPCA use cases that the platforms will easily integrate ML services (annotations, trained models, access to GPU clusters, etc.).},
	number = {OGC 20-045},
	urldate = {2020-11-18},
	institution = {Open Geospatial Consortium},
	author = {Landry, Tom},
	month = oct,
	year = {2020},
}

@incollection{kaushik_building_2019,
	address = {New York, NY},
	series = {Methods in {Molecular} {Biology}},
	title = {Building {Portable} and {Reproducible} {Cancer} {Informatics} {Workflows}: {An} {RNA} {Sequencing} {Case} {Study}},
	volume = {1878},
	isbn = {978-1-4939-8868-6},
	shorttitle = {Building {Portable} and {Reproducible} {Cancer} {Informatics} {Workflows}},
	url = {https://doi.org/10.1007/978-1-4939-8868-6_2},
	abstract = {The Seven Bridges Cancer Genomics Cloud (CGC) is part of the National Cancer Institute Cloud Resource project, which was created to explore the paradigm of co-locating massive datasets with the computational resources to analyze them. The CGC was designed to allow researchers to easily find the data they need and analyze it with robust applications in a scalable and reproducible fashion. To enable this, individual tools are packaged within Docker containers and described by the Common Workflow Language (CWL), an emerging standard for enabling reproducible data analysis. On the CGC, researchers can deploy individual tools and customize massive workflows by chaining together tools. Here, we discuss a case study in which RNA sequencing data is analyzed with different methods and compared on the Seven Bridges CGC. We highlight best practices for designing command line tools, Docker containers, and CWL descriptions to enable massively parallelized and reproducible biomedical computation with cloud resources.},
	language = {en},
	urldate = {2020-12-06},
	booktitle = {Cancer {Bioinformatics}},
	publisher = {Springer},
	author = {Kaushik, Gaurav and Davis-Dusenbery, Brandi},
	editor = {Krasnitz, Alexander},
	year = {2019},
	doi = {10.1007/978-1-4939-8868-6_2},
	keywords = {AWS, Bioinformatics, Cancer informatics, Cloud, Docker, Reproducibility, Software design, TCGA},
	pages = {39--64},
}

@article{georgeson_bionitio_2019,
	title = {Bionitio: demonstrating and facilitating best practices for bioinformatics command-line software},
	volume = {8},
	issn = {2047-217X},
	shorttitle = {Bionitio},
	url = {https://doi.org/10.1093/gigascience/giz109},
	doi = {10.1093/gigascience/giz109},
	abstract = {Bioinformatics software tools are often created ad hoc, frequently by people without extensive training in software development. In particular, for beginners, the barrier to entry in bioinformatics software development is high, especially if they want to adopt good programming practices. Even experienced developers do not always follow best practices. This results in the proliferation of poorer-quality bioinformatics software, leading to limited scalability and inefficient use of resources; lack of reproducibility, usability, adaptability, and interoperability; and erroneous or inaccurate results.We have developed Bionitio, a tool that automates the process of starting new bioinformatics software projects following recommended best practices. With a single command, the user can create a new well-structured project in 1 of 12 programming languages. The resulting software is functional, carrying out a prototypical bioinformatics task, and thus serves as both a working example and a template for building new tools. Key features include command-line argument parsing, error handling, progress logging, defined exit status values, a test suite, a version number, standardized building and packaging, user documentation, code documentation, a standard open source software license, software revision control, and containerization.Bionitio serves as a learning aid for beginner-to-intermediate bioinformatics programmers and provides an excellent starting point for new projects. This helps developers adopt good programming practices from the beginning of a project and encourages high-quality tools to be developed more rapidly. This also benefits users because tools are more easily installed and consistent in their usage. Bionitio is released as open source software under the MIT License and is available at https://github.com/bionitio-team/bionitio.},
	number = {giz109},
	urldate = {2021-03-02},
	journal = {GigaScience},
	author = {Georgeson, Peter and Syme, Anna and Sloggett, Clare and Chung, Jessica and Dashnow, Harriet and Milton, Michael and Lonsdale, Andrew and Powell, David and Seemann, Torsten and Pope, Bernard},
	month = sep,
	year = {2019},
}

@article{taylor_overview_2010,
	title = {An overview of the {Hadoop}/{MapReduce}/{HBase} framework and its current applications in bioinformatics},
	volume = {11},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/1471-2105-11-S12-S1},
	doi = {10.1186/1471-2105-11-S12-S1},
	abstract = {Bioinformatics researchers are now confronted with analysis of ultra large-scale data sets, a problem that will only increase at an alarming rate in coming years. Recent developments in open source software, that is, the Hadoop project and associated software, provide a foundation for scaling to petabyte scale data warehouses on Linux clusters, providing fault-tolerant parallelized analysis on such data using a programming style named MapReduce.},
	number = {12},
	urldate = {2021-04-16},
	journal = {BMC Bioinformatics},
	author = {Taylor, Ronald C.},
	month = dec,
	year = {2010},
	keywords = {Cloud Computing, Hadoop Distribute File System, Message Passing Interface, Open Source Software Project, Pacific Northwest National Laboratory},
	pages = {S1},
}

@inproceedings{jiang_cloud-agnostic_2020,
	title = {A {Cloud}-{Agnostic} {Framework} to {Enable} {Cost}-{Aware} {Scheduling} of {Applications} in a {Multi}-{Cloud} {Environment}},
	doi = {10.1109/NOMS47738.2020.9110325},
	abstract = {We have witnessed a surge in both the big data applications being hosted by an assortment of cloud vendors, and in the astronomical amount of data they produce and consume on a daily basis. Traditional cluster computing frameworks can hardly cope with the unprecedented data volume and the geo-distributed, cross-cloud data distribution due to their limited scalability and adaptability across the heterogeneous clouds. Moreover, running data-intensive applications across clouds at will is extremely cost-inefficient and likely to incur outrageous expenses. Hence, we introduce our cloud-agnostic system PIVOT with the novel cost-aware scheduling algorithm, which enables data-intensive applications to run and scale across clouds instantly in a cost-efficient manner. We evaluate our system and scheduling algorithm extensively with the Alibaba production cluster trace, as well as real-world big data applications on a 100-node deployment across 11 regions (31 availability zones) on AWS and GCP. The experimental results show that PIVOT achieves up to 90.8\% saving in expense for VM subscription and 99.2\% for egress network traffic compared to the state-of-the-art baselines. Notably, the cost-aware scheduling also achieves over 4x speedup in data transfers for data-intensive applications.},
	booktitle = {{NOMS} 2020 - 2020 {IEEE}/{IFIP} {Network} {Operations} and {Management} {Symposium}},
	author = {Jiang, Fan and Ferriter, Kyle and Castillo, Claris},
	month = apr,
	year = {2020},
	note = {ISSN: 2374-9709},
	pages = {1--9},
}

@article{berthold_knime_2009,
	title = {{KNIME} - the {Konstanz} information miner: version 2.0 and beyond},
	volume = {11},
	issn = {1931-0145},
	shorttitle = {{KNIME} - the {Konstanz} information miner},
	url = {https://doi.org/10.1145/1656274.1656280},
	doi = {10.1145/1656274.1656280},
	abstract = {The Konstanz Information Miner is a modular environment, which enables easy visual assembly and interactive execution of a data pipeline. It is designed as a teaching, research and collaboration platform, which enables simple integration of new algorithms and tools as well as data manipulation or visualization methods in the form of new modules or nodes. In this paper we describe some of the design aspects of the underlying architecture, briey sketch how new nodes can be incorporated, and highlight some of the new features of version 2.0.},
	number = {1},
	urldate = {2021-03-02},
	journal = {ACM SIGKDD Explorations Newsletter},
	author = {Berthold, Michael R. and Cebron, Nicolas and Dill, Fabian and Gabriel, Thomas R. and Kötter, Tobias and Meinl, Thorsten and Ohl, Peter and Thiel, Kilian and Wiswedel, Bernd},
	month = nov,
	year = {2009},
	pages = {26--31},
}

@article{feitelson_repeatability_2015,
	title = {From {Repeatability} to {Reproducibility} and {Corroboration}},
	volume = {49},
	issn = {0163-5980},
	url = {https://doi.org/10.1145/2723872.2723875},
	doi = {10.1145/2723872.2723875},
	abstract = {Being able to repeat experiments is considered a hallmark of the scientific method, used to confirm or refute hypotheses and previously obtained results. But this can take many forms, from precise repetition using the original experimental artifacts, to conceptual reproduction of the main experimental idea using new artifacts. Furthermore, the conclusions from previous work can also be corroborated using a different experimental methodology altogether. In order to promote a better understanding and use of such methodologies we propose precise definitions for different terms, and suggest when and why each should be used.},
	number = {1},
	urldate = {2021-01-13},
	journal = {ACM SIGOPS Operating Systems Review},
	author = {Feitelson, Dror G.},
	month = jan,
	year = {2015},
	pages = {3--11},
}

@article{ivie_reproducibility_2018,
	title = {Reproducibility in {Scientific} {Computing}},
	volume = {51},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3186266},
	doi = {10.1145/3186266},
	abstract = {Reproducibility is widely considered to be an essential requirement of the scientific process. However, a number of serious concerns have been raised recently, questioning whether today’s computational work is adequately reproducible. In principle, it should be possible to specify a computation to sufficient detail that anyone should be able to reproduce it exactly. But in practice, there are fundamental, technical, and social barriers to doing so. The many objectives and meanings of reproducibility are discussed within the context of scientific computing. Technical barriers to reproducibility are described, extant approaches surveyed, and open areas of research are identified.},
	number = {3},
	urldate = {2021-01-13},
	journal = {ACM Computing Surveys},
	author = {Ivie, Peter and Thain, Douglas},
	month = jul,
	year = {2018},
	keywords = {Reproducibility, computational science, replicability, reproducible, scientific computing, scientific workflow, scientific workflows, workflow, workflows},
	pages = {63:1--63:36},
}

@misc{noauthor_lofar_nodate,
	title = {{LOFAR} and the radio astronomy community {\textbar} {EOSC} {Portal}},
	url = {https://www.eosc-portal.eu/lofar-and-radio-astronomy-community},
	urldate = {2021-01-13},
}

@misc{noauthor_eoscpilot_nodate,
	title = {{EOSCpilot} {Science} {Demonstrator}: {eWaterCycle} \& {SWITCH}-{ON} - {FAIR} data for hydrology {\textbar} {EOSC} {Portal}},
	url = {https://www.eosc-portal.eu/eoscpilot-science-demonstrator-ewatercycle-switch-fair-data-hydrology},
	urldate = {2021-01-13},
}

@book{chapman_common_2016,
	title = {Common {Workflow} {Language}, v1.0},
	url = {https://w3id.org/cwl/v1.0/},
	abstract = {The Common Workflow Language (CWL) is an informal, multi-vendor working group consisting of various organizations and individuals that have an interest in portability of data analysis workflows. Our goal is to create specifications that enable data scientists to describe analysis tools and workflows that are powerful, easy to use, portable, and support reproducibility.CWL builds on technologies such as JSON-LD and Avro for data modeling and Docker for portable runtime environments. CWL is designed to express workflows for data-intensive science, such as Bioinformatics, Medical Imaging, Chemistry, Physics, and Astronomy.This is v1.0 of the CWL tool and workflow specification, released on 2016-07-08},
	urldate = {2017-08-16},
	publisher = {figshare},
	author = {Chapman, Brad and Chilton, John and Heuer, Michael and Kartashov, Andrey and Leehr, Dan and Ménager, Hervé and Nedeljkovich, Maya and Scales, Matt and Soiland-Reyes, Stian and Stojanovic, Luka},
	editor = {Amstutz, Peter and Crusoe, Michael R. and Tijanić, Nebojša},
	month = jul,
	year = {2016},
	doi = {10.6084/M9.FIGSHARE.3115156.V2},
	note = {00030 },
}

@book{robinson_cwl_2017,
	title = {{CWL} {Viewer}: {The} {Common} {Workflow} {Language} {Viewer}},
	volume = {6},
	url = {https://www.open-bio.org/w/images/4/42/BOSC2017-complete-program-compressed.pdf},
	abstract = {The Common Workflow Language (CWL) project emerged from the BOSC 2014},
	urldate = {2017-09-07},
	publisher = {F1000Research},
	author = {Robinson, Mark and Soiland-Reyes, Stian and Crusoe, Michael R and Goble, Carole},
	month = jul,
	year = {2017},
	doi = {10.7490/f1000research.1114375.1},
	note = {00000 },
}

@inproceedings{babuji_parsl_2019,
	address = {New York, NY, USA},
	series = {{HPDC} '19},
	title = {Parsl: {Pervasive} {Parallel} {Programming} in {Python}},
	isbn = {978-1-4503-6670-0},
	shorttitle = {Parsl},
	url = {https://doi.org/10.1145/3307681.3325400},
	doi = {10.1145/3307681.3325400},
	abstract = {High-level programming languages such as Python are increasingly used to provide intuitive interfaces to libraries written in lower-level languages and for assembling applications from various components. This migration towards orchestration rather than implementation, coupled with the growing need for parallel computing (e.g., due to big data and the end of Moore's law), necessitates rethinking how parallelism is expressed in programs. Here, we present Parsl, a parallel scripting library that augments Python with simple, scalable, and flexible constructs for encoding parallelism. These constructs allow Parsl to construct a dynamic dependency graph of components that it can then execute efficiently on one or many processors. Parsl is designed for scalability, with an extensible set of executors tailored to different use cases, such as low-latency, high-throughput, or extreme-scale execution. We show, via experiments on the Blue Waters supercomputer, that Parsl executors can allow Python scripts to execute components with as little as 5 ms of overhead, scale to more than 250000 workers across more than 8000 nodes, and process upward of 1200 tasks per second. Other Parsl features simplify the construction and execution of composite programs by supporting elastic provisioning and scaling of infrastructure, fault-tolerant execution, and integrated wide-area data management. We show that these capabilities satisfy the needs of many-task, interactive, online, and machine learning applications in fields such as biology, cosmology, and materials science.},
	urldate = {2020-12-13},
	booktitle = {Proceedings of the 28th {International} {Symposium} on {High}-{Performance} {Parallel} and {Distributed} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Babuji, Yadu and Woodard, Anna and Li, Zhuozhao and Katz, Daniel S. and Clifford, Ben and Kumar, Rohan and Lacinski, Lukasz and Chard, Ryan and Wozniak, Justin M. and Foster, Ian and Wilde, Michael and Chard, Kyle},
	month = jun,
	year = {2019},
	keywords = {parallel programming, parsl, python},
	pages = {25--36},
}

@article{belhajjame_using_2015,
	title = {Using a suite of ontologies for preserving workflow-centric research objects},
	volume = {32},
	issn = {1570-8268},
	url = {http://www.sciencedirect.com/science/article/pii/S1570826815000049},
	doi = {10.1016/j.websem.2015.01.003},
	abstract = {Scientific workflows are a popular mechanism for specifying and automating data-driven in silico experiments. A significant aspect of their value lies in their potential to be reused. Once shared, workflows become useful building blocks that can be combined or modified for developing new experiments. However, previous studies have shown that storing workflow specifications alone is not sufficient to ensure that they can be successfully reused, without being able to understand what the workflows aim to achieve or to re-enact them. To gain an understanding of the workflow, and how it may be used and repurposed for their needs, scientists require access to additional resources such as annotations describing the workflow, datasets used and produced by the workflow, and provenance traces recording workflow executions. In this article, we present a novel approach to the preservation of scientific workflows through the application of research objects—aggregations of data and metadata that enrich the workflow specifications. Our approach is realised as a suite of ontologies that support the creation of workflow-centric research objects. Their design was guided by requirements elicited from previous empirical analyses of workflow decay and repair. The ontologies developed make use of and extend existing well known ontologies, namely the Object Reuse and Exchange (ORE) vocabulary, the Annotation Ontology (AO) and the W3C PROV ontology (PROVO). We illustrate the application of the ontologies for building Workflow Research Objects with a case-study that investigates Huntington’s disease, performed in collaboration with a team from the Leiden University Medial Centre (HG-LUMC). Finally we present a number of tools developed for creating and managing workflow-centric research objects.},
	language = {en},
	urldate = {2021-01-13},
	journal = {Journal of Web Semantics},
	author = {Belhajjame, Khalid and Zhao, Jun and Garijo, Daniel and Gamble, Matthew and Hettne, Kristina and Palma, Raul and Mina, Eleni and Corcho, Oscar and Gómez-Pérez, José Manuel and Bechhofer, Sean and Klyne, Graham and Goble, Carole},
	month = may,
	year = {2015},
	keywords = {Annotation, Ontologies, Preservation, Provenance, Research object, Scientific workflow},
	pages = {16--42},
}

@inproceedings{missier_w3c_2013,
	address = {New York, NY, USA},
	series = {{EDBT} '13},
	title = {The {W3C} {PROV} family of specifications for modelling provenance metadata},
	isbn = {978-1-4503-1597-5},
	url = {https://doi.org/10.1145/2452376.2452478},
	doi = {10.1145/2452376.2452478},
	abstract = {Provenance, a form of structured metadata designed to record the origin or source of information, can be instrumental in deciding whether information is to be trusted, how it can be integrated with other diverse information sources, and how to establish attribution of information to authors throughout its history. The PROV set of specifications, produced by the World Wide Web Consortium (W3C), is designed to promote the publication of provenance information on the Web, and offers a basis for interoperability across diverse provenance management systems. The PROV provenance model is deliberately generic and domain-agnostic, but extension mechanisms are available and can be exploited for modelling specific domains. This tutorial provides an account of these specifications. Starting from intuitive and informal examples that present idiomatic provenance patterns, it progressively introduces the relational model of provenance along with the constraints model for validation of provenance documents, and concludes with example applications that show the extension points in use.},
	urldate = {2021-01-13},
	booktitle = {Proceedings of the 16th {International} {Conference} on {Extending} {Database} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Missier, Paolo and Belhajjame, Khalid and Cheney, James},
	month = mar,
	year = {2013},
	pages = {773--776},
}

@article{tange_gnu_2011,
	title = {{GNU} {Parallel} - {The} {Command}-{Line} {Power} {Tool}},
	volume = {36},
	issn = {1044-6397},
	url = {https://www.usenix.org/system/files/login/articles/105438-Tange.pdf},
	journal = {;login: The USENIX Magazine},
	author = {Tange, O},
	month = feb,
	year = {2011},
	pages = {42--47},
}

@article{tejedor_pycompss_2017,
	title = {{PyCOMPSs}: {Parallel} computational workflows in {Python}},
	volume = {31},
	issn = {1094-3420},
	shorttitle = {{PyCOMPSs}},
	url = {https://doi.org/10.1177/1094342015594678},
	doi = {10.1177/1094342015594678},
	abstract = {The use of the Python programming language for scientific computing has been gaining momentum in the last years. The fact that it is compact and readable and its complete set of scientific libraries are two important characteristics that favour its adoption. Nevertheless, Python still lacks a solution for easily parallelizing generic scripts on distributed infrastructures, since the current alternatives mostly require the use of APIs for message passing or are restricted to embarrassingly parallel computations. In that sense, this paper presents PyCOMPSs, a framework that facilitates the development of parallel computational workflows in Python. In this approach, the user programs her script in a sequential fashion and decorates the functions to be run as asynchronous parallel tasks. A runtime system is in charge of exploiting the inherent concurrency of the script, detecting the data dependencies between tasks and spawning them to the available resources. Furthermore, we show how this programming model can be built on top of a Big Data storage architecture, where the data stored in the backend is abstracted and accessed from the application in the form of persistent objects.},
	language = {en},
	number = {1},
	urldate = {2020-12-13},
	journal = {The International Journal of High Performance Computing Applications},
	author = {Tejedor, Enric and Becerra, Yolanda and Alomar, Guillem and Queralt, Anna and Badia, Rosa M and Torres, Jordi and Cortes, Toni and Labarta, Jesús},
	month = jan,
	year = {2017},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {Big Data storage, Python, Scientic computing, parallel programming models},
	pages = {66--82},
}

@misc{noauthor_pycompss_nodate,
	title = {{PyCOMPSs}: {Parallel} computational workflows in {Python} - {Enric} {Tejedor}, {Yolanda} {Becerra}, {Guillem} {Alomar}, {Anna} {Queralt}, {Rosa} {M} {Badia}, {Jordi} {Torres}, {Toni} {Cortes}, {Jesús} {Labarta}, 2017},
	url = {https://journals-sagepub-com.vu-nl.idm.oclc.org/doi/10.1177/1094342015594678},
	urldate = {2020-12-13},
}

@article{mons_cloudy_2017,
	title = {Cloudy, increasingly {FAIR}; revisiting the {FAIR} {Data} guiding principles for the {European} {Open} {Science} {Cloud}},
	volume = {37},
	issn = {0167-5265},
	url = {http://content.iospress.com/articles/information-services-and-use/isu824},
	doi = {10.3233/ISU-170824},
	abstract = {The FAIR Data Principles propose that all scholarly output should be Findable, Accessible, Interoperable, and Reusable. As a set of guiding principles, expressing only the kinds of behaviours that researchers should expect from contemporary data reso},
	language = {en},
	number = {1},
	urldate = {2020-07-15},
	journal = {Information Services \& Use},
	author = {Mons, Barend and Neylon, Cameron and Velterop, Jan and Dumontier, Michel and da Silva Santos, Luiz Olavo Bonino and Wilkinson, Mark D.},
	month = jan,
	year = {2017},
	note = {Publisher: IOS Press},
	pages = {49--56},
}

@article{wilkinson_fair_2016,
	title = {The {FAIR} {Guiding} {Principles} for scientific data management and stewardship},
	volume = {3},
	copyright = {2016 The Author(s)},
	issn = {2052-4463},
	url = {http://www.nature.com/articles/sdata201618},
	doi = {10.1038/sdata.2016.18},
	abstract = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders—representing academia, industry, funding agencies, and scholarly publishers—have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.},
	language = {en},
	number = {1},
	urldate = {2020-07-15},
	journal = {Scientific Data},
	author = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, IJsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and da Silva Santos, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Mercè and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J. G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and ’t Hoen, Peter A. C. and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and van Schaik, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and van der Lei, Johan and van Mulligen, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
	month = mar,
	year = {2016},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {160018},
}

@article{koster_snakemakescalable_2012,
	title = {Snakemake—a scalable bioinformatics workflow engine},
	volume = {28},
	issn = {1367-4803},
	url = {https://academic.oup.com/bioinformatics/article/28/19/2520/290322},
	doi = {10.1093/bioinformatics/bts480},
	abstract = {Abstract.  Summary: Snakemake is a workflow engine that provides a readable Python-based workflow definition language and a powerful execution environment that},
	language = {en},
	number = {19},
	urldate = {2019-08-01},
	journal = {Bioinformatics},
	author = {Köster, Johannes and Rahmann, Sven},
	month = oct,
	year = {2012},
	pages = {2520--2522},
}

@article{deelman_future_2017,
	title = {The future of scientific workflows},
	volume = {32},
	issn = {1094-3420},
	url = {http://journals.sagepub.com/doi/10.1177/1094342017704893},
	doi = {10.1177/1094342017704893},
	abstract = {Today’s computational, experimental, and observational sciences rely on computations that involve many related tasks. The success of a scientific mission often hinges on the computer automation of these workflows. In April 2015, the US Department of Energy (DOE) invited a diverse group of domain and computer scientists from national laboratories supported by the Office of Science, the National Nuclear Security Administration, from industry, and from academia to review the workflow requirements of DOE’s science and national security missions, to assess the current state of the art in science workflows, to understand the impact of emerging extreme-scale computing systems on those workflows, and to develop requirements for automated workflow management in future and existing environments. This article is a summary of the opinions of over 50 leading researchers attending this workshop. We highlight use cases, computing systems, workflow needs and conclude by summarizing the remaining challenges this community...},
	number = {1},
	urldate = {2020-01-20},
	journal = {The international journal of high performance computing applications},
	author = {Deelman, Ewa and Peterka, Tom and Altintas, Ilkay and Carothers, Christopher D and Kleese van Dam, Kerstin and Moreland, Kenneth and Parashar, Manish and Ramakrishnan, Lavanya and Taufer, Michela and Vetter, Jeffrey},
	month = apr,
	year = {2017},
	pages = {109434201770489},
}

@article{schaduangrat_towards_2020,
	title = {Towards reproducible computational drug discovery},
	volume = {12},
	issn = {1758-2946},
	url = {https://doi.org/10.1186/s13321-020-0408-x},
	doi = {10.1186/s13321-020-0408-x},
	abstract = {The reproducibility of experiments has been a long standing impediment for further scientific progress. Computational methods have been instrumental in drug discovery efforts owing to its multifaceted utilization for data collection, pre-processing, analysis and inference. This article provides an in-depth coverage on the reproducibility of computational drug discovery. This review explores the following topics: (1) the current state-of-the-art on reproducible research, (2) research documentation (e.g. electronic laboratory notebook, Jupyter notebook, etc.), (3) science of reproducible research (i.e. comparison and contrast with related concepts as replicability, reusability and reliability), (4) model development in computational drug discovery, (5) computational issues on model development and deployment, (6) use case scenarios for streamlining the computational drug discovery protocol. In computational disciplines, it has become common practice to share data and programming codes used for numerical calculations as to not only facilitate reproducibility, but also to foster collaborations (i.e. to drive the project further by introducing new ideas, growing the data, augmenting the code, etc.). It is therefore inevitable that the field of computational drug design would adopt an open approach towards the collection, curation and sharing of data/code.},
	number = {1},
	urldate = {2020-01-30},
	journal = {Journal of Cheminformatics},
	author = {Schaduangrat, Nalini and Lampa, Samuel and Simeon, Saw and Gleeson, Matthew Paul and Spjuth, Ola and Nantasenamat, Chanin},
	month = jan,
	year = {2020},
	pages = {9},
}

@article{belhajjame_privacy-aware_2020,
	title = {On privacy-aware {eScience} workflows},
	issn = {1436-5057},
	url = {https://doi.org/10.1007/s00607-019-00783-8},
	doi = {10.1007/s00607-019-00783-8},
	abstract = {Computing-intensive experiments in modern sciences have become increasingly data-driven illustrating perfectly the Big-Data era. These experiments are usually specified and enacted in the form of workflows that would need to manage (i.e., read, write, store, and retrieve) highly-sensitive data like persons’ medical records. We assume for this work that the operations that constitute a workflow are 1-to-1 operations, in the sense that for each input data record they produce a single data record. While there is an active research body on how to protect sensitive data by, for instance, anonymizing datasets, there is a limited number of approaches that would assist scientists with identifying the datasets, generated by the workflows, that need to be anonymized along with setting the anonymization degree that must be met. We present in this paper a solution privacy requirements of datasets used and generated by a workflow execution. We also present a technique for anonymizing workflow data given an anonymity degree.},
	journal = {Computing},
	author = {Belhajjame, Khalid and Faci, Noura and Maamar, Zakaria and Burégio, Vanilson and Soares, Edvan and Barhamgi, Mahmoud},
	month = jan,
	year = {2020},
}

@article{gruning_bioconda_2018,
	title = {Bioconda: sustainable and comprehensive software distribution for the life sciences.},
	volume = {15},
	issn = {1548-7091},
	url = {http://www.nature.com/articles/s41592-018-0046-7},
	doi = {10.1038/s41592-018-0046-7},
	number = {7},
	urldate = {2018-07-13},
	journal = {Nature Methods},
	author = {Grüning, Björn and Dale, Ryan and Sjödin, Andreas and Chapman, Brad A and Rowe, Jillian and Tomkins-Tinch, Christopher H and Valieris, Renan and Köster, Johannes and Team, Bioconda},
	year = {2018},
	pmid = {29967506},
	pages = {475--476},
}

@article{papageorgiou_genomic_2018,
	title = {Genomic big data hitting the storage bottleneck.},
	volume = {24},
	url = {http://dx.doi.org/10.14806/ej.24.0.910},
	doi = {10.14806/ej.24.0.910},
	abstract = {During the last decades, there is a vast data explosion in bioinformatics. Big data centres are trying to face this data crisis, reaching high storage capacity levels. Although several scientific giants examine how to handle the enormous pile of information in their cupboards, the problem remains unsolved. On a daily basis, there is a massive quantity of permanent loss of extensive information due to infrastructure and storage space problems. The motivation for sequencing has fallen behind. Sometimes, the time that is spent to solve storage space problems is longer than the one dedicated to collect and analyse data. To bring sequencing to the foreground, scientists have to slide over such obstacles and find alternative ways to approach the issue of data volume. Scientific community experiences the data crisis era, where, out of the box solutions may ease the typical research workflow, until technological development meets the needs of Bioinformatics.},
	urldate = {2018-06-05},
	journal = {EMBnet.journal},
	author = {Papageorgiou, Louis and Eleni, Picasi and Raftopoulou, Sofia and Mantaiou, Meropi and Megalooikonomou, Vasileios and Vlachakis, Dimitrios},
	month = apr,
	year = {2018},
	pmid = {29782620},
	pmcid = {PMC5958914},
}

@article{glatard_boutiques_2018,
	title = {Boutiques: a flexible framework to integrate command-line applications in computing platforms.},
	volume = {7},
	issn = {2047-217X},
	url = {https://academic.oup.com/gigascience/advance-article/doi/10.1093/gigascience/giy016/4951979},
	doi = {10.1093/gigascience/giy016},
	abstract = {We present Boutiques, a system to automatically publish, integrate and execute command-line applications across computational platforms. Boutiques applications are installed through software containers described in a rich and flexible JSON language. A set of core tools facilitate the construction, validation, import, execution, and publishing of applications. Boutiques is currently supported by several distinct virtual research platforms, and it has been used to describe dozens of applications in the neuroinformatics domain. We expect Boutiques to improve the quality of application integration in computational platforms, to reduce redundancy of effort, to contribute to computational reproducibility, and to foster Open Science.},
	number = {5},
	urldate = {2018-05-29},
	journal = {GigaScience},
	author = {Glatard, Tristan and Kiar, Gregory and Aumentado-Armstrong, Tristan and Beck, Natacha and Bellec, Pierre and Bernard, Rémi and Bonnet, Axel and Brown, Shawn T and Camarasu-Pop, Sorina and Cervenansky, Frédéric and Das, Samir and Ferreira da Silva, Rafael and Flandin, Guillaume and Girard, Pascal and Gorgolewski, Krzysztof J and Guttmann, Charles R G and Hayot-Sasson, Valérie and Quirion, Pierre-Olivier and Rioux, Pierre and Rousseau, Marc-Étienne and Evans, Alan C},
	month = mar,
	year = {2018},
	pmid = {29718199},
	pmcid = {PMC6007562},
}

@article{van_mourik_porcupine_2018,
	title = {Porcupine: {A} visual pipeline tool for neuroimaging analysis.},
	volume = {14},
	url = {http://dx.doi.org/10.1371/journal.pcbi.1006064},
	doi = {10.1371/journal.pcbi.1006064},
	abstract = {The field of neuroimaging is rapidly adopting a more reproducible approach to data acquisition and analysis. Data structures and formats are being standardised and data analyses are getting more automated. However, as data analysis becomes more complicated, researchers often have to write longer analysis scripts, spanning different tools across multiple programming languages. This makes it more difficult to share or recreate code, reducing the reproducibility of the analysis. We present a tool, Porcupine, that constructs one's analysis visually and automatically produces analysis code. The graphical representation improves understanding of the performed analysis, while retaining the flexibility of modifying the produced code manually to custom needs. Not only does Porcupine produce the analysis code, it also creates a shareable environment for running the code in the form of a Docker image. Together, this forms a reproducible way of constructing, visualising and sharing one's analysis. Currently, Porcupine links to Nipype functionalities, which in turn accesses most standard neuroimaging analysis tools. Our goal is to release researchers from the constraints of specific implementation details, thereby freeing them to think about novel and creative ways to solve a given problem. Porcupine improves the overview researchers have of their processing pipelines, and facilitates both the development and communication of their work. This will reduce the threshold at which less expert users can generate reusable pipelines. With Porcupine, we bridge the gap between a conceptual and an implementational level of analysis and make it easier for researchers to create reproducible and shareable science. We provide a wide range of examples and documentation, as well as installer files for all platforms on our website: https://timvanmourik.github.io/Porcupine. Porcupine is free, open source, and released under the GNU General Public License v3.0.},
	number = {5},
	urldate = {2018-06-05},
	journal = {PLoS Computational Biology},
	author = {van Mourik, Tim and Snoek, Lukas and Knapen, Tomas and Norris, David G},
	month = may,
	year = {2018},
	pmid = {29746461},
	pmcid = {PMC5963801},
	pages = {e1006064},
}

@article{ko_closha_2018,
	title = {Closha: bioinformatics workflow system for the analysis of massive sequencing data.},
	volume = {19},
	issn = {1471-2105},
	url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2019-3},
	doi = {10.1186/s12859-018-2019-3},
	abstract = {BACKGROUND: While next-generation sequencing (NGS) costs have fallen in recent years, the cost and complexity of computation remain substantial obstacles to the use of NGS in bio-medical care and genomic research. The rapidly increasing amounts of data available from the new high-throughput methods have made data processing infeasible without automated pipelines. The integration of data and analytic resources into workflow systems provides a solution to the problem by simplifying the task of data analysis. RESULTS: To address this challenge, we developed a cloud-based workflow management system, Closha, to provide fast and cost-effective analysis of massive genomic data. We implemented complex workflows making optimal use of high-performance computing clusters. Closha allows users to create multi-step analyses using drag and drop functionality and to modify the parameters of pipeline tools. Users can also import the Galaxy pipelines into Closha. Closha is a hybrid system that enables users to use both analysis programs providing traditional tools and MapReduce-based big data analysis programs simultaneously in a single pipeline. Thus, the execution of analytics algorithms can be parallelized, speeding up the whole process. We also developed a high-speed data transmission solution, KoDS, to transmit a large amount of data at a fast rate. KoDS has a file transfer speed of up to 10 times that of normal FTP and HTTP. The computer hardware for Closha is 660 CPU cores and 800 TB of disk storage, enabling 500 jobs to run at the same time. CONCLUSIONS: Closha is a scalable, cost-effective, and publicly available web service for large-scale genomic data analysis. Closha supports the reliable and highly scalable execution of sequencing analysis workflows in a fully automated manner. Closha provides a user-friendly interface to all genomic scientists to try to derive accurate results from NGS platform data. The Closha cloud server is freely available for use from http://closha.kobic.re.kr/ .},
	number = {Suppl 1},
	urldate = {2018-05-29},
	journal = {BMC Bioinformatics},
	author = {Ko, GunHwan and Kim, Pan-Gyu and Yoon, Jongcheol and Han, Gukhee and Park, Seong-Jin and Song, Wangho and Lee, Byungwook},
	month = feb,
	year = {2018},
	pmid = {29504905},
	pmcid = {PMC5836837},
	pages = {43},
}

@article{digles_accessing_2018,
	title = {Accessing the {Open} {PHACTS} {Discovery} {Platform} with {Workflow} {Tools}.},
	volume = {1787},
	url = {http://dx.doi.org/10.1007/978-1-4939-7847-2_14},
	doi = {10.1007/978-1-4939-7847-2_14},
	abstract = {The Open PHACTS Discovery Platform integrates several public databases, which can be of interest when annotating the results of a phenotypic screening campaign. Workflow tools provide easy-to-customize possibilities to access the platform. Here, we describe how to create such workflows for two different workflow tools (KNIME and Pipeline Pilot), including a protocol to annotate compounds (e.g., phenotypic screening hits) with compound classification, known protein targets, and classifications of the targets.},
	urldate = {2018-05-29},
	journal = {Methods in Molecular Biology},
	author = {Digles, Daniela and Caracoti, Andrei and Jacoby, Edgar},
	year = {2018},
	pmid = {29736719},
	pages = {183--193},
}

@article{wimalaratne_uniform_2018,
	title = {Uniform resolution of compact identifiers for biomedical data.},
	volume = {5},
	issn = {2052-4463},
	url = {http://www.nature.com/articles/sdata201829},
	doi = {10.1038/sdata.2018.29},
	abstract = {Most biomedical data repositories issue locally-unique accessions numbers, but do not provide globally unique, machine-resolvable, persistent identifiers for their datasets, as required by publishers wishing to implement data citation in accordance with widely accepted principles. Local accessions may however be prefixed with a namespace identifier, providing global uniqueness. Such "compact identifiers" have been widely used in biomedical informatics to support global resource identification with local identifier assignment. We report here on our project to provide robust support for machine-resolvable, persistent compact identifiers in biomedical data citation, by harmonizing the Identifiers.org and N2T.net (Name-To-Thing) meta-resolvers and extending their capabilities. Identifiers.org services hosted at the European Molecular Biology Laboratory - European Bioinformatics Institute (EMBL-EBI), and N2T.net services hosted at the California Digital Library (CDL), can now resolve any given identifier from over 600 source databases to its original source on the Web, using a common registry of prefix-based redirection rules. We believe these services will be of significant help to publishers and others implementing persistent, machine-resolvable citation of research data.},
	urldate = {2018-05-29},
	journal = {Scientific data},
	author = {Wimalaratne, Sarala M and Juty, Nick and Kunze, John and Janée, Greg and McMurry, Julie A and Beard, Niall and Jimenez, Rafael and Grethe, Jeffrey S and Hermjakob, Henning and Martone, Maryann E and Clark, Tim},
	month = may,
	year = {2018},
	pmid = {29737976},
	pmcid = {PMC5944906},
	pages = {180029},
}

@article{miksa_using_2017,
	title = {Using ontologies for verification and validation of workflow-based experiments},
	volume = {43},
	issn = {15708268},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1570826817300112},
	doi = {10.1016/j.websem.2017.01.002},
	abstract = {Scientific experiments performed in the eScience domain require special tooling, software, and workflows that allow researchers to link, transform, visualize and interpret data. Recent studies report that such experiments often cannot be replicated due to dierences in the underlying infrastructure. The provenance collection mechanisms were built into workflow engines to increase research replicability. However, the traces do not contain the execution context that consists of software, hardware and external services used to produce the result which may change between executions. The problem thus remains on how to identify such context and how to store such data. To address this challenge we propose the context model that integrates ontologies which describe workflow and its environment. It includes not only high level description of workflow steps and services but also low level technical details on infrastructure, including hardware, software, and files. In this paper we discuss which ontologies that compose the context model must be instantiated to enable verification of a workflow re-execution. We use a tool that monitors a workflow execution and automatically creates the context model. We also authored the VPlan ontology that enables modelling validation requirements. It contains a controlled vocabulary of metrics that can be used for quantification of requirements. We evaluate the proposed ontologies on five Taverna workflows that dier in the degree on which they depend on additional software and services. The results show that the proposed ontologies are necessary and can be used for verification and validation of scientific workflows re-executions in dierent environments without the necessity of accessing the original environment at the same time. Thus the scientists can state whether the scientific experiment is replicable.},
	number = {0},
	urldate = {2018-04-30},
	journal = {Web Semantics: Science, Services and Agents on the World Wide Web},
	author = {Miksa, Tomasz and Rauber, Andreas},
	month = mar,
	year = {2017},
	pages = {25--45},
}

@article{peng_sos_2018,
	title = {{SoS} {Notebook}: {An} {Interactive} {Multi}-{Language} {Data} {Analysis} {Environment}.},
	volume = {34},
	url = {http://dx.doi.org/10.1093/bioinformatics/bty405},
	doi = {10.1093/bioinformatics/bty405},
	abstract = {Motivation: Complex bioinformatic data analysis workflows involving multiple scripts in different languages can be difficult to consolidate, share, and reproduce. An environment that streamlines the entire processes of data collection, analysis, visualization and reporting of such multi-language analyses is currently lacking. Results: We developed Script of Scripts (SoS) Notebook, a web-based notebook environment that allows the use of multiple scripting language in a single notebook, with data flowing freely within and across languages. SoS Notebook enables researchers to perform sophisticated bioinformatic analysis using the most suitable tools for different parts of the workflow, without the limitations of a particular language or complications of cross-language communications. Availability: SoS Notebook is hosted at http://vatlab.github.io/SoS/ and is distributed under a BSD license. Contact: bpeng@mdanderson.org.},
	number = {21},
	urldate = {2018-05-29},
	journal = {Bioinformatics},
	author = {Peng, Bo and Wang, Gao and Ma, Jun and Leong, Man Chong and Wakefield, Chris and Melott, James and Chiu, Yulun and Du, Di and Weinstein, John N},
	month = may,
	year = {2018},
	pmid = {29790910},
	pmcid = {PMC6198852},
	pages = {3768--3770},
}

@article{koster_snakemake-scalable_2018,
	title = {Snakemake-a scalable bioinformatics workflow engine.},
	volume = {34},
	issn = {1367-4803},
	url = {https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/bty350/4996910},
	doi = {10.1093/bioinformatics/bty350},
	number = {20},
	urldate = {2018-05-29},
	journal = {Bioinformatics},
	author = {Köster, Johannes and Rahmann, Sven},
	month = oct,
	year = {2018},
	pmid = {29788404},
	pages = {3600},
}

@article{kluge_watchdog_2018,
	title = {Watchdog - a workflow management system for the distributed analysis of large-scale experimental data.},
	volume = {19},
	url = {http://dx.doi.org/10.1186/s12859-018-2107-4},
	doi = {10.1186/s12859-018-2107-4},
	abstract = {BACKGROUND: The development of high-throughput experimental technologies, such as next-generation sequencing, have led to new challenges for handling, analyzing and integrating the resulting large and diverse datasets. Bioinformatical analysis of these data commonly requires a number of mutually dependent steps applied to numerous samples for multiple conditions and replicates. To support these analyses, a number of workflow management systems (WMSs) have been developed to allow automated execution of corresponding analysis workflows. Major advantages of WMSs are the easy reproducibility of results as well as the reusability of workflows or their components. RESULTS: In this article, we present Watchdog, a WMS for the automated analysis of large-scale experimental data. Main features include straightforward processing of replicate data, support for distributed computer systems, customizable error detection and manual intervention into workflow execution. Watchdog is implemented in Java and thus platform-independent and allows easy sharing of workflows and corresponding program modules. It provides a graphical user interface (GUI) for workflow construction using pre-defined modules as well as a helper script for creating new module definitions. Execution of workflows is possible using either the GUI or a command-line interface and a web-interface is provided for monitoring the execution status and intervening in case of errors. To illustrate its potentials on a real-life example, a comprehensive workflow and modules for the analysis of RNA-seq experiments were implemented and are provided with the software in addition to simple test examples. CONCLUSIONS: Watchdog is a powerful and flexible WMS for the analysis of large-scale high-throughput experiments. We believe it will greatly benefit both users with and without programming skills who want to develop and apply bioinformatical workflows with reasonable overhead. The software, example workflows and a comprehensive documentation are freely available at www.bio.ifi.lmu.de/watchdog.},
	number = {1},
	urldate = {2018-04-28},
	journal = {BMC Bioinformatics},
	author = {Kluge, Michael and Friedel, Caroline C},
	month = mar,
	year = {2018},
	pmid = {29534677},
	pmcid = {PMC5850912},
	pages = {97},
}

@inproceedings{singh_deep_2017,
	title = {Deep {Learning} on {Operational} {Facility} {Data} {Related} to {Large}-{Scale} {Distributed} {Area} {Scientific} {Workflows}},
	isbn = {978-1-5386-2686-3},
	url = {http://ieeexplore.ieee.org/document/8109199/},
	doi = {10.1109/eScience.2017.94},
	abstract = {Distributed computing platforms provide a robust mechanism to perform large-scale computations by splitting the task and data among multiple locations, possibly located thousands of miles apart geographically. Although such distribution of resources can lead to benefits, it also comes with its associated problems such as rampant duplication of file transfers increasing congestion, long job completion times, unexpected site crashing, suboptimal data transfer rates, unpredictable reliability in a time range, and suboptimal usage of storage elements. In addition, each sub-system becomes a potential failure node that can trigger system wide disruptions. In this vision paper, we outline our approach to leveraging Deep Learning algorithms to discover solutions to unique problems that arise in a system with computational infrastructure that is spread over a wide area. The presented vision, motivated by a real scientific use case from Belle II experiments, is to develop multilayer neural networks to tackle forecasting, anomaly detection and optimization challenges in a complex and distributed data movement environment. Through this vision based on Deep Learning principles, we aim to achieve reduced congestion events, faster file transfer rates, and enhanced site reliability.},
	urldate = {2018-04-28},
	booktitle = {2017 {IEEE} 13th {International} {Conference} on e-{Science} (e-{Science})},
	publisher = {IEEE},
	author = {Singh, Alok and Stephan, Eric and Schram, Malachi and Altintas, Ilkay},
	month = oct,
	year = {2017},
	pages = {586--591},
}

@inproceedings{zamani_supporting_2017,
	title = {Supporting {Data}-{Driven} {Workflows} {Enabled} by {Large} {Scale} {Observatories}},
	isbn = {978-1-5386-2686-3},
	url = {http://ieeexplore.ieee.org/document/8109200/},
	doi = {10.1109/eScience.2017.95},
	abstract = {Large scale observatories are shared-use resources that provide open access to data from geographically distributed sensors and instruments. This data has the potential to accelerate scientific discovery. However, seamlessly integrating the data into scientific workflows remains a challenge. In this paper, we summarize our ongoing work in supporting data-driven and data-intensive workflows and outline our vision for how these observatories can improve large-scale science. Specifically, we present programming abstractions and runtime management services to enable the automatic integration of data in scientific workflows. Further, we show how approximation techniques can be used to address network and processing variations by studying constraint limitations and their associated latencies. We use the Ocean Observatories Initiative (OOI) as a driving use case for this work.},
	urldate = {2018-04-28},
	booktitle = {2017 {IEEE} 13th {International} {Conference} on e-{Science} (e-{Science})},
	publisher = {IEEE},
	author = {Zamani, Ali Reza and AbdelBaky, Moustafa and Balouek-Thomert, Daniel and Rodero, Ivan and Parashar, Manish},
	month = oct,
	year = {2017},
	pages = {592--595},
}

@inproceedings{mandal_toward_2017,
	title = {Toward prioritization of data flows for scientific workflows using virtual software defined exchanges},
	isbn = {978-1-5386-2686-3},
	url = {http://ieeexplore.ieee.org/document/8109197/},
	doi = {10.1109/eScience.2017.92},
	abstract = {Recent advances in cloud systems, on-demand circuits and software-defined networking have created new opportunities to enable complex, data-intensive scientific applications to run on dynamic networked cloud infrastructures. In this work, we present an end-to-end framework for autonomic adaptation for scientific workflows on networked cloud systems, which leverages novel network provisioning technologies. We present an application-independent controller framework called Mobius++ that includes dynamic network adaptation capabilities using Software-Defined Networking (SDN) mechanisms, which enables workflow management systems to address competing priorities of workflow operations, data movements in particular. We use a representative, data-intensive bioinformatics workflow as a driving use case to showcase the above capabilities. Experimental results show that the Mobius++ framework, in conjunction with a novel virtual Software Defined Exchange (SDX) platform, is able to dynamically prioritize bandwidths between different end-points, on-demand, and being driven by priority directives from a workflow management system. We show that data transfer jobs from two workflows with different priorities are accurately arbitrated as the relative priorities change.},
	urldate = {2018-04-28},
	booktitle = {2017 {IEEE} 13th {International} {Conference} on e-{Science} (e-{Science})},
	publisher = {IEEE},
	author = {Mandal, Anirban and Ruth, Paul and Baldin, Ilya and Da Silva, Rafael Ferreira and Deelman, Ewa},
	month = oct,
	year = {2017},
	pages = {566--575},
}

@inproceedings{diaz_workflowhunt_2017,
	title = {Workflowhunt: combining keyword and semantic search in scientific workflow repositories},
	isbn = {978-1-5386-2686-3},
	url = {http://ieeexplore.ieee.org/document/8109131/},
	doi = {10.1109/eScience.2017.26},
	abstract = {Scientific datasets and the experiments that analyze them are growing in size and complexity, and scientists are facing difficulties to share such resources. Some initiatives have emerged to try to solve this problem. One of them involves the use of scientific workflows to represent and enact experiment execution. There is an increasing number of workflows that are potentially relevant for more than one scientific domain. However, it is hard to find workflows suitable for reuse given an experiment. Creating a workflow takes time and resources, and their reuse helps scientists to build new workflows faster and in a more reliable way. Search mechanisms in workflow repositories should provide different options for workflow discovery, but it is difficult for generic repositories to provide multiple mechanisms. This paper presents WorkflowHunt, a hybrid architecture for workflow search and discovery for generic repositories, which combines keyword and semantic search to allow finding relevant workflows using different search methods. We validated our architecture creating a prototype that uses real workflows and metadata from myExperiment, and compare search results via WorkflowHunt and via myExperiment's search interface.},
	urldate = {2018-04-28},
	booktitle = {2017 {IEEE} 13th {International} {Conference} on e-{Science} (e-{Science})},
	publisher = {IEEE},
	author = {Diaz, Juan Sebastian Beleno and Medeiros, Claudia Bauzer},
	month = oct,
	year = {2017},
	pages = {138--147},
}

@article{penders_ten_2018,
	title = {Ten simple rules for responsible referencing.},
	volume = {14},
	issn = {1553-7358},
	url = {http://dx.plos.org/10.1371/journal.pcbi.1006036},
	doi = {10.1371/journal.pcbi.1006036},
	number = {4},
	urldate = {2018-05-29},
	journal = {PLoS Computational Biology},
	author = {Penders, Bart},
	month = apr,
	year = {2018},
	pmid = {29649210},
	pmcid = {PMC5896885},
	pages = {e1006036},
}

@article{devenyi_ten_2018,
	title = {Ten simple rules for collaborative lesson development.},
	volume = {14},
	issn = {1553-7358},
	url = {http://dx.plos.org/10.1371/journal.pcbi.1005963},
	doi = {10.1371/journal.pcbi.1005963},
	number = {3},
	urldate = {2018-05-29},
	journal = {PLoS Computational Biology},
	author = {Devenyi, Gabriel A and Emonet, Rémi and Harris, Rayna M and Hertweck, Kate L and Irving, Damien and Milligan, Ian and Wilson, Greg},
	month = mar,
	year = {2018},
	pmid = {29494585},
	pmcid = {PMC5832188},
	pages = {e1005963},
}

@article{atkinson_scientific_2017,
	title = {Scientific workflows: {Past}, present and future},
	volume = {75},
	issn = {0167739X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0167739X17311202},
	doi = {10.1016/j.future.2017.05.041},
	abstract = {This special issue and our editorial celebrate 10 years of progress with data-intensive or scientific workflows. There have been very substantial advances in the representation of workflows and in the engineering of workflow management systems (WMS). The creation and refinement stages are now well supported, with a significant improvement in usability. Improved abstraction supports cross-fertilisation between different workflow communities and consistent interpretation as WMS evolve. Through such re-engineering the WMS deliver much improved performance, significantly increased scale and sophisticated reliability mechanisms. Further improvement is anticipated from substantial advances in optimisation. We invited papers from those who have delivered these advances and selected 14 to represent today’s achievements and representative plans for future progress. This editorial introduces those contributions with an overview and categorisation of the papers. Furthermore, it elucidates responses from a survey of major workflow systems, which provides evidence of substantial progress and a structured index of related papers. We conclude with suggestions on areas where further research and development is needed and offer a vision of future research directions.},
	urldate = {2018-07-13},
	journal = {Future Generation Computer Systems},
	author = {Atkinson, Malcolm and Gesing, Sandra and Montagnat, Johan and Taylor, Ian},
	month = oct,
	year = {2017},
	pages = {216--227},
}

@incollection{christensen_store_1995,
	address = {Berlin, Heidelberg},
	series = {Lecture notes in computer science},
	title = {Store — a system for handling third-party applications in a heterogeneous computer environment},
	volume = {1005},
	isbn = {978-3-540-47768-6},
	url = {http://link.springer.com/10.1007/3-540-60578-9_22},
	urldate = {2017-11-03},
	booktitle = {Software {Configuration} {Management}},
	publisher = {Springer Berlin Heidelberg},
	author = {Christensen, Anders and Egge, Tor},
	editor = {Estublier, Jacky and Goos, Gerhard and Hartmanis, Juris and Leeuwen, Jan},
	year = {1995},
	doi = {10.1007/3-540-60578-9_22},
	pages = {263--276},
}

@article{ferreira_da_silva_characterization_2017,
	title = {A characterization of workflow management systems for extreme-scale applications},
	volume = {75},
	issn = {0167739X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0167739X17302510},
	doi = {10.1016/j.future.2017.02.026},
	abstract = {Automation of the execution of computational tasks is at the heart of improving scientific productivity. Over the last years, scientific workflows have been established as an important abstraction that captures data processing and computation of large and complex scientific applications. By allowing scientists to model and express entire data processing steps and their dependencies, workflow management systems relieve scientists from the details of an application and manage its execution on a computational infrastructure. As the resource requirements of today’s computational and data science applications that process vast amounts of data keep increasing, there is a compelling case for a new generation of advances in high-performance computing, commonly termed as extreme-scale computing, which will bring forth multiple challenges for the design of workflow applications and management systems. This paper presents a novel characterization of workflow management systems using features commonly associated with extreme-scale computing applications. We classify 15 popular workflow management systems in terms of workflow execution models, heterogeneous computing environments, and data access methods. The paper also surveys workflow applications and identifies gaps for future research on the road to extreme-scale workflows and management systems.},
	urldate = {2020-01-20},
	journal = {Future Generation Computer Systems},
	author = {Ferreira da Silva, Rafael and Filgueira, Rosa and Pietri, Ilia and Jiang, Ming and Sakellariou, Rizos and Deelman, Ewa},
	month = oct,
	year = {2017},
	pages = {228--238},
}

@article{carey_ten_2018,
	title = {Ten simple rules for biologists learning to program.},
	volume = {14},
	issn = {1553-7358},
	url = {http://dx.plos.org/10.1371/journal.pcbi.1005871},
	doi = {10.1371/journal.pcbi.1005871},
	number = {1},
	urldate = {2018-05-29},
	journal = {PLoS Computational Biology},
	author = {Carey, Maureen A and Papin, Jason A},
	month = jan,
	year = {2018},
	pmid = {29300745},
	pmcid = {PMC5754048},
	pages = {e1005871},
}

@phdthesis{klingstrom_data_2017,
	type = {{THESIS}.{DOCTORAL}},
	title = {Data integration and handling},
	url = {https://pub.epsilon.slu.se/14669/},
	abstract = {Modern technology allows researchers to generate data at an ever increasing rate,outpacing the capacity of researchers to analyse it. Developing automated supportsystems for the collection, management and distribution of information is therefore animportant step to reduce error rates and accelerate progress to enable high-qualityresearch based on big data volumes. This thesis encompasses five articles, describingstrategies for the creation of technical research platforms, as well as descriptions of thetechnical platforms themselves.The key conclusion of the thesis is that technical solutions for many issues have beenavailable for a long time. These technical solutions are however overlooked, or simplyignored, if they fail to recognise the social dimensions of the issues they try to solve.The Molecular Methods database is an example of a technically sound but onlypartially successful solution in regards to social viability. Thousands of researchershave used the website to access protocols, but only a handful have shared their ownwork on MolMeth. Experiences from the Molecular Methods database and otherprojects have provided a foundation for studies supporting the development of theeB3KitThe eB3Kit is a portable, robust and scalable informatics platform for structured datamanagement. Deploying the platform enables research groups to carry out advancedresearch projects with very limited means. With the eB3Kit researchers can integratedata from a wide variety of sources, including the local laboratory informationmanagement system and analyse it using the Galaksio interface. Galaksio provides userfriendly access to the Galaxy workflow management system and provides eB3Kit userswith access to tools developed by a far larger user community than the one activelydeveloping the eB3Kit. Using a workflow management system improvesreproducibility and enables bioinformaticians to prepare workflows without directlyaccessing ethically or commercially sensitive data. Therefore, it is especially well-suited for applications where researchers are worried about privacy and during diseaseoutbreaks where persistent storage and analysis capacity must be established quickly.},
	urldate = {2017-11-08},
	school = {Uppsala : Sveriges lantbruksuniv},
	author = {Klingström, Tomas},
	month = oct,
	year = {2017},
}

@article{perens_debians_1997,
	title = {Debian’s “{Social} {Contract}” with the {Free} {Software} {Community}},
	url = {https://lists.debian.org/debian-announce/1997/msg00017.html},
	number = {msg00017},
	urldate = {2017-11-03},
	journal = {debian-announce@lists.debian.org},
	author = {Perens, Bruce},
	month = jul,
	year = {1997},
	keywords = {⛔ No DOI found},
}

@article{murdock_debian_1994,
	title = {The {Debian} {Linux} {Manifesto}},
	url = {http://www.ibiblio.org/pub/historic-linux/distributions/debian-0.91/info/Manifesto},
	urldate = {2017-11-03},
	author = {Murdock, Ian A.},
	month = jun,
	year = {1994},
	keywords = {⛔ No DOI found},
}

@article{gruning_bioconda_2017,
	title = {Bioconda: {A} sustainable and comprehensive software distribution for the life sciences},
	url = {http://biorxiv.org/lookup/doi/10.1101/207092},
	doi = {10.1101/207092},
	abstract = {We present Bioconda (https://bioconda.github.io), a distribution of bioinformatics software for the lightweight, multi-platform and language-agnostic package manager Conda. Currently, Bioconda offers a collection of over 3000 software packages, which is continuously maintained, updated, and extended by a growing global community of more than 200 contributors. Bioconda improves analysis reproducibility by allowing users to define isolated environments with defined software versions, all of which are easily installed and managed without administrative privileges.},
	urldate = {2017-11-03},
	journal = {BioRxiv},
	author = {Grüning, Björn and Dale, Ryan and Sjödin, Andreas and Rowe, Jillian and Chapman, Brad A. and Tomkins-Tinch, Christopher H. and Valieris, Renan and Team, The Bioconda and Köster, Johannes},
	month = oct,
	year = {2017},
}

@article{krabbenhoft_integrating_2008,
	title = {Integrating {ARC} grid middleware with {Taverna} workflows.},
	volume = {24},
	url = {http://dx.doi.org/10.1093/bioinformatics/btn095},
	doi = {10.1093/bioinformatics/btn095},
	abstract = {SUMMARY: This work presents two independent approaches for a seamless integration of computational grids with the bioinformatics workflow suite Taverna. These are supported by a unique relational database to link applications with grid resources and presents those as workflow elements. A web portal facilitates its collaborative maintenance. The first approach implements a gateway service to handle authentication certificates and all communication with the grid. It reads the database to spawn web services for workflow elements which are in turn used by Taverna. The second approach lets Taverna communicate with the grid on its own, by means of a newly developed plug-in. It reads the database and executes the needed tasks directly on the grid. While the gateway service is non-intrusive, the plug-in has technical advantages, e.g. by allowing data to remain on the grid while being passed between workflow elements. AVAILABILITY: http://grid.inb.uni-luebeck.de/},
	number = {9},
	urldate = {2017-11-03},
	journal = {Bioinformatics},
	author = {Krabbenhöft, Hajo N and Möller, Steffen and Bayer, Daniel},
	month = may,
	year = {2008},
	pmid = {18353787},
	pages = {1221--1222},
}

@article{laurie_wet-lab_2016,
	title = {From {Wet}-{Lab} to {Variations}: {Concordance} and {Speed} of {Bioinformatics} {Pipelines} for {Whole} {Genome} and {Whole} {Exome} {Sequencing}.},
	volume = {37},
	url = {http://dx.doi.org/10.1002/humu.23114},
	doi = {10.1002/humu.23114},
	abstract = {As whole genome sequencing becomes cheaper and faster, it will progressively substitute targeted next-generation sequencing as standard practice in research and diagnostics. However, computing cost-performance ratio is not advancing at an equivalent rate. Therefore, it is essential to evaluate the robustness of the variant detection process taking into account the computing resources required. We have benchmarked six combinations of state-of-the-art read aligners (BWA-MEM and GEM3) and variant callers (FreeBayes, GATK HaplotypeCaller, SAMtools) on whole genome and whole exome sequencing data from the NA12878 human sample. Results have been compared between them and against the NIST Genome in a Bottle (GIAB) variants reference dataset. We report differences in speed of up to 20 times in some steps of the process and have observed that SNV, and to a lesser extent InDel, detection is highly consistent in 70\% of the genome. SNV, and especially InDel, detection is less reliable in 20\% of the genome, and almost unfeasible in the remaining 10\%. These findings will aid in choosing the appropriate tools bearing in mind objectives, workload, and computing infrastructure available. {\textbackslash}copyright 2016 The Authors. **Human Mutation published by Wiley Periodicals, Inc.},
	number = {12},
	urldate = {2017-11-03},
	journal = {Human Mutation},
	author = {Laurie, Steve and Fernandez-Callejo, Marcos and Marco-Sola, Santiago and Trotta, Jean-Remi and Camps, Jordi and Chacón, Alejandro and Espinosa, Antonio and Gut, Marta and Gut, Ivo and Heath, Simon and Beltran, Sergi},
	month = sep,
	year = {2016},
	pmid = {27604516},
	pmcid = {PMC5129537},
	pages = {1263--1271},
}

@article{schubert_immunonodes_2017,
	title = {{ImmunoNodes} - graphical development of complex immunoinformatics workflows.},
	volume = {18},
	url = {http://dx.doi.org/10.1186/s12859-017-1667-z},
	doi = {10.1186/s12859-017-1667-z},
	abstract = {BACKGROUND: Immunoinformatics has become a crucial part in biomedical research. Yet many immunoinformatics tools have command line interfaces only and can be difficult to install. Web-based immunoinformatics tools, on the other hand, are difficult to integrate with other tools, which is typically required for the complex analysis and prediction pipelines required for advanced applications. RESULT: We present ImmunoNodes, an immunoinformatics toolbox that is fully integrated into the visual workflow environment KNIME. By dragging and dropping tools and connecting them to indicate the data flow through the pipeline, it is possible to construct very complex workflows without the need for coding. CONCLUSION: ImmunoNodes allows users to build complex workflows with an easy to use and intuitive interface with a few clicks on any desktop computer.},
	number = {1},
	urldate = {2017-11-03},
	journal = {BMC Bioinformatics},
	author = {Schubert, Benjamin and de la Garza, Luis and Mohr, Christopher and Walzer, Mathias and Kohlbacher, Oliver},
	month = may,
	year = {2017},
	pmid = {28482806},
	pmcid = {PMC5422934},
	pages = {242},
}

@article{hastreiter_knime4ngs_2017,
	title = {{KNIME4NGS}: a comprehensive toolbox for {Next} {Generation} {Sequencing} analysis.},
	url = {http://dx.doi.org/10.1093/bioinformatics/btx003},
	doi = {10.1093/bioinformatics/btx003},
	abstract = {Analysis of Next Generation Sequencing (NGS) data requires the processing of large datasets by chaining various tools with complex input and output formats. In order to automate data analysis, we propose to standardize NGS tasks into modular workflows. This simplifies reliable handling and processing of NGS data, and corresponding solutions become substantially more reproducible and easier to maintain. Here, we present a documented, linux-based, toolbox of 42 processing modules that are combined to construct workflows facilitating a variety of tasks such as DNAseq and RNAseq analysis. We also describe important technical extensions. The high throughput executor (HTE) helps to increase the reliability and to reduce manual interventions when processing complex datasets. We also provide a dedicated binary manager that assists users in obtaining the modules' executables and keeping them up to date. As basis for this actively developed toolbox we use the workflow management software KNIME. AVAILABILITY: See http://ibisngs.github.io/knime4ngs for nodes and user manual (GPLv3 license) CONTACT: robert.kueffner@helmholtz-muenchen.de. {\textbackslash}copyright The Author (2017). Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com.},
	urldate = {2017-11-03},
	journal = {Bioinformatics},
	author = {Hastreiter, Maximilian and Jeske, Tim and Hoser, Jonathan and Kluge, Michael and Ahomaa, Kaarin and Friedl, Marie-Sophie and Kopetzky, Sebastian J and Quell, Jan-Dominik and Werner Mewes, H- and Küffner, Robert},
	month = jan,
	year = {2017},
	pmid = {28069593},
}

@article{amadio_portage_2016,
	title = {Portage: {Bringing} {Hackers}' {Wisdom} to {Science}},
	volume = {abs/1610.02742},
	url = {http://arxiv.org/abs/1610.02742},
	urldate = {2017-11-03},
	journal = {CoRR},
	author = {Amadio, Guilherme and Xu, Benda},
	year = {2016},
	keywords = {⛔ No DOI found},
}

@phdthesis{kalas_efforts_2015,
	type = {{THESIS}.{DOCTORAL}},
	title = {Efforts towards accessible and reliable bioinformatics},
	url = {http://hdl.handle.net/1956/10658},
	urldate = {2017-11-03},
	school = {University of Bergen, Norway},
	author = {Kalaš, Matúš},
	month = nov,
	year = {2015},
}

@article{kent_assembly_2001,
	title = {Assembly of the working draft of the human genome with {GigAssembler}.},
	volume = {11},
	issn = {1088-9051},
	url = {http://dx.doi.org/10.1101/gr.183201},
	doi = {10.1101/gr.183201},
	abstract = {The data for the public working draft of the human genome contains roughly 400,000 initial sequence contigs in approximately 30,000 large insert clones. Many of these initial sequence contigs overlap. A program, GigAssembler, was built to merge them and to order and orient the resulting larger sequence contigs based on mRNA, paired plasmid ends, EST, BAC end pairs, and other information. This program produced the first publicly available assembly of the human genome, a working draft containing roughly 2.7 billion base pairs and covering an estimated 88\% of the genome that has been used for several recent studies of the genome. Here we describe the algorithm used by GigAssembler.},
	number = {9},
	urldate = {2017-11-03},
	journal = {Genome Research},
	author = {Kent, W J and Haussler, D},
	month = sep,
	year = {2001},
	pmid = {11544197},
	pmcid = {PMC311095},
	pages = {1541--1548},
}

@article{yung_large-scale_2017,
	title = {Large-{Scale} {Uniform} {Analysis} of {Cancer} {Whole} {Genomes} in {Multiple} {Computing}},
	url = {http://www.biorxiv.org/content/early/2017/07/10/161638.abstract},
	abstract = {The International Cancer Genome Consortium (ICGC) 9s Pan-Cancer Analysis},
	urldate = {2017-11-03},
	journal = {bioRxiv},
	author = {Yung, C K and O'Connor, B D and Yakneen, S and Zhang, J and Ellrott, K and others},
	year = {2017},
	keywords = {⛔ No DOI found},
}

@article{van_neste_forensic_2015,
	title = {Forensic massively parallel sequencing data analysis tool: {Implementation} of {MyFLq} as a standalone web- and {Illumina} {BaseSpace}(®)-application.},
	volume = {15},
	url = {http://dx.doi.org/10.1016/j.fsigen.2014.10.006},
	doi = {10.1016/j.fsigen.2014.10.006},
	abstract = {Routine use of massively parallel sequencing (MPS) for forensic genomics is on the horizon. The last few years, several algorithms and workflows have been developed to analyze forensic MPS data. However, none have yet been tailored to the needs of the forensic analyst who does not possess an extensive bioinformatics background. We developed our previously published forensic MPS data analysis framework MyFLq (My-Forensic-Loci-queries) into an open-source, user-friendly, web-based application. It can be installed as a standalone web application, or run directly from the Illumina BaseSpace environment. In the former, laboratories can keep their data on-site, while in the latter, data from forensic samples that are sequenced on an Illumina sequencer can be uploaded to Basespace during acquisition, and can subsequently be analyzed using the published MyFLq BaseSpace application. Additional features were implemented such as an interactive graphical report of the results, an interactive threshold selection bar, and an allele length-based analysis in addition to the sequenced-based analysis. Practical use of the application is demonstrated through the analysis of four 16-plex short tandem repeat (STR) samples, showing the complementarity between the sequence- and length-based analysis of the same MPS data. Copyright {\textbackslash}copyright 2014 The Authors. Published by Elsevier Ireland Ltd.. All rights reserved.},
	urldate = {2017-11-03},
	journal = {Forensic science international. Genetics},
	author = {Van Neste, Christophe and Gansemans, Yannick and De Coninck, Dieter and Van Hoofstat, David and Van Criekinge, Wim and Deforce, Dieter and Van Nieuwerburgh, Filip},
	month = mar,
	year = {2015},
	pmid = {25457631},
	pages = {2--7},
}

@article{marcus_credibility_2015,
	title = {Credibility and reproducibility.},
	volume = {22},
	url = {http://dx.doi.org/10.1016/j.chembiol.2014.12.008},
	doi = {10.1016/j.chembiol.2014.12.008},
	number = {1},
	urldate = {2017-11-03},
	journal = {Chemistry \& Biology},
	author = {Marcus, Emilie},
	month = jan,
	year = {2015},
	pmid = {25615949},
	pages = {3--4},
}

@book{nedeljkovic_cwl-svg_nodate,
	title = {{CWL}-svg: an open-source workflow visualization library for the {Common}},
	url = {https://github.com/rabix/cwl-svg},
	urldate = {2017-11-03},
	author = {Nedeljkovic, Maja and Pajic, Boban and Batic, Ivan and Sharma, Adrian and Kaushik, Gaurav and Davis-Dusenbery, Brandi},
}

@article{amstutz_portable_2015,
	title = {Portable, {Reproducible} {Analysis} with {Arvados}},
	volume = {4},
	urldate = {2017-09-07},
	journal = {F1000Research},
	author = {Amstutz, Peter},
	month = jul,
	year = {2015},
	keywords = {⛔ No DOI found},
}

@article{xu_fdas_2016,
	title = {The {FDA}'s {Experience} with {Emerging} {Genomics} {Technologies}-{Past}, {Present}, and {Future}.},
	volume = {18},
	url = {http://dx.doi.org/10.1208/s12248-016-9917-y},
	doi = {10.1208/s12248-016-9917-y},
	abstract = {The rapid advancement of emerging genomics technologies and their application for assessing safety and efficacy of FDA-regulated products require a high standard of reliability and robustness supporting regulatory decision-making in the FDA. To facilitate the regulatory application, the FDA implemented a novel data submission program, Voluntary Genomics Data Submission (VGDS), and also to engage the stakeholders. As part of the endeavor, for the past 10 years, the FDA has led an international consortium of regulatory agencies, academia, pharmaceutical companies, and genomics platform providers, which was named MicroArray Quality Control Consortium (MAQC), to address issues such as reproducibility, precision, specificity/sensitivity, and data interpretation. Three projects have been completed so far assessing these genomics technologies: gene expression microarrays, whole genome genotyping arrays, and whole transcriptome sequencing (i.e., RNA-seq). The resultant studies provide the basic parameters for fit-for-purpose application of these new data streams in regulatory environments, and the solutions have been made available to the public through peer-reviewed publications. The latest MAQC project is also called the SEquencing Quality Control (SEQC) project focused on next-generation sequencing. Using reference samples with built-in controls, SEQC studies have demonstrated that relative gene expression can be measured accurately and reliably across laboratories and RNA-seq platforms. Besides prediction performance comparable to microarrays in clinical settings and safety assessments, RNA-seq is shown to have better sensitivity for low expression and reveal novel transcriptomic features. Future effort of MAQC will be focused on quality control of whole genome sequencing and targeted sequencing.},
	number = {4},
	urldate = {2017-11-03},
	journal = {The AAPS Journal},
	author = {Xu, Joshua and Thakkar, Shraddha and Gong, Binsheng and Tong, Weida},
	month = apr,
	year = {2016},
	pmid = {27116022},
	pmcid = {PMC4973466},
	pages = {814--818},
}

@book{simonovic_rabix_nodate,
	title = {Rabix {Executor}: an open-source executor supporting the recomputability and},
	url = {https://github.com/rabix/bunny},
	urldate = {2017-11-03},
	author = {Simonovic, Janko and Kaushik, Gaurav and Ivkovic, Sinisa and Stojanovic, Luka and Tijanic, Nebojsa and Sharma, Adrian and Davis-Dusenbery, Brandi},
}

@book{amstutz_common-workflow-languagecwltool_2017,
	address = {GitHub},
	title = {common-workflow-language/cwltool},
	url = {https://github.com/common-workflow-language/cwltool/releases/tag/1.0.20170828135420},
	urldate = {2017-08-28},
	publisher = {GitHub},
	author = {Amstutz, Peter and Crusoe, Michael R and Singh, Manvendra and Kumar, Kapil and Chilton, John and Boysha and Soiland-Reyes, Stian and Chapman, Brad and Kotliar, Michael and Leehr, Dan and Carrasco, Guillermo and Kartashov, Andrey and Tijanic, Nebojsa and Ménager, Hervé and Safont, Pau Ruiz and Porter, James J and Molenaar, Gijs and Yuen, Denis and Barrera, Alejandro and Ivkovic, Sinisa and Spangler, Ryan and psaffrey-illumina and Tanjo, Tomoya and Vandewege, Ward and Randall, Joshua C and Kern, John and Bradley, John and Li, Jiayong and der Zwaan, Janneke van and Connelly, Abram},
	year = {2017},
}

@book{preston-werner_semantic_2013,
	title = {Semantic {Versioning} 2.0.0},
	url = {http://semver.org/spec/v2.0.0.html},
	urldate = {2017-09-07},
	author = {Preston-Werner, Tom},
	month = jun,
	year = {2013},
}

@book{noauthor_stellaris_nodate,
	title = {Stellaris on {Steam}},
	url = {http://store.steampowered.com/app/281990/Stellaris/},
	abstract = {Explore a vast galaxy full of wonder! Paradox Development Studio, makers},
	urldate = {2017-09-07},
}

@book{ssi_top_2016,
	title = {Top tips},
	url = {https://www.software.ac.uk/resources/top-tips},
	urldate = {2017-09-07},
	author = {{\textbackslash}SSI},
	year = {2016},
}

@article{amstutz_using_2016,
	title = {Using the {Common} {Workflow} {Language} ({CWL}) to run portable workflows with},
	volume = {5},
	url = {http://dx.doi.org/10.7490/f1000research.1112523.1},
	abstract = {Read this work by Amstutz P, at F1000Research.},
	urldate = {2017-09-07},
	journal = {F1000Res.},
	author = {Amstutz, Peter and Zaranek, Alexander Wait and O'Connor, Brian and Paten, Benedict},
	month = jul,
	year = {2016},
	keywords = {⛔ No DOI found},
}

@article{kanwal_investigating_2017,
	title = {Investigating reproducibility and tracking provenance - {A} genomic workflow case study.},
	volume = {18},
	url = {http://dx.doi.org/10.1186/s12859-017-1747-0},
	doi = {10.1186/s12859-017-1747-0},
	abstract = {BACKGROUND: Computational bioinformatics workflows are extensively used to analyse genomics data, with different approaches available to support implementation and execution of these workflows. Reproducibility is one of the core principles for any scientific workflow and remains a challenge, which is not fully addressed. This is due to incomplete understanding of reproducibility requirements and assumptions of workflow definition approaches. Provenance information should be tracked and used to capture all these requirements supporting reusability of existing workflows. RESULTS: We have implemented a complex but widely deployed bioinformatics workflow using three representative approaches to workflow definition and execution. Through implementation, we identified assumptions implicit in these approaches that ultimately produce insufficient documentation of workflow requirements resulting in failed execution of the workflow. This study proposes a set of recommendations that aims to mitigate these assumptions and guides the scientific community to accomplish reproducible science, hence addressing reproducibility crisis. CONCLUSIONS: Reproducing, adapting or even repeating a bioinformatics workflow in any environment requires substantial technical knowledge of the workflow execution environment, resolving analysis assumptions and rigorous compliance with reproducibility requirements. Towards these goals, we propose conclusive recommendations that along with an explicit declaration of workflow specification would result in enhanced reproducibility of computational genomic analyses.},
	number = {1},
	urldate = {2017-11-03},
	journal = {BMC Bioinformatics},
	author = {Kanwal, Sehrish and Khan, Farah Zaib and Lonie, Andrew and Sinnott, Richard O},
	month = jul,
	year = {2017},
	pmid = {28701218},
	pmcid = {PMC5508699},
	pages = {337},
}

@book{van_der_zwaan_flexible_nodate,
	title = {Flexible {NLP} {Pipelines} for {Digital} {Humanities} {Research}},
	url = {https://dh2017.adho.org/abstracts/215/215.pdf},
	urldate = {2017-11-03},
	author = {van der Zwaan, Janneke M and Smink, Wouter and Sools, Anneke and Westerhof, Gerben and Veldkamp, Bernard and Wiegersma, Sytske},
}

@book{batic_rabix_nodate,
	title = {Rabix {Composer}: an open-source integrated development environment for the},
	url = {https://github.com/rabix/composer},
	urldate = {2017-11-03},
	author = {Batic, Ivan and Nedeljkovic, Maja and Pajic, Boban and Tijanic, Nebojsa and Stojanovic, Luka and Lekic, Marijan and Sharma, Adrian and Kaushik, Gaurav and Davis-Dusenbery, Brandi Davis-Dusenbery},
}

@article{prins_developing_2015,
	title = {Developing an {Arvados} {BWA}-{GATK} pipeline},
	volume = {4},
	url = {http://dx.doi.org/10.7490/f1000research.1110026.1},
	abstract = {Read this work by Prins P, at F1000Research.},
	urldate = {2017-09-07},
	journal = {F1000Res.},
	author = {Prins, Pjotr and de Ligt, Joep and Nijman, Isaac J and Amstutz, Peter and Cosca, Bryan and Vandewege, Ward},
	month = jul,
	year = {2015},
	keywords = {⛔ No DOI found},
}

@article{williams_running_2014,
	title = {Running {Taverna} {Workflows} within {IPython} {Notebook}},
	url = {https://zenodo.org/record/11360#.WcokBsiGNPY},
	urldate = {2017-09-26},
	author = {Williams and Pawlik and Goble},
	month = jul,
	year = {2014},
	keywords = {⛔ No DOI found},
}

@book{robinson_common-workflow-languagecwlviewer_2017,
	title = {common-workflow-language/cwlviewer: {CWL} {Viewer}},
	url = {https://doi.org/10.5281/zenodo.848163},
	urldate = {2017-11-03},
	author = {Robinson, Mark and Soiland-Reyes, Stian and Crusoe, Michael R},
	month = aug,
	year = {2017},
}

@article{fjukstad_review_2017,
	title = {A review of scalable bioinformatics pipelines},
	volume = {2},
	issn = {2364-1185},
	url = {http://link.springer.com/10.1007/s41019-017-0047-z},
	doi = {10.1007/s41019-017-0047-z},
	abstract = {Scalability is increasingly important for bioinformatics analysis services, since these must handle larger datasets, more jobs, and more users. The pipelines used to implement analyses must therefore scale with respect to the resources on a single compute node, the number of nodes on a cluster, and also to cost-performance. Here, we survey several scalable bioinformatics pipelines and compare their design and their use of underlying frameworks and infrastructures. We also discuss current trends for bioinformatics pipeline development.},
	number = {3},
	urldate = {2017-10-30},
	journal = {Data Science and Engineering},
	author = {Fjukstad, Bjørn and Bongo, Lars Ailo},
	month = oct,
	year = {2017},
	pages = {245--251},
}

@article{schulz_use_2016,
	title = {Use of application containers and workflows for genomic data analysis.},
	volume = {7},
	url = {http://dx.doi.org/10.4103/2153-3539.197197},
	doi = {10.4103/2153-3539.197197},
	abstract = {BACKGROUND: The rapid acquisition of biological data and development of computationally intensive analyses has led to a need for novel approaches to software deployment. In particular, the complexity of common analytic tools for genomics makes them difficult to deploy and decreases the reproducibility of computational experiments. METHODS: Recent technologies that allow for application virtualization, such as Docker, allow developers and bioinformaticians to isolate these applications and deploy secure, scalable platforms that have the potential to dramatically increase the efficiency of big data processing. RESULTS: While limitations exist, this study demonstrates a successful implementation of a pipeline with several discrete software applications for the analysis of next-generation sequencing (NGS) data. CONCLUSIONS: With this approach, we significantly reduced the amount of time needed to perform clonal analysis from NGS data in acute myeloid leukemia.},
	urldate = {2017-11-03},
	journal = {Journal of pathology informatics},
	author = {Schulz, Wade L and Durant, Thomas J S and Siddon, Alexa J and Torres, Richard},
	month = dec,
	year = {2016},
	pmid = {28163975},
	pmcid = {PMC5248400},
	pages = {53},
}

@book{robinson_reproducible_2017,
	title = {Reproducible {Research} using {Research} {Objects}},
	url = {https://doi.org/10.5281/zenodo.823295},
	abstract = {Scientific workflows play a crucial role in conducting large scale scientific experiments and enabling them to be easily reproducible. However, it is vital that these are specified well with useful metadata for consumption by applications in order to make sharing them simple and convenient.This project focuses on two up-and-coming standards - the Common Workflow Language and Research Objects - which complement each other to achieve this goal and thus aid in reproducible and transparent research. In order to unite these standards, this report concerns the planning, development and evaluation process of a web application, ‘CWL Viewer’, to allow the sharing of workflows written in the Common Workflow Language.This is accomplished by providing visualisation and summary of important details as well as a downloadable Research Object Bundle designed to provide metadata relevant to workflow management and other applications.},
	urldate = {2017-09-18},
	publisher = {The University of Manchester},
	author = {Robinson, Mark},
	month = may,
	year = {2017},
	keywords = {cwl},
}

@article{yakneen_enabling_2017,
	title = {Enabling rapid cloud-based analysis of thousands of human genomes via {Butler}},
	url = {http://biorxiv.org/lookup/doi/10.1101/185736},
	doi = {10.1101/185736},
	abstract = {We present Butler, a computational framework developed in the context of the international Pan-cancer Analysis of Whole Genomes (PCAWG) project to overcome the challenges of orchestrating analyses of thousands of human genomes on the cloud. Butler operates equally well on public and academic clouds. This highly flexible framework facilitates management of virtual cloud infrastructure, software configuration, genomics workflow development, and provides unique capabilities in workflow execution management. By comprehensively collecting and analysing metrics and logs, performing anomaly detection as well as notification and cluster self-healing, Butler enables large-scale analytical processing of human genomes with 43\% increased throughput compared to prior setups. Butler was key for delivering the germline genetic variant call-sets in 2,834 cancer genomes analysed by PCAWG.},
	urldate = {2017-09-08},
	journal = {BioRxiv},
	author = {Yakneen, Sergei and Waszak, Sebastian and Gertz, Michael and Korbel, Jan O. and Group, PCAWG Germline Cancer Genome Working and Group, PCAWG Technical Working and Net, ICGC/TCGA Pan-Cancer Analysis of Whole Genomes},
	month = sep,
	year = {2017},
}

@article{cohen-boulakia_scientific_2017,
	title = {Scientific workflows for computational reproducibility in the life sciences: {Status}, challenges and opportunities},
	volume = {75},
	issn = {0167739X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0167739X17300316},
	doi = {10.1016/j.future.2017.01.012},
	abstract = {With the development of new experimental technologies, biologists are faced with an avalanche of data to be computationally analyzed for scientific advancements and discoveries to emerge. Faced with the complexity of analysis pipelines, the large number of computational tools, and the enormous amount of data to manage, there is compelling evidence that many if not most scientific discoveries will not stand the test of time: increasing the reproducibility of computed results is of paramount importance. The objective we set out in this paper is to place scientific workflows in the context of reproducibility. To do so, we define several kinds of repro-ducibility that can be reached when scientific workflows are used to perform experiments. We characterize and define the criteria that need to be catered for by reproducibility-friendly scientific workflow systems, and use such criteria to place several representative and widely used workflow systems and companion tools within such a framework. We also discuss the remaining challenges posed by reproducible scientific workflows in the life sciences. Our study was guided by three use cases from the life science domain involving in silico experiments.},
	urldate = {2018-07-13},
	journal = {Future Generation Computer Systems},
	author = {Cohen-Boulakia, Sarah and Belhajjame, Khalid and Collin, Olivier and Chopard, Jérôme and Froidevaux, Christine and Gaignard, Alban and Hinsen, Konrad and Larmande, Pierre and Bras, Yvan Le and Lemoine, Frédéric and Mareuil, Fabien and Ménager, Hervé and Pradal, Christophe and Blanchet, Christophe},
	month = oct,
	year = {2017},
	pages = {284--298},
}

@article{jongeneel_assessing_2017,
	title = {Assessing computational genomics skills: {Our} experience in the {H3ABioNet} {African} bioinformatics network.},
	volume = {13},
	url = {http://dx.doi.org/10.1371/journal.pcbi.1005419},
	doi = {10.1371/journal.pcbi.1005419},
	abstract = {The H3ABioNet pan-African bioinformatics network, which is funded to support the Human Heredity and Health in Africa (H3Africa) program, has developed node-assessment exercises to gauge the ability of its participating research and service groups to analyze typical genome-wide datasets being generated by H3Africa research groups. We describe a framework for the assessment of computational genomics analysis skills, which includes standard operating procedures, training and test datasets, and a process for administering the exercise. We present the experiences of 3 research groups that have taken the exercise and the impact on their ability to manage complex projects. Finally, we discuss the reasons why many H3ABioNet nodes have declined so far to participate and potential strategies to encourage them to do so.},
	number = {6},
	urldate = {2017-11-03},
	journal = {PLoS Computational Biology},
	author = {Jongeneel, C Victor and Achinike-Oduaran, Ovokeraye and Adebiyi, Ezekiel and Adebiyi, Marion and Adeyemi, Seun and Akanle, Bola and Aron, Shaun and Ashano, Efejiro and Bendou, Hocine and Botha, Gerrit and Chimusa, Emile and Choudhury, Ananyo and Donthu, Ravikiran and Drnevich, Jenny and Falola, Oluwadamila and Fields, Christopher J and Hazelhurst, Scott and Hendry, Liesl and Isewon, Itunuoluwa and Khetani, Radhika S and Kumuthini, Judit and Kimuda, Magambo Phillip and Magosi, Lerato and Mainzer, Liudmila Sergeevna and Maslamoney, Suresh and Mbiyavanga, Mamana and Meintjes, Ayton and Mugutso, Danny and Mpangase, Phelelani and Munthali, Richard and Nembaware, Victoria and Ndhlovu, Andrew and Odia, Trust and Okafor, Adaobi and Oladipo, Olaleye and Panji, Sumir and Pillay, Venesa and Rendon, Gloria and Sengupta, Dhriti and Mulder, Nicola},
	month = jun,
	year = {2017},
	pmid = {28570565},
	pmcid = {PMC5453403},
	pages = {e1005419},
}

@article{moller_community-driven_2014,
	title = {Community-driven development for computational biology at {Sprints}, {Hackathons} and {Codefests}.},
	volume = {15 Suppl 14},
	url = {http://dx.doi.org/10.1186/1471-2105-15-S14-S7},
	doi = {10.1186/1471-2105-15-S14-S7},
	abstract = {BACKGROUND: Computational biology comprises a wide range of technologies and approaches. Multiple technologies can be combined to create more powerful workflows if the individuals contributing the data or providing tools for its interpretation can find mutual understanding and consensus. Much conversation and joint investigation are required in order to identify and implement the best approaches. Traditionally, scientific conferences feature talks presenting novel technologies or insights, followed up by informal discussions during coffee breaks. In multi-institution collaborations, in order to reach agreement on implementation details or to transfer deeper insights in a technology and practical skills, a representative of one group typically visits the other. However, this does not scale well when the number of technologies or research groups is large. Conferences have responded to this issue by introducing Birds-of-a-Feather (BoF) sessions, which offer an opportunity for individuals with common interests to intensify their interaction. However, parallel BoF sessions often make it hard for participants to join multiple BoFs and find common ground between the different technologies, and BoFs are generally too short to allow time for participants to program together. RESULTS: This report summarises our experience with computational biology Codefests, Hackathons and Sprints, which are interactive developer meetings. They are structured to reduce the limitations of traditional scientific meetings described above by strengthening the interaction among peers and letting the participants determine the schedule and topics. These meetings are commonly run as loosely scheduled "unconferences" (self-organized identification of participants and topics for meetings) over at least two days, with early introductory talks to welcome and organize contributors, followed by intensive collaborative coding sessions. We summarise some prominent achievements of those meetings and describe differences in how these are organised, how their audience is addressed, and their outreach to their respective communities. CONCLUSIONS: Hackathons, Codefests and Sprints share a stimulating atmosphere that encourages participants to jointly brainstorm and tackle problems of shared interest in a self-driven proactive environment, as well as providing an opportunity for new participants to get involved in collaborative projects.},
	urldate = {2017-08-16},
	journal = {BMC Bioinformatics},
	author = {Möller, Steffen and Afgan, Enis and Banck, Michael and Bonnal, Raoul J P and Booth, Timothy and Chilton, John and Cock, Peter J A and Gumbel, Markus and Harris, Nomi and Holland, Richard and Kalaš, Matúš and Kaján, László and Kibukawa, Eri and Powel, David R and Prins, Pjotr and Quinn, Jacqueline and Sallou, Olivier and Strozzi, Francesco and Seemann, Torsten and Sloggett, Clare and Soiland-Reyes, Stian and Spooner, William and Steinbiss, Sascha and Tille, Andreas and Travis, Anthony J and Guimera, Roman and Katayama, Toshiaki and Chapman, Brad A},
	month = nov,
	year = {2014},
	pmid = {25472764},
	pmcid = {PMC4255748},
	pages = {S7},
}

@article{kurtzer_singularity_2017,
	title = {Singularity: {Scientific} containers for mobility of compute.},
	volume = {12},
	url = {http://dx.doi.org/10.1371/journal.pone.0177459},
	doi = {10.1371/journal.pone.0177459},
	abstract = {Here we present Singularity, software developed to bring containers and reproducibility to scientific computing. Using Singularity containers, developers can work in reproducible environments of their choosing and design, and these complete environments can easily be copied and executed on other platforms. Singularity is an open source initiative that harnesses the expertise of system and software engineers and researchers alike, and integrates seamlessly into common workflows for both of these groups. As its primary use case, Singularity brings mobility of computing to both users and HPC centers, providing a secure means to capture and distribute software and compute environments. This ability to create and deploy reproducible environments across these centers, a previously unmet need, makes Singularity a game changing development for computational science.},
	number = {5},
	urldate = {2017-11-03},
	journal = {Plos One},
	author = {Kurtzer, Gregory M and Sochat, Vanessa and Bauer, Michael W},
	month = may,
	year = {2017},
	pmid = {28494014},
	pmcid = {PMC5426675},
	pages = {e0177459},
}

@article{guimera_bcbio-nextgen_2012,
	title = {bcbio-nextgen: {Automated}, distributed next-gen sequencing pipeline},
	volume = {17},
	issn = {2226-6089},
	url = {http://journal.embnet.org/index.php/embnetjournal/article/view/286},
	doi = {10.14806/ej.17.B.286},
	number = {B},
	urldate = {2017-09-07},
	journal = {EMBnet.journal},
	author = {Guimera, Roman Valls},
	month = feb,
	year = {2012},
	pages = {30},
}

@article{taschuk_ten_2017,
	title = {Ten simple rules for making research software more robust.},
	volume = {13},
	url = {http://dx.doi.org/10.1371/journal.pcbi.1005412},
	doi = {10.1371/journal.pcbi.1005412},
	abstract = {Software produced for research, published and otherwise, suffers from a number of common problems that make it difficult or impossible to run outside the original institution or even off the primary developer's computer. We present ten simple rules to make such software robust enough to be run by anyone, anywhere, and thereby delight your users and collaborators.},
	number = {4},
	urldate = {2017-11-03},
	journal = {PLoS Computational Biology},
	author = {Taschuk, Morgan and Wilson, Greg},
	month = apr,
	year = {2017},
	pmid = {28407023},
	pmcid = {PMC5390961},
	pages = {e1005412},
}

@article{henry_omictools_2014,
	title = {{OMICtools}: an informative directory for multi-omic data analysis.},
	volume = {2014},
	url = {http://dx.doi.org/10.1093/database/bau069},
	doi = {10.1093/database/bau069},
	abstract = {Recent advances in 'omic' technologies have created unprecedented opportunities for biological research, but current software and database resources are extremely fragmented. OMICtools is a manually curated metadatabase that provides an overview of more than 4400 web-accessible tools related to genomics, transcriptomics, proteomics and metabolomics. All tools have been classified by omic technologies (next-generation sequencing, microarray, mass spectrometry and nuclear magnetic resonance) associated with published evaluations of tool performance. Information about each tool is derived either from a diverse set of developers, the scientific literature or from spontaneous submissions. OMICtools is expected to serve as a useful didactic resource not only for bioinformaticians but also for experimental researchers and clinicians. Database URL: http://omictools.com/. {\textbackslash}copyright The Author(s) 2014. Published by Oxford University Press.},
	urldate = {2017-11-03},
	journal = {Database: the Journal of Biological Databases and Curation},
	author = {Henry, Vincent J and Bandrowski, Anita E and Pepin, Anne-Sophie and Gonzalez, Bruno J and Desfeux, Arnaud},
	month = jul,
	year = {2014},
	pmid = {25024350},
	pmcid = {PMC4095679},
}

@article{oconnor_dockstore_2017,
	title = {The {Dockstore}: enabling modular, community-focused sharing of {Docker}-based genomics tools and workflows.},
	volume = {6},
	url = {http://dx.doi.org/10.12688/f1000research.10137.1},
	doi = {10.12688/f1000research.10137.1},
	abstract = {As genomic datasets continue to grow, the feasibility of downloading data to a local organization and running analysis on a traditional compute environment is becoming increasingly problematic. Current large-scale projects, such as the ICGC PanCancer Analysis of Whole Genomes (PCAWG), the Data Platform for the U.S. Precision Medicine Initiative, and the NIH Big Data to Knowledge Center for Translational Genomics, are using cloud-based infrastructure to both host and perform analysis across large data sets. In PCAWG, over 5,800 whole human genomes were aligned and variant called across 14 cloud and HPC environments; the processed data was then made available on the cloud for further analysis and sharing. If run locally, an operation at this scale would have monopolized a typical academic data centre for many months, and would have presented major challenges for data storage and distribution. However, this scale is increasingly typical for genomics projects and necessitates a rethink of how analytical tools are packaged and moved to the data. For PCAWG, we embraced the use of highly portable Docker images for encapsulating and sharing complex alignment and variant calling workflows across highly variable environments. While successful, this endeavor revealed a limitation in Docker containers, namely the lack of a standardized way to describe and execute the tools encapsulated inside the container. As a result, we created the Dockstore ( https://dockstore.org), a project that brings together Docker images with standardized, machine-readable ways of describing and running the tools contained within. This service greatly improves the sharing and reuse of genomics tools and promotes interoperability with similar projects through emerging web service standards developed by the Global Alliance for Genomics and Health (GA4GH).},
	urldate = {2018-07-13},
	journal = {F1000Research},
	author = {O'Connor, Brian D and Yuen, Denis and Chung, Vincent and Duncan, Andrew G and Liu, Xiang Kun and Patricia, Janice and Paten, Benedict and Stein, Lincoln and Ferretti, Vincent},
	month = jan,
	year = {2017},
	pmid = {28344774},
	pmcid = {PMC5333608},
	pages = {52},
}

@article{di_tommaso_nextflow_2017,
	title = {Nextflow enables reproducible computational workflows.},
	volume = {35},
	url = {http://dx.doi.org/10.1038/nbt.3820},
	doi = {10.1038/nbt.3820},
	number = {4},
	urldate = {2018-07-13},
	journal = {Nature Biotechnology},
	author = {Di Tommaso, Paolo and Chatzou, Maria and Floden, Evan W and Barja, Pablo Prieto and Palumbo, Emilio and Notredame, Cedric},
	month = apr,
	year = {2017},
	pmid = {28398311},
	pages = {316--319},
}

@article{vivian_toil_2017,
	title = {Toil enables reproducible, open source, big biomedical data analyses.},
	volume = {35},
	url = {http://dx.doi.org/10.1038/nbt.3772},
	doi = {10.1038/nbt.3772},
	number = {4},
	urldate = {2017-11-03},
	journal = {Nature Biotechnology},
	author = {Vivian, John and Rao, Arjun Arkal and Nothaft, Frank Austin and Ketchum, Christopher and Armstrong, Joel and Novak, Adam and Pfeil, Jacob and Narkizian, Jake and Deran, Alden D and Musselman-Brown, Audrey and Schmidt, Hannes and Amstutz, Peter and Craft, Brian and Goldman, Mary and Rosenbloom, Kate and Cline, Melissa and O'Connor, Brian and Hanna, Megan and Birger, Chet and Kent, W James and Patterson, David A and Joseph, Anthony D and Zhu, Jingchun and Zaranek, Sasha and Getz, Gad and Haussler, David and Paten, Benedict},
	month = apr,
	year = {2017},
	pmid = {28398314},
	pmcid = {PMC5546205},
	pages = {314--316},
}

@article{stodden_enhancing_2016,
	title = {Enhancing reproducibility for computational methods.},
	volume = {354},
	issn = {0036-8075},
	url = {http://www.sciencemag.org/cgi/doi/10.1126/science.aah6168},
	doi = {10.1126/science.aah6168},
	number = {6317},
	urldate = {2018-07-23},
	journal = {Science},
	author = {Stodden, Victoria and McNutt, Marcia and Bailey, David H and Deelman, Ewa and Gil, Yolanda and Hanson, Brooks and Heroux, Michael A and Ioannidis, John P A and Taufer, Michela},
	month = dec,
	year = {2016},
	pmid = {27940837},
	pages = {1240--1241},
}

@article{artaza_top_2016,
	title = {Top 10 metrics for life science software good practices.},
	volume = {5},
	url = {http://dx.doi.org/10.12688/f1000research.9206.1},
	doi = {10.12688/f1000research.9206.1},
	abstract = {Metrics for assessing adoption of good development practices are a useful way to ensure that software is sustainable, reusable and functional. Sustainability means that the software used today will be available - and continue to be improved and supported - in the future. We report here an initial set of metrics that measure good practices in software development. This initiative differs from previously developed efforts in being a community-driven grassroots approach where experts from different organisations propose good software practices that have reasonable potential to be adopted by the communities they represent. We not only focus our efforts on understanding and prioritising good practices, we assess their feasibility for implementation and publish them here.},
	urldate = {2017-11-03},
	journal = {F1000Research},
	author = {Artaza, Haydee and Chue Hong, Neil and Corpas, Manuel and Corpuz, Angel and Hooft, Rob and Jimenez, Rafael C and Leskošek, Brane and Olivier, Brett G and Stourac, Jan and Svobodová Vařeková, Radka and Van Parys, Thomas and Vaughan, Daniel},
	month = aug,
	year = {2016},
	pmid = {27635232},
	pmcid = {PMC5007752},
}

@article{moreews_bioshadock_2015,
	title = {{BioShaDock}: a community driven bioinformatics shared {Docker}-based tools registry.},
	volume = {4},
	url = {http://dx.doi.org/10.12688/f1000research.7536.1},
	doi = {10.12688/f1000research.7536.1},
	abstract = {Linux container technologies, as represented by Docker, provide an alternative to complex and time-consuming installation processes needed for scientiﬁc software. The ease of deployment and the process isolation they enable, as well as the reproducibility they permit across environments and versions, are among the qualities that make them interesting candidates for the construction of bioinformatic infrastructures, at any scale from single workstations to high throughput computing architectures. The Docker Hub is a public registry which can be used to distribute bioinformatic software as Docker images. However, its lack of curation and its genericity make it difﬁcult for a bioinformatics user to ﬁnd the most appropriate images needed. BioShaDock is a bioinformatics-focused Docker registry, which provides a local and fully controlled environment to build and publish bioinformatic software as portable Docker images. It provides a number of improvements over the base Docker registry on authentication and permissions management, that enable its integration in existing bioinformatic infrastructures such as computing platforms. The metadata associated with the registered images are domain-centric, including for instance concepts deﬁned in the EDAM ontology, a shared and structured vocabulary of commonly used terms in bioinformatics. The registry also includes user deﬁned tags to facilitate its discovery, as well as a link to the tool description in the ELIXIR registry if it already exists. If it does not, the BioShaDock registry will synchronize with the registry to create a new description in the Elixir registry, based on the BioShaDock entry metadata. This link will help users get more information on the tool such as its EDAM operations, input and output types. This allows integration with the ELIXIR Tools and Data Services Registry, thus providing the appropriate visibility of such images to the bioinformatics community.},
	urldate = {2017-11-03},
	journal = {F1000Research},
	author = {Moreews, François and Sallou, Olivier and Ménager, Hervé and Le Bras, Yvan and Monjeaud, Cyril and Blanchet, Christophe and Collin, Olivier},
	month = dec,
	year = {2015},
	pmid = {26913191},
	pmcid = {PMC4743153},
	pages = {1443},
}

@article{kaushik_rabix_2017,
	title = {Rabix: an open-source workflow executor supporting recomputability and interoperability of workflow descriptions.},
	volume = {22},
	url = {http://dx.doi.org/10.1142/9789813207813_0016},
	doi = {10.1142/9789813207813_0016},
	abstract = {As biomedical data has become increasingly easy to generate in large quantities, the methods used to analyze it have proliferated rapidly. Reproducible and reusable methods are required to learn from large volumes of data reliably. To address this issue, numerous groups have developed workflow specifications or execution engines, which provide a framework with which to perform a sequence of analyses. One such specification is the Common Workflow Language, an emerging standard which provides a robust and flexible framework for describing data analysis tools and workflows. In addition, reproducibility can be furthered by executors or workflow engines which interpret the specification and enable additional features, such as error logging, file organization, optim1izations to computation and job scheduling, and allow for easy computing on large volumes of data. To this end, we have developed the Rabix Executor, an open-source workflow engine for the purposes of improving reproducibility through reusability and interoperability of workflow descriptions.},
	urldate = {2017-11-03},
	journal = {Pacific Symposium on Biocomputing},
	author = {Kaushik, Gaurav and Ivkovic, Sinisa and Simonovic, Janko and Tijanic, Nebojsa and Davis-Dusenbery, Brandi and Kural, Deniz},
	year = {2017},
	pmid = {27896971},
	pmcid = {PMC5166558},
	pages = {154--165},
}

@article{shanahan_bioinformatics_2014,
	title = {Bioinformatics on the cloud computing platform {Azure}.},
	volume = {9},
	url = {http://dx.doi.org/10.1371/journal.pone.0102642},
	doi = {10.1371/journal.pone.0102642},
	abstract = {We discuss the applicability of the Microsoft cloud computing platform, Azure, for bioinformatics. We focus on the usability of the resource rather than its performance. We provide an example of how R can be used on Azure to analyse a large amount of microarray expression data deposited at the public database ArrayExpress. We provide a walk through to demonstrate explicitly how Azure can be used to perform these analyses in Appendix S1 and we offer a comparison with a local computation. We note that the use of the Platform as a Service (PaaS) offering of Azure can represent a steep learning curve for bioinformatics developers who will usually have a Linux and scripting language background. On the other hand, the presence of an additional set of libraries makes it easier to deploy software in a parallel (scalable) fashion and explicitly manage such a production run with only a few hundred lines of code, most of which can be incorporated from a template. We propose that this environment is best suited for running stable bioinformatics software by users not involved with its development.},
	number = {7},
	urldate = {2017-11-03},
	journal = {Plos One},
	author = {Shanahan, Hugh P and Owen, Anne M and Harrison, Andrew P},
	month = jul,
	year = {2014},
	pmid = {25050811},
	pmcid = {PMC4106841},
	pages = {e102642},
}

@article{irwin_community_2008,
	title = {Community benchmarks for virtual screening.},
	volume = {22},
	url = {http://dx.doi.org/10.1007/s10822-008-9189-4},
	doi = {10.1007/s10822-008-9189-4},
	abstract = {Ligand enrichment among top-ranking hits is a key metric of virtual screening. To avoid bias, decoys should resemble ligands physically, so that enrichment is not attributable to simple differences of gross features. We therefore created a directory of useful decoys (DUD) by selecting decoys that resembled annotated ligands physically but not topologically to benchmark docking performance. DUD has 2950 annotated ligands and 95,316 property-matched decoys for 40 targets. It is by far the largest and most comprehensive public data set for benchmarking virtual screening programs that I am aware of. This paper outlines several ways that DUD can be improved to provide better telemetry to investigators seeking to understand both the strengths and the weaknesses of current docking methods. I also highlight several pitfalls for the unwary: a risk of over-optimization, questions about chemical space, and the proper scope for using DUD. Careful attention to both the composition of benchmarks and how they are used is essential to avoid being misled by overfitting and bias.},
	number = {3-4},
	urldate = {2017-11-03},
	journal = {Journal of Computer-Aided Molecular Design},
	author = {Irwin, John J},
	month = apr,
	year = {2008},
	pmid = {18273555},
	pages = {193--199},
}

@article{perez-riverol_ten_2016,
	title = {Ten simple rules for taking advantage of git and github.},
	volume = {12},
	issn = {1553-7358},
	url = {http://dx.plos.org/10.1371/journal.pcbi.1004947},
	doi = {10.1371/journal.pcbi.1004947},
	number = {7},
	urldate = {2017-11-03},
	journal = {PLoS Computational Biology},
	author = {Perez-Riverol, Yasset and Gatto, Laurent and Wang, Rui and Sachsenberg, Timo and Uszkoreit, Julian and Leprevost, Felipe da Veiga and Fufezan, Christian and Ternent, Tobias and Eglen, Stephen J and Katz, Daniel S and Pollard, Tom J and Konovalov, Alexander and Flight, Robert M and Blin, Kai and Vizcaíno, Juan Antonio},
	month = jul,
	year = {2016},
	pmid = {27415786},
	pmcid = {PMC4945047},
	pages = {e1004947},
}

@incollection{berthold_knime_2008,
	address = {Berlin, Heidelberg},
	series = {Studies in {Classification}, {Data} {Analysis}, and {Knowledge} {Organization}},
	title = {{KNIME}: {The} {Konstanz} {Information} {Miner}},
	isbn = {978-3-540-78239-1},
	url = {http://link.springer.com/10.1007/978-3-540-78246-9_38},
	abstract = {The Konstanz Information Miner is a modular environment, which enables easy visual assembly and interactive execution of a data pipeline. It is designed as a teaching, research and collaboration platform, which enables simple integration of new algorithms and tools as well as data manipulation or visualization methods in the form of new modules or nodes. In this paper we describe some of the design aspects of the underlying architecture and briefly sketch how new nodes can be incorporated.},
	urldate = {2017-11-03},
	booktitle = {Data {Analysis}, {Machine} {Learning} and {Applications}},
	publisher = {Springer Berlin Heidelberg},
	author = {Berthold, Michael R. and Cebron, Nicolas and Dill, Fabian and Gabriel, Thomas R. and Kötter, Tobias and Meinl, Thorsten and Ohl, Peter and Sieb, Christoph and Thiel, Kilian and Wiswedel, Bernd},
	editor = {Preisach, Christine and Burkhardt, Hans and Schmidt-Thieme, Lars and Decker, Reinhold},
	year = {2008},
	doi = {10.1007/978-3-540-78246-9_38},
	pages = {319--326},
}

@article{moller_community-driven_2010,
	title = {Community-driven computational biology with {Debian} {Linux}.},
	volume = {11 Suppl 12},
	url = {http://dx.doi.org/10.1186/1471-2105-11-S12-S5},
	doi = {10.1186/1471-2105-11-S12-S5},
	abstract = {BACKGROUND: The Open Source movement and its technologies are popular in the bioinformatics community because they provide freely available tools and resources for research. In order to feed the steady demand for updates on software and associated data, a service infrastructure is required for sharing and providing these tools to heterogeneous computing environments. RESULTS: The Debian Med initiative provides ready and coherent software packages for medical informatics and bioinformatics. These packages can be used together in Taverna workflows via the UseCase plugin to manage execution on local or remote machines. If such packages are available in cloud computing environments, the underlying hardware and the analysis pipelines can be shared along with the software. CONCLUSIONS: Debian Med closes the gap between developers and users. It provides a simple method for offering new releases of software and data resources, thus provisioning a local infrastructure for computational biology. For geographically distributed teams it can ensure they are working on the same versions of tools, in the same conditions. This contributes to the world-wide networking of researchers.},
	urldate = {2017-11-03},
	journal = {BMC Bioinformatics},
	author = {Möller, Steffen and Krabbenhöft, Hajo Nils and Tille, Andreas and Paleino, David and Williams, Alan and Wolstencroft, Katy and Goble, Carole and Holland, Richard and Belhachemi, Dominique and Plessy, Charles},
	month = dec,
	year = {2010},
	pmid = {21210984},
	pmcid = {PMC3040531},
	pages = {S5},
}

@article{afgan_galaxy_2016,
	title = {The {Galaxy} platform for accessible, reproducible and collaborative biomedical analyses: 2016 update.},
	volume = {44},
	url = {http://dx.doi.org/10.1093/nar/gkw343},
	doi = {10.1093/nar/gkw343},
	abstract = {High-throughput data production technologies, particularly 'next-generation' DNA sequencing, have ushered in widespread and disruptive changes to biomedical research. Making sense of the large datasets produced by these technologies requires sophisticated statistical and computational methods, as well as substantial computational power. This has led to an acute crisis in life sciences, as researchers without informatics training attempt to perform computation-dependent analyses. Since 2005, the Galaxy project has worked to address this problem by providing a framework that makes advanced computational tools usable by non experts. Galaxy seeks to make data-intensive research more accessible, transparent and reproducible by providing a Web-based environment in which users can perform computational analyses and have all of the details automatically tracked for later inspection, publication, or reuse. In this report we highlight recently added features enabling biomedical analyses on a large scale. {\textbackslash}copyright The Author(s) 2016. Published by Oxford University Press on behalf of Nucleic Acids Research.},
	number = {W1},
	urldate = {2017-11-03},
	journal = {Nucleic Acids Research},
	author = {Afgan, Enis and Baker, Dannon and van den Beek, Marius and Blankenberg, Daniel and Bouvier, Dave and Čech, Martin and Chilton, John and Clements, Dave and Coraor, Nate and Eberhard, Carl and Grüning, Björn and Guerler, Aysam and Hillman-Jackson, Jennifer and Von Kuster, Greg and Rasche, Eric and Soranzo, Nicola and Turaga, Nitesh and Taylor, James and Nekrutenko, Anton and Goecks, Jeremy},
	month = jul,
	year = {2016},
	pmid = {27137889},
	pmcid = {PMC4987906},
	pages = {W3--W10},
}

@article{ison_edam_2013,
	title = {{EDAM}: an ontology of bioinformatics operations, types of data and identifiers, topics and formats.},
	volume = {29},
	url = {http://dx.doi.org/10.1093/bioinformatics/btt113},
	doi = {10.1093/bioinformatics/btt113},
	abstract = {MOTIVATION: Advancing the search, publication and integration of bioinformatics tools and resources demands consistent machine-understandable descriptions. A comprehensive ontology allowing such descriptions is therefore required. RESULTS: EDAM is an ontology of bioinformatics operations (tool or workflow functions), types of data and identifiers, application domains and data formats. EDAM supports semantic annotation of diverse entities such as Web services, databases, programmatic libraries, standalone tools, interactive applications, data schemas, datasets and publications within bioinformatics. EDAM applies to organizing and finding suitable tools and data and to automating their integration into complex applications or workflows. It includes over 2200 defined concepts and has successfully been used for annotations and implementations. AVAILABILITY: The latest stable version of EDAM is available in OWL format from http://edamontology.org/EDAM.owl and in OBO format from http://edamontology.org/EDAM.obo. It can be viewed online at the NCBO BioPortal and the EBI Ontology Lookup Service. For documentation and license please refer to http://edamontology.org. This article describes version 1.2 available at http://edamontology.org/EDAM\_1.2.owl. CONTACT: jison@ebi.ac.uk.},
	number = {10},
	urldate = {2018-07-13},
	journal = {Bioinformatics},
	author = {Ison, Jon and Kalas, Matús and Jonassen, Inge and Bolser, Dan and Uludag, Mahmut and McWilliam, Hamish and Malone, James and Lopez, Rodrigo and Pettifer, Steve and Rice, Peter},
	month = may,
	year = {2013},
	pmid = {23479348},
	pmcid = {PMC3654706},
	pages = {1325--1332},
}

@article{wolstencroft_taverna_2013,
	title = {The {Taverna} workflow suite: designing and executing workflows of {Web} {Services} on the desktop, web or in the cloud.},
	volume = {41},
	url = {http://dx.doi.org/10.1093/nar/gkt328},
	doi = {10.1093/nar/gkt328},
	abstract = {The Taverna workflow tool suite (http://www.taverna.org.uk) is designed to combine distributed Web Services and/or local tools into complex analysis pipelines. These pipelines can be executed on local desktop machines or through larger infrastructure (such as supercomputers, Grids or cloud environments), using the Taverna Server. In bioinformatics, Taverna workflows are typically used in the areas of high-throughput omics analyses (for example, proteomics or transcriptomics), or for evidence gathering methods involving text mining or data mining. Through Taverna, scientists have access to several thousand different tools and resources that are freely available from a large range of life science institutions. Once constructed, the workflows are reusable, executable bioinformatics protocols that can be shared, reused and repurposed. A repository of public workflows is available at http://www.myexperiment.org. This article provides an update to the Taverna tool suite, highlighting new features and developments in the workbench and the Taverna Server.},
	number = {Web Server issue},
	urldate = {2018-07-13},
	journal = {Nucleic Acids Research},
	author = {Wolstencroft, Katherine and Haines, Robert and Fellows, Donal and Williams, Alan and Withers, David and Owen, Stuart and Soiland-Reyes, Stian and Dunlop, Ian and Nenadic, Aleksandra and Fisher, Paul and Bhagat, Jiten and Belhajjame, Khalid and Bacall, Finn and Hardisty, Alex and Nieva de la Hidalga, Abraham and Balcazar Vargas, Maria P and Sufi, Shoaib and Goble, Carole},
	month = jul,
	year = {2013},
	pmid = {23640334},
	pmcid = {PMC3692062},
	pages = {W557--61},
}

@article{ison_tools_2016,
	title = {Tools and data services registry: a community effort to document bioinformatics resources.},
	volume = {44},
	url = {http://dx.doi.org/10.1093/nar/gkv1116},
	doi = {10.1093/nar/gkv1116},
	abstract = {Life sciences are yielding huge data sets that underpin scientific discoveries fundamental to improvement in human health, agriculture and the environment. In support of these discoveries, a plethora of databases and tools are deployed, in technically complex and diverse implementations, across a spectrum of scientific disciplines. The corpus of documentation of these resources is fragmented across the Web, with much redundancy, and has lacked a common standard of information. The outcome is that scientists must often struggle to find, understand, compare and use the best resources for the task at hand.Here we present a community-driven curation effort, supported by ELIXIR-the European infrastructure for biological information-that aspires to a comprehensive and consistent registry of information about bioinformatics resources. The sustainable upkeep of this Tools and Data Services Registry is assured by a curation effort driven by and tailored to local needs, and shared amongst a network of engaged partners.As of November 2015, the registry includes 1785 resources, with depositions from 126 individual registrations including 52 institutional providers and 74 individuals. With community support, the registry can become a standard for dissemination of information about bioinformatics resources: we welcome everyone to join us in this common endeavour. The registry is freely available at https://bio.tools. {\textbackslash}copyright The Author(s) 2015. Published by Oxford University Press on behalf of Nucleic Acids Research.},
	number = {D1},
	urldate = {2017-11-03},
	journal = {Nucleic Acids Research},
	author = {Ison, Jon and Rapacki, Kristoffer and Ménager, Hervé and Kalaš, Matúš and Rydza, Emil and Chmura, Piotr and Anthon, Christian and Beard, Niall and Berka, Karel and Bolser, Dan and Booth, Tim and Bretaudeau, Anthony and Brezovsky, Jan and Casadio, Rita and Cesareni, Gianni and Coppens, Frederik and Cornell, Michael and Cuccuru, Gianmauro and Davidsen, Kristian and Vedova, Gianluca Della and Dogan, Tunca and Doppelt-Azeroual, Olivia and Emery, Laura and Gasteiger, Elisabeth and Gatter, Thomas and Goldberg, Tatyana and Grosjean, Marie and Grüning, Björn and Helmer-Citterich, Manuela and Ienasescu, Hans and Ioannidis, Vassilios and Jespersen, Martin Closter and Jimenez, Rafael and Juty, Nick and Juvan, Peter and Koch, Maximilian and Laibe, Camille and Li, Jing-Woei and Licata, Luana and Mareuil, Fabien and Mičetić, Ivan and Friborg, Rune Møllegaard and Moretti, Sebastien and Morris, Chris and Möller, Steffen and Nenadic, Aleksandra and Peterson, Hedi and Profiti, Giuseppe and Rice, Peter and Romano, Paolo and Roncaglia, Paola and Saidi, Rabie and Schafferhans, Andrea and Schwämmle, Veit and Smith, Callum and Sperotto, Maria Maddalena and Stockinger, Heinz and Vařeková, Radka Svobodová and Tosatto, Silvio C E and de la Torre, Victor and Uva, Paolo and Via, Allegra and Yachdav, Guy and Zambelli, Federico and Vriend, Gert and Rost, Burkhard and Parkinson, Helen and Løngreen, Peter and Brunak, Søren},
	month = jan,
	year = {2016},
	pmid = {26538599},
	pmcid = {PMC4702812},
	pages = {D38--47},
}

@article{leipzig_review_2017,
	title = {A review of bioinformatic pipeline frameworks.},
	volume = {18},
	url = {http://dx.doi.org/10.1093/bib/bbw020},
	doi = {10.1093/bib/bbw020},
	abstract = {High-throughput bioinformatic analyses increasingly rely on pipeline frameworks to process sequence and metadata. Modern implementations of these frameworks differ on three key dimensions: using an implicit or explicit syntax, using a configuration, convention or class-based design paradigm and offering a command line or workbench interface. Here I survey and compare the design philosophies of several current pipeline frameworks. I provide practical recommendations based on analysis requirements and the user base. {\textbackslash}copyright The Author 2016. Published by Oxford University Press.},
	number = {3},
	urldate = {2018-07-13},
	journal = {Briefings in Bioinformatics},
	author = {Leipzig, Jeremy},
	month = may,
	year = {2017},
	pmid = {27013646},
	pmcid = {PMC5429012},
	pages = {530--536},
}

@article{moller_evaluation_2001,
	title = {Evaluation of methods for the prediction of membrane spanning regions.},
	volume = {17},
	url = {http://dx.doi.org/10.1093/bioinformatics/18.1.218},
	doi = {10.1093/bioinformatics/18.1.218},
	abstract = {MOTIVATION: A variety of tools are available to predict the topology of transmembrane proteins. To date no independent evaluation of the performance of these tools has been published. A better understanding of the strengths and weaknesses of the different tools would guide both the biologist and the bioinformatician to make better predictions of membrane protein topology. RESULTS: Here we present an evaluation of the performance of the currently best known and most widely used methods for the prediction of transmembrane regions in proteins. Our results show that TMHMM is currently the best performing transmembrane prediction program.},
	number = {7},
	urldate = {2017-11-03},
	journal = {Bioinformatics},
	author = {Möller, S and Croning, M D and Apweiler, R},
	month = jul,
	year = {2001},
	pmid = {11448883},
	pages = {646--653},
}

@article{spjuth_experiences_2015,
	title = {Experiences with workflows for automating data-intensive bioinformatics.},
	volume = {10},
	issn = {1745-6150},
	url = {http://www.biologydirect.com/content/10/1/43},
	doi = {10.1186/s13062-015-0071-8},
	abstract = {High-throughput technologies, such as next-generation sequencing, have turned molecular biology into a data-intensive discipline, requiring bioinformaticians to use high-performance computing resources and carry out data management and analysis tasks on large scale. Workflow systems can be useful to simplify construction of analysis pipelines that automate tasks, support reproducibility and provide measures for fault-tolerance. However, workflow systems can incur significant development and administration overhead so bioinformatics pipelines are often still built without them. We present the experiences with workflows and workflow systems within the bioinformatics community participating in a series of hackathons and workshops of the EU COST action SeqAhead. The organizations are working on similar problems, but we have addressed them with different strategies and solutions. This fragmentation of efforts is inefficient and leads to redundant and incompatible solutions. Based on our experiences we define a set of recommendations for future systems to enable efficient yet simple bioinformatics workflow construction and execution.},
	urldate = {2017-11-03},
	journal = {Biology Direct},
	author = {Spjuth, Ola and Bongcam-Rudloff, Erik and Hernández, Guillermo Carrasco and Forer, Lukas and Giovacchini, Mario and Guimera, Roman Valls and Kallio, Aleksi and Korpelainen, Eija and Kańduła, Maciej M and Krachunov, Milko and Kreil, David P and Kulev, Ognyan and Łabaj, Paweł P and Lampa, Samuel and Pireddu, Luca and Schönherr, Sebastian and Siretskiy, Alexey and Vassilev, Dimitar},
	month = aug,
	year = {2015},
	pmid = {26282399},
	pmcid = {PMC4539931},
	pages = {43},
}

@article{zook_integrating_2014,
	title = {Integrating human sequence data sets provides a resource of benchmark {SNP} and indel genotype calls.},
	volume = {32},
	url = {http://dx.doi.org/10.1038/nbt.2835},
	doi = {10.1038/nbt.2835},
	abstract = {Clinical adoption of human genome sequencing requires methods that output genotypes with known accuracy at millions or billions of positions across a genome. Because of substantial discordance among calls made by existing sequencing methods and algorithms, there is a need for a highly accurate set of genotypes across a genome that can be used as a benchmark. Here we present methods to make high-confidence, single-nucleotide polymorphism (SNP), indel and homozygous reference genotype calls for NA12878, the pilot genome for the Genome in a Bottle Consortium. We minimize bias toward any method by integrating and arbitrating between 14 data sets from five sequencing technologies, seven read mappers and three variant callers. We identify regions for which no confident genotype call could be made, and classify them into different categories based on reasons for uncertainty. Our genotype calls are publicly available on the Genome Comparison and Analytic Testing website to enable real-time benchmarking of any method.},
	number = {3},
	urldate = {2018-06-12},
	journal = {Nature Biotechnology},
	author = {Zook, Justin M and Chapman, Brad and Wang, Jason and Mittelman, David and Hofmann, Oliver and Hide, Winston and Salit, Marc},
	month = mar,
	year = {2014},
	pmid = {24531798},
	pages = {246--251},
}

@article{prins_toward_2015,
	title = {Toward effective software solutions for big biology.},
	volume = {33},
	url = {http://dx.doi.org/10.1038/nbt.3240},
	doi = {10.1038/nbt.3240},
	number = {7},
	urldate = {2017-11-03},
	journal = {Nature Biotechnology},
	author = {Prins, Pjotr and de Ligt, Joep and Tarasov, Artem and Jansen, Ritsert C and Cuppen, Edwin and Bourne, Philip E},
	month = jul,
	year = {2015},
	pmid = {26154002},
	pages = {686--687},
}

@article{goecks_galaxy_2010,
	title = {Galaxy: a comprehensive approach for supporting accessible, reproducible, and transparent computational research in the life sciences.},
	volume = {11},
	url = {http://dx.doi.org/10.1186/gb-2010-11-8-r86},
	doi = {10.1186/gb-2010-11-8-r86},
	abstract = {Increased reliance on computational approaches in the life sciences has revealed grave concerns about how accessible and reproducible computation-reliant results truly are. Galaxy http://usegalaxy.org, an open web-based platform for genomic research, addresses these problems. Galaxy automatically tracks and manages data provenance and provides support for capturing the context and intent of computational methods. Galaxy Pages are interactive, web-based documents that provide users with a medium to communicate a complete computational analysis.},
	number = {8},
	urldate = {2017-11-03},
	journal = {Genome Biology},
	author = {Goecks, Jeremy and Nekrutenko, Anton and Taylor, James and Team, Galaxy},
	month = aug,
	year = {2010},
	pmid = {20738864},
	pmcid = {PMC2945788},
	pages = {R86},
}

@article{garijo_common_2014,
	title = {Common motifs in scientific workflows: {An} empirical analysis},
	volume = {36},
	issn = {0167739X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0167739X13001970},
	doi = {10.1016/j.future.2013.09.018},
	abstract = {Workflow technology continues to play an important role as a means for specifying and enacting computational experiments in modern science. Reusing and re-purposing workflows allow scientists to do new experiments faster, since the workflows capture useful expertise from others. As workflow libraries grow, scientists face the challenge of finding workflows appropriate for their task, understanding what each workflow does, and reusing relevant portions of a given workflow. We believe that workflows would be easier to understand and reuse if high-level views (abstractions) of their activities were available in workflow libraries. As a first step towards obtaining these abstractions, we report in this paper on the results of a manual analysis performed over a set of real-world scientific workflows from Taverna, Wings, Galaxy and Vistrails. Our analysis has resulted in a set of scientific workflow motifs that outline (i) the kinds of data-intensive activities that are observed in workflows (Data-Operation motifs), and (ii) the different manners in which activities are implemented within workflows (Workflow-Oriented motifs). These motifs are helpful to identify the functionality of the steps in a given workflow, to develop best practices for workflow design, and to develop approaches for automated generation of workflow abstractions.},
	urldate = {2020-01-20},
	journal = {Future Generation Computer Systems},
	author = {Garijo, Daniel and Alper, Pinar and Belhajjame, Khalid and Corcho, Oscar and Gil, Yolanda and Goble, Carole},
	month = jul,
	year = {2014},
	pages = {338--351},
}

@article{goble_myexperiment_2010,
	title = {{myExperiment}: a repository and social network for the sharing of bioinformatics workflows.},
	volume = {38},
	url = {http://dx.doi.org/10.1093/nar/gkq429},
	doi = {10.1093/nar/gkq429},
	abstract = {myExperiment (http://www.myexperiment.org) is an online research environment that supports the social sharing of bioinformatics workflows. These workflows are procedures consisting of a series of computational tasks using web services, which may be performed on data from its retrieval, integration and analysis, to the visualization of the results. As a public repository of workflows, myExperiment allows anybody to discover those that are relevant to their research, which can then be reused and repurposed to their specific requirements. Conversely, developers can submit their workflows to myExperiment and enable them to be shared in a secure manner. Since its release in 2007, myExperiment currently has over 3500 registered users and contains more than 1000 workflows. The social aspect to the sharing of these workflows is facilitated by registered users forming virtual communities bound together by a common interest or research project. Contributors of workflows can build their reputation within these communities by receiving feedback and credit from individuals who reuse their work. Further documentation about myExperiment including its REST web service is available from http://wiki.myexperiment.org. Feedback and requests for support can be sent to bugs@myexperiment.org.},
	number = {Web Server issue},
	urldate = {2017-11-03},
	journal = {Nucleic Acids Research},
	author = {Goble, Carole A and Bhagat, Jiten and Aleksejevs, Sergejs and Cruickshank, Don and Michaelides, Danius and Newman, David and Borkum, Mark and Bechhofer, Sean and Roos, Marco and Li, Peter and De Roure, David},
	month = jul,
	year = {2010},
	pmid = {20501605},
	pmcid = {PMC2896080},
	pages = {W677--82},
}

@article{li_seqanswers_2012,
	title = {The {SEQanswers} wiki: a wiki database of tools for high-throughput sequencing analysis.},
	volume = {40},
	url = {http://dx.doi.org/10.1093/nar/gkr1058},
	doi = {10.1093/nar/gkr1058},
	abstract = {Recent advances in sequencing technology have created unprecedented opportunities for biological research. However, the increasing throughput of these technologies has created many challenges for data management and analysis. As the demand for sophisticated analyses increases, the development time of software and algorithms is outpacing the speed of traditional publication. As technologies continue to be developed, methods change rapidly, making publications less relevant for users. The SEQanswers wiki (SEQwiki) is a wiki database that is actively edited and updated by the members of the SEQanswers community (http://SEQanswers.com/). The wiki provides an extensive catalogue of tools, technologies and tutorials for high-throughput sequencing (HTS), including information about HTS service providers. It has been implemented in MediaWiki with the Semantic MediaWiki and Semantic Forms extensions to collect structured data, providing powerful navigation and reporting features. Within 2 years, the community has created pages for over 500 tools, with approximately 400 literature references and 600 web links. This collaborative effort has made SEQwiki the most comprehensive database of HTS tools anywhere on the web. The wiki includes task-focused mini-reviews of commonly used tools, and a growing collection of more than 100 HTS service providers. SEQwiki is available at: http://wiki.SEQanswers.com/.},
	number = {Database issue},
	urldate = {2017-11-03},
	journal = {Nucleic Acids Research},
	author = {Li, Jing-Woei and Robison, Keith and Martin, Marcel and Sjödin, Andreas and Usadel, Björn and Young, Matthew and Olivares, Eric C and Bolser, Dan M},
	month = jan,
	year = {2012},
	pmid = {22086956},
	pmcid = {PMC3245082},
	pages = {D1313--7},
}

@article{bandrowski_resource_2015,
	title = {The {Resource} {Identification} {Initiative}: {A} cultural shift in publishing.},
	volume = {4},
	url = {http://dx.doi.org/10.12688/f1000research.6555.2},
	doi = {10.12688/f1000research.6555.2},
	abstract = {A central tenet in support of research reproducibility is the ability to uniquely identify research resources, i.e., reagents, tools, and materials that are used to perform experiments. However, current reporting practices for research resources are insufficient to allow humans and algorithms to identify the exact resources that are reported or answer basic questions such as "What other studies used resource X?" To address this issue, the Resource Identification Initiative was launched as a pilot project to improve the reporting standards for research resources in the methods sections of papers and thereby improve identifiability and reproducibility. The pilot engaged over 25 biomedical journal editors from most major publishers, as well as scientists and funding officials. Authors were asked to include Research Resource Identifiers (RRIDs) in their manuscripts prior to publication for three resource types: antibodies, model organisms, and tools (including software and databases). RRIDs represent accession numbers assigned by an authoritative database, e.g., the model organism databases, for each type of resource. To make it easier for authors to obtain RRIDs, resources were aggregated from the appropriate databases and their RRIDs made available in a central web portal ( www.scicrunch.org/resources). RRIDs meet three key criteria: they are machine readable, free to generate and access, and are consistent across publishers and journals. The pilot was launched in February of 2014 and over 300 papers have appeared that report RRIDs. The number of journals participating has expanded from the original 25 to more than 40. Here, we present an overview of the pilot project and its outcomes to date. We show that authors are generally accurate in performing the task of identifying resources and supportive of the goals of the project. We also show that identifiability of the resources pre- and post-pilot showed a dramatic improvement for all three resource types, suggesting that the project has had a significant impact on reproducibility relating to research resources.},
	urldate = {2017-11-03},
	journal = {F1000Research},
	author = {Bandrowski, Anita and Brush, Matthew and Grethe, Jeffery S and Haendel, Melissa A and Kennedy, David N and Hill, Sean and Hof, Patrick R and Martone, Maryann E and Pols, Maaike and Tan, Serena and Washington, Nicole and Zudilova-Seinstra, Elena and Vasilevsky, Nicole and https://www.force11.org/node/4463/members, Resource Identification Initiative Members are listed here:},
	month = may,
	year = {2015},
	pmid = {26594330},
	pmcid = {PMC4648211},
	pages = {134},
}

@article{gentleman_bioconductor_2004,
	title = {Bioconductor: open software development for computational biology and bioinformatics.},
	volume = {5},
	url = {http://dx.doi.org/10.1186/gb-2004-5-10-r80},
	doi = {10.1186/gb-2004-5-10-r80},
	abstract = {The Bioconductor project is an initiative for the collaborative creation of extensible software for computational biology and bioinformatics. The goals of the project include: fostering collaborative development and widespread use of innovative software, reducing barriers to entry into interdisciplinary scientific research, and promoting the achievement of remote reproducibility of research results. We describe details of our aims and methods, identify current challenges, compare Bioconductor to other open bioinformatics projects, and provide working examples.},
	number = {10},
	urldate = {2017-02-20},
	journal = {Genome Biology},
	author = {Gentleman, Robert C and Carey, Vincent J and Bates, Douglas M and Bolstad, Ben and Dettling, Marcel and Dudoit, Sandrine and Ellis, Byron and Gautier, Laurent and Ge, Yongchao and Gentry, Jeff and Hornik, Kurt and Hothorn, Torsten and Huber, Wolfgang and Iacus, Stefano and Irizarry, Rafael and Leisch, Friedrich and Li, Cheng and Maechler, Martin and Rossini, Anthony J and Sawitzki, Gunther and Smith, Colin and Smyth, Gordon and Tierney, Luke and Yang, Jean Y H and Zhang, Jianhua},
	month = sep,
	year = {2004},
	pmid = {15461798},
	pmcid = {PMC545600},
	pages = {R80},
}

@article{sroka_formal_2010,
	series = {Special {Issue}: {Scientific} {Workflow} 2009},
	title = {A formal semantics for the {Taverna} 2 workflow model},
	volume = {76},
	issn = {0022-0000},
	url = {http://www.sciencedirect.com/science/article/pii/S0022000009001251},
	doi = {10.1016/j.jcss.2009.11.009},
	abstract = {This paper presents a formal semantics for the Taverna 2 scientific workflow system. Taverna 2 is a successor to Taverna, an open-source workflow system broadly adopted within the e-science community worldwide. The new version improves upon the existing model in two main ways: (i) by adding support for data pipelining, which in turns enables input streams of indefinite length to be processed efficiently; and (ii) by providing new extensibility points that make it possible to add new operators to the workflow model. Consistent with previous work by some of the authors, we use trace semantics to describe the effect of workflow computations, and we show how they can be used to describe the new features in the Taverna 2 model.},
	language = {en},
	number = {6},
	urldate = {2020-01-21},
	journal = {Journal of Computer and System Sciences},
	author = {Sroka, Jacek and Hidders, Jan and Missier, Paolo and Goble, Carole},
	month = sep,
	year = {2010},
	keywords = {Formal semantics, Scientific workflows, Taverna 2, Trace semantics},
	pages = {490--508},
}

@article{khan_sharing_2019,
	title = {Sharing interoperable workflow provenance: {A} review of best practices and their practical application in {CWLProv}},
	volume = {8},
	shorttitle = {Sharing interoperable workflow provenance},
	url = {https://academic.oup.com/gigascience/article/8/11/giz095/5611001},
	doi = {10.1093/gigascience/giz095},
	abstract = {AbstractBackground.  The automation of data analysis in the form of scientific workflows has become a widely adopted practice in many fields of research. Comput},
	language = {en},
	number = {11},
	urldate = {2019-11-04},
	journal = {GigaScience},
	author = {Khan, Farah Zaib and Soiland-Reyes, Stian and Sinnott, Richard O. and Lonie, Andrew and Goble, Carole and Crusoe, Michael R.},
	month = nov,
	year = {2019},
}

@misc{noauthor_schema_salad_nodate,
	title = {schema\_salad {\textbar} {Read} the {Docs}},
	url = {https://readthedocs.org/projects/schema-salad/builds/9859889/},
	urldate = {2019-10-25},
}

@article{surrey_genomic_nodate,
	title = {Genomic {Analysis} of {Dysembryoplastic} {Neuroepithelial} {Tumor} {Spectrum} {Reveals} a {Diversity} of {Molecular} {Alterations} {Dysregulating} the {MAPK} and {PI3K}/{mTOR} {Pathways}},
	url = {https://academic.oup.com/jnen/advance-article/doi/10.1093/jnen/nlz101/5588597},
	doi = {10.1093/jnen/nlz101},
	abstract = {Abstract.  Dysembryoplastic neuroepithelial tumors (DNT) lacking key diagnostic criteria are challenging to diagnose and sometimes fall into the broader categor},
	language = {en},
	urldate = {2019-10-23},
	journal = {Journal of Neuropathology \& Experimental Neurology},
	author = {Surrey, Lea F. and Jain, Payal and Zhang, Bo and Straka, Joshua and Zhao, Xiaonan and Harding, Brian N. and Resnick, Adam C. and Storm, Phillip B. and Buccoliero, Anna Maria and Genitori, Lorenzo and Li, Marilyn M. and Waanders, Angela J. and Santi, Mariarita},
}

@article{ison_community_nodate,
	title = {Community curation of bioinformatics software and data resources},
	url = {https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bbz075/5560007},
	doi = {10.1093/bib/bbz075},
	abstract = {Abstract.  The corpus of bioinformatics resources is huge and expanding rapidly, presenting life scientists with a growing challenge in selecting tools that fit},
	language = {en},
	urldate = {2019-10-23},
	journal = {Briefings in Bioinformatics},
	author = {Ison, Jon and Ménager, Hervé and Brancotte, Bryan and Jaaniso, Erik and Salumets, Ahto and Raček, Tomáš and Lamprecht, Anna-Lena and Palmblad, Magnus and Kalaš, Matúš and Chmura, Piotr and Hancock, John M. and Schwämmle, Veit and Ienasescu, Hans-Ioan},
}

@article{vazquez_patient_2019,
	title = {Patient {Dossier}: {Healthcare} queries over distributed resources},
	volume = {15},
	issn = {1553-7358},
	shorttitle = {Patient {Dossier}},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007291},
	doi = {10.1371/journal.pcbi.1007291},
	abstract = {As with many other aspects of the modern world, in healthcare, the explosion of data and resources opens new opportunities for the development of added-value services. Still, a number of specific conditions on this domain greatly hinders these developments, including ethical and legal issues, fragmentation of the relevant data in different locations, and a level of (meta)data complexity that requires great expertise across technical, clinical, and biological domains. We propose the Patient Dossier paradigm as a way to organize new innovative healthcare services that sorts the current limitations. The Patient Dossier conceptual framework identifies the different issues and suggests how they can be tackled in a safe, efficient, and responsible way while opening options for independent development for different players in the healthcare sector. An initial implementation of the Patient Dossier concepts in the Rbbt framework is available as open-source at https://github.com/mikisvaz and https://github.com/Rbbt-Workflows.},
	language = {en},
	number = {10},
	urldate = {2019-10-23},
	journal = {PLOS Computational Biology},
	author = {Vazquez, Miguel and Valencia, Alfonso},
	month = oct,
	year = {2019},
	keywords = {Bioinformatics, Computational pipelines, Data management, Data processing, Genome analysis, Genomic medicine, Genomics, Next-generation sequencing},
	pages = {e1007291},
}

@article{hey_fourth_2019,
	title = {The {Fourth} {Paradigm} 10 {Years} {On}},
	issn = {1432-122X},
	url = {https://doi.org/10.1007/s00287-019-01215-9},
	doi = {10.1007/s00287-019-01215-9},
	language = {en},
	urldate = {2019-10-23},
	journal = {Informatik Spektrum},
	author = {Hey, Tony and Trefethen, Anne},
	month = oct,
	year = {2019},
}

@misc{noauthor_fair_nodate,
	title = {The {FAIR} {Guiding} {Principles} for scientific data management and stewardship {\textbar} {Scientific} {Data}},
	url = {https://www.nature.com/articles/sdata201618},
	urldate = {2019-08-01},
}

@misc{noauthor_future_nodate,
	title = {The future of scientific workflows - {Ewa} {Deelman}, {Tom} {Peterka}, {Ilkay} {Altintas}, {Christopher} {D} {Carothers}, {Kerstin} {Kleese} van {Dam}, {Kenneth} {Moreland}, {Manish} {Parashar}, {Lavanya} {Ramakrishnan}, {Michela} {Taufer}, {Jeffrey} {Vetter}, 2018},
	url = {https://journals.sagepub.com/doi/10.1177/1094342017704893},
	urldate = {2019-08-01},
}

@book{crusoe_khmer-protocols_2013,
	title = {khmer-protocols 0.8.4 documentation},
	url = {http://doi.org/10.6084/M9.FIGSHARE.878460.V2},
	author = {Crusoe, Michael R. and Brown, C. Titus and Scott, Camille and Crusoe, Michael R. and Sheneman, Leigh and Rosenthal, Josh and Howe, Adina},
	year = {2013},
	doi = {10.6084/M9.FIGSHARE.878460.V2},
	note = {00003 },
}

@article{da_veiga_leprevost_biocontainers:_2017,
	title = {{BioContainers}: an open-source and community-driven framework for software standardization},
	volume = {33},
	issn = {1367-4803},
	shorttitle = {{BioContainers}},
	url = {https://academic.oup.com/bioinformatics/article/33/16/2580/3096437},
	doi = {10.1093/bioinformatics/btx192},
	abstract = {AbstractMotivation.  BioContainers (biocontainers.pro) is an open-source and community-driven framework which provides platform independent executable environme},
	language = {en},
	number = {16},
	urldate = {2019-08-01},
	journal = {Bioinformatics},
	author = {da Veiga Leprevost, Felipe and Grüning, Björn A. and Alves Aflitos, Saulo and Röst, Hannes L. and Uszkoreit, Julian and Barsnes, Harald and Vaudel, Marc and Moreno, Pablo and Gatto, Laurent and Weber, Jonas and Bai, Mingze and Jimenez, Rafael C. and Sachsenberg, Timo and Pfeuffer, Julianus and Vera Alvarez, Roberto and Griss, Johannes and Nesvizhskii, Alexey I. and Perez-Riverol, Yasset},
	month = aug,
	year = {2017},
	pages = {2580--2582},
}

@article{alimena_searching_2019,
	title = {Searching for long-lived particles beyond the {Standard} {Model} at the {Large} {Hadron} {Collider}},
	url = {https://arxiv.org/abs/1903.04497v1},
	abstract = {Particles beyond the Standard Model (SM) can generically have lifetimes that
are long compared to SM particles at the weak scale. When produced at
experiments such as the Large Hadron Collider (LHC) at CERN, these long-lived
particles (LLPs) can decay far from the interaction vertex of the primary
proton-proton collision. Such LLP signatures are distinct from those of
promptly decaying particles that are targeted by the majority of searches for
new physics at the LHC, often requiring customized techniques to identify, for
example, significantly displaced decay vertices, tracks with atypical
properties, and short track segments. Given their non-standard nature, a
comprehensive overview of LLP signatures at the LHC is beneficial to ensure
that possible avenues of the discovery of new physics are not overlooked. Here
we report on the joint work of a community of theorists and experimentalists
with the ATLAS, CMS, and LHCb experiments --- as well as those working on
dedicated experiments such as MoEDAL, milliQan, MATHUSLA, CODEX-b, and FASER
--- to survey the current state of LLP searches at the LHC, and to chart a path
for the development of LLP searches into the future, both in the upcoming Run 3
and at the High-Luminosity LHC. The work is organized around the current and
future potential capabilities of LHC experiments to generally discover new
LLPs, and takes a signature-based approach to surveying classes of models that
give rise to LLPs rather than emphasizing any particular theory motivation. We
develop a set of simplified models; assess the coverage of current searches;
document known, often unexpected backgrounds; explore the capabilities of
proposed detector upgrades; provide recommendations for the presentation of
search results; and look towards the newest frontiers, namely high-multiplicity
"dark showers", highlighting opportunities for expanding the LHC reach for
these signals.},
	language = {en},
	urldate = {2019-03-19},
	author = {Alimena, Juliette and Beacham, James and Borsato, Martino and Cheng, Yangyang and Vidal, Xabier Cid and Cottin, Giovanna and De Roeck, Albert and Desai, Nishita and Curtin, David and Evans, Jared A. and Knapen, Simon and Kraml, Sabine and Lessa, Andre and Liu, Zhen and Mehlhase, Sascha and Ramsey-Musolf, Michael J. and Russell, Heather and Shelton, Jessie and Shuve, Brian and Verducci, Monica and Zurita, Jose and Adams, Todd and Adersberger, Michael and Alpigiani, Cristiano and Apresyan, Artur and Bainbridge, Robert John and Batozskaya, Varvara and Beauchesne, Hugues and Benato, Lisa and Berlendis, S. and Bhal, Eshwen and Blekman, Freya and Borovilou, Christina and Boyd, Jamie and Brau, Benjamin P. and Bryngemark, Lene and Buchmueller, Oliver and Buschmann, Malte and Buttinger, William and Campanelli, Mario and Cesarotti, Cari and Chen, Chunhui and Cheng, Hsin-Chia and Cheong, Sanha and Citron, Matthew and Coccaro, Andrea and Coco, V. and Conte, Eric and Cormier, Félix and Corpe, Louie D. and Craig, Nathaniel and Cui, Yanou and Dall'Occo, Elena and Dallapiccola, C. and Darwish, M. R. and Davoli, Alessandro and de Cosa, Annapaola and De Simone, Andrea and Rose, Luigi Delle and Deppisch, Frank F. and Dey, Biplab and Diamond, Miriam D. and Dienes, Keith R. and Dildick, Sven and Döbrich, Babette and Drewes, Marco and Eich, Melanie and ElSawy, M. and del Valle, Alberto Escalante and Facini, Gabriel and Farina, Marco and Feng, Jonathan L. and Fischer, Oliver and Flaecher, H. U. and Foldenauer, Patrick and Freytsis, Marat and Fuks, Benjamin and Galon, Iftah and Gershtein, Yuri and Giagu, Stefano and Giammanco, Andrea and Gligorov, Vladimir V. and Golling, Tobias and Grancagnolo, Sergio and Gustavino, Giuliano and Haas, Andrew and Hahn, Kristian and Hajer, Jan and Hammad, Ahmed and Heinrich, Lukas and Heisig, Jan and Helo, J. C. and Hesketh, Gavin and Hill, Christopher S. and Hirsch, Martin and Hohlmann, M. and Hulsbergen, W. and Huth, John and Ilten, Philip and Jacques, Thomas},
	month = mar,
	year = {2019},
	note = {00001},
}

@article{hillion_using_2017,
	title = {Using bio.tools to generate and annotate workbench tool descriptions},
	volume = {6},
	url = {https://doi.org/10.12688%2Ff1000research.12974.1},
	doi = {10.12688/f1000research.12974.1},
	journal = {F1000Research},
	author = {Hillion, Kenzo-Hugo and Kuzmin, Ivan and Khodak, Anton and Rasche, Eric and Crusoe, Michael and Peterson, Hedi and Ison, Jon and Ménager, Hervé},
	month = nov,
	year = {2017},
	note = {00001},
	pages = {2074},
}

@book{crusoe_khmer_2014,
	title = {The khmer software package: enabling efficient sequence analysis},
	url = {http://doi.org/10.6084/M9.FIGSHARE.979190.V1},
	author = {Crusoe, Michael R. and Brown, C. Titus and Crusoe, Michael R. and Edvenson, Greg and Fish, Jordan and Howe, Adina and McDonald, Eric and Nahum, Joshua and Nanlohy, Kaben and Ortiz-Zuazaga, Humberto and Pell, Jason and Simpson, Jared and Scott, Camille and Srinivasan, Ramakrishnan Rajaram and Zhang, Qingpeng},
	year = {2014},
	doi = {10.6084/M9.FIGSHARE.979190.V1},
	note = {00023 },
}

@book{crusoe_should_2019,
	title = {Should you cite this particular piece of software?},
	url = {http://doi.org/10.5281/zenodo.2559141},
	author = {Crusoe, Michael R. and Brown, Louise and Crusoe, Michael R. and Miļajevs, Dmitrijs and Romanowska, Iza},
	year = {2019},
	doi = {10.5281/zenodo.2559141},
	note = {00000 },
}

@article{maller_robust_2017,
	title = {Robust {Cross}-{Platform} {Workflows}: {How} {Technical} and {Scientific} {Communities} {Collaborate} to {Develop}, {Test} and {Share} {Best} {Practices} for {Data} {Analysis}},
	volume = {2},
	url = {https://doi.org/10.1007%2Fs41019-017-0050-4},
	doi = {10.1007/s41019-017-0050-4},
	number = {3},
	journal = {Data Science and Engineering},
	author = {MÃ¶ller, Steffen and Prescott, Stuart W. and Wirzenius, Lars and Reinholdtsen, Petter and Chapman, Brad and Prins, Pjotr and Soiland-Reyes, Stian and KlÃ¶tzl, Fabian and Bagnacani, Andrea and Kalaš, Matúš and Tille, Andreas and Crusoe, Michael R.},
	month = sep,
	year = {2017},
	note = {00000},
	pages = {232--244},
}

@book{crusoe_request_2017,
	title = {Request for {Community} partnership in data resource licensing planning},
	url = {http://doi.org/10.6084/M9.FIGSHARE.4972709},
	author = {Crusoe, Michael R. and Haendel, Melissa and Mungall, Chris and Su, Andrew and Robinson, Peter and Chute, Chris and Altman, Russ B. and Payne, Philip R. O. and Lawler, Mark and Oprea, Tudor I. and Willbanks, John and Srinivasan, Subha and Hunter, Lawrence and Sim, Ida and McDonald, Sean and Mooney, Sean and Smedley, Damian and Ganley, Emma and Kenall, Amye and Clark, Timothy and Goble, Carole and Dumontier, Michel and Holmes, Kristi and Diekans, Mark and Zell, Adrienne and Overby, Casey and Glusman, Gustavo and Carmody, Leigh and Jiang, Guoqian and Munos-Torres, Monica and Hoatlin, Maureen and Goecks, Jeremy and Jongeneel, Victor and Bittker, Joshua and Gourdine, Jean-Philippe and Brush, Matthew H. and Zhu, Richard L. and Mangravite, Lara and Tyler, Brett and Wilkinson, Mark D. and Crusoe, Michael R. and Mazumder, Raja and Tatonetti, Nicholas P. and D'Eustachio, Peter and Vasilevsky, Nicole and McMurry, Julie and Champieux, Robin},
	year = {2017},
	doi = {10.6084/M9.FIGSHARE.4972709},
	note = {00000 },
}

@inproceedings{tollis_mining_2014,
	title = {Mining the {Most} {Species}-{Rich} {Amniote} {Genus}: de novo {Sequencing} of {Three} {Anole} {Lizards} for {Comparative} {Genomic} {Analysis}},
	url = {https://pag.confex.com/pag/xxii/webprogram/Paper11290.html},
	booktitle = {International {Plant} and {Animal} {Genome} {XXII}},
	author = {Tollis, Marc and Hutchins, Elizabeth D and Eckalbar, Walter and Crusoe, Michael R and May, Catherine M. and Stapley, Jessica and Kulik, Elise and Huentelman, Matt J. and Fisher, Rebecca E and Kusumi, Kenro},
	year = {2014},
	note = {00000},
}

@book{crusoe_khmer-protocols_2013-1,
	title = {khmer-protocols 0.8.3 documentation},
	url = {http://doi.org/10.6084/M9.FIGSHARE.878460.V1},
	author = {Crusoe, Michael R. and Brown, C. Titus and Scott, Camille and Crusoe, Michael R. and Sheneman, Leigh and Rosenthal, Josh and Howe, Adina},
	year = {2013},
	doi = {10.6084/M9.FIGSHARE.878460.V1},
	note = {00000 },
}

@book{crusoe_galaxy_2016,
	title = {Galaxy {Planemo} 0.35.0},
	author = {Crusoe, Michael R. and Chilton, John and Cock, Peter and Rasche, Eric and Turaga, Nitesh and Cech, Martin and Grüning, Björn and Soranzo, Nicola and Bouvier, Dave and Blankenberg, Dan and Baker, Dannon and Ellrott, Kyle and Coraor, Nate and Beek, Marius van den and Parsons, Lance and Marenco, Remi and Freeberg, Mallory and Chambers, Matt and Heusden, Peter van and Stewart, Paul and Crusoe, Michael and Einon, Mark and Taylor, James and Corguillé, Gildas Le and Kuster, Greg Von},
	year = {2016},
	note = {00000},
}

@book{valencia_facial_2004,
	title = {Facial recognition using near infrared images to match visible images},
	copyright = {All rights reserved},
	author = {Valencia, Valorie S. and {Sandalphon} and Crusoe, Michael R.},
	year = {2004},
	note = {00000},
}

@article{alterovitz_enabling_2018,
	title = {Enabling precision medicine via standard communication of {HTS} provenance, analysis, and results},
	volume = {16},
	url = {https://doi.org/10.1371%2Fjournal.pbio.3000099},
	doi = {10.1371/journal.pbio.3000099},
	number = {12},
	journal = {PLOS Biology},
	author = {Alterovitz, Gil and Dean, Dennis and Goble, Carole and Crusoe, Michael R. and Soiland-Reyes, Stian and Bell, Amanda and Hayes, Anais and Suresh, Anita and Purkayastha, Anjan and King, Charles H. and Taylor, Dan and Johanson, Elaine and Thompson, Elaine E. and Donaldson, Eric and Morizono, Hiroki and Tsang, Hsinyi and Vora, Jeet K. and Goecks, Jeremy and Yao, Jianchao and Almeida, Jonas S. and Keeney, Jonathon and Addepalli, KanakaDurga and Krampis, Konstantinos and Smith, Krista M. and Guo, Lydia and Walderhaug, Mark and Schito, Marco and Ezewudo, Matthew and Guimera, Nuria and Walsh, Paul and Kahsay, Robel and Gottipati, Srikanth and Rodwell, Timothy C. and Bloom, Toby and Lai, Yuching and Simonyan, Vahan and Mazumder, Raja},
	month = dec,
	year = {2018},
	note = {00003},
	pages = {e3000099},
}

@book{crusoe_criteria-based_2015,
	title = {Criteria-based assessment of the khmer suite v1.0},
	url = {http://doi.org/10.6084/M9.FIGSHARE.1558322},
	author = {Crusoe, Michael R. and Crusoe, Michael R.},
	year = {2015},
	doi = {10.6084/M9.FIGSHARE.1558322},
	note = {00000 },
}

@book{crusoe_criteria-based_2015-1,
	title = {Criteria-based assessment of the khmer suite},
	url = {http://doi.org/10.6084/M9.FIGSHARE.1558321},
	author = {Crusoe, Michael R. and Crusoe, Michael R.},
	year = {2015},
	doi = {10.6084/M9.FIGSHARE.1558321},
	note = {00000 },
}

@article{tollis_comparative_2018,
	title = {Comparative genomics reveals accelerated evolution in conserved pathways during the diversification of anole lizards},
	volume = {10},
	copyright = {All rights reserved},
	number = {2},
	journal = {Genome biology and evolution},
	author = {Tollis, Marc and Hutchins, Elizabeth D and Stapley, Jessica and Rupp, Shawn M and Eckalbar, Walter L and Maayan, Inbar and Lasku, Eris and Infante, Carlos R and Dennis, Stuart R and Robertson, Joel A and {others}},
	year = {2018},
	note = {00006},
	pages = {489--506},
}

@book{crusoe_common-workflow-language/cwlprov:_2018,
	title = {Common-{Workflow}-{Language}/{Cwlprov}: {Cwlprov} 0.6.0},
	url = {http://doi.org/10.5281/zenodo.1471585},
	author = {Crusoe, Michael R. and Soiland-Reyes, Stian and Khan, Farah Zaib and Crusoe, Michael R.},
	year = {2018},
	doi = {10.5281/zenodo.1471585},
	note = {00000 },
}

@book{crusoe_finger_2004,
	title = {Finger minutiae-based biometric profile for seafarers' identity documents},
	isbn = {92-2-118217-7},
	author = {Crusoe, Michael R.},
	year = {2004},
	note = {00000},
}

@inproceedings{crusoe_modeling_2009,
	title = {Modeling {Malaria} {Pathogenesis}},
	booktitle = {Arizona {State} {University} {College} of {Liberal} {Arts} and {Sciences} {Undergraduate} {Research} {Symposium}},
	author = {Crusoe, Michael R.},
	year = {2009},
	note = {00000},
}

@book{crusoe_ged_2013,
	title = {{GED} submission to {First} {Workshop} on {Sustainable} {Software} for {Science}: {Practice} and {Experiences}},
	url = {http://doi.org/10.6084/M9.FIGSHARE.791567.V1},
	author = {Crusoe, Michael R. and Crusoe, Michael R. and Brown, C. Titus},
	year = {2013},
	doi = {10.6084/M9.FIGSHARE.791567.V1},
	note = {00000 },
}

@book{crusoe_khmer:_2014,
	title = {khmer: k-mer counting \& filtering {FTW}},
	url = {http://doi.org/10.6084/M9.FIGSHARE.963559.V1},
	author = {Crusoe, Michael R. and Crusoe, Michael R. and Edvenson, Greg and Fish, Jordan and Howe, Adina and McDonald, Eric and Nahum, Joshua and Nanlohy, Kaben and Pell, Jason and Simpson, Jared and Scott, Camille and Zhang, Qingpeng and Brown, C. Titus},
	year = {2014},
	doi = {10.6084/M9.FIGSHARE.963559.V1},
	note = {00004 },
}

@book{crusoe_better_2013,
	title = {Better science through superior software},
	author = {Crusoe, Michael R. and Crusoe, Michael R.},
	year = {2013},
	note = {00000},
}

@book{brown;_khmer:_2014,
	title = {khmer: k-mer counting \&amp; filtering {FTW}},
	url = {http://dx.doi.org/10.6084/M9.FIGSHARE.963559},
	publisher = {Figshare},
	author = {Brown;, Michael R. Crusoe; Greg Edvenson; Jordan Fish; Adina Howe; Eric McDonald; Joshua Nahum; Kaben Nanlohy; Jason Pell; Jared Simpson; Camille Scott; Qingpeng Zhang; C. Titus},
	year = {2014},
	doi = {10.6084/M9.FIGSHARE.963559},
	note = {00000 },
}

@book{crusoe;_khmer_2014,
	title = {The khmer software package: enabling efficient sequence analysis},
	url = {http://dx.doi.org/10.6084/M9.FIGSHARE.979190},
	publisher = {Figshare},
	author = {Crusoe;, C. Titus Brown; Michael R.},
	year = {2014},
	doi = {10.6084/M9.FIGSHARE.979190},
	note = {00023 },
}

@inproceedings{colbry_less_2014,
	title = {Less talking, more doing: crowd-sourcing the integration of {Galaxy} with a high-performance computing cluster},
	booktitle = {Galaxy {Community} {Conference}},
	author = {Colbry, Dirk and Crusoe, Michael R. and Keen, Andy and Mason, Greg and Muffett, Jason and Scholz, Matthew and Teal, Tracy K.},
	month = jul,
	year = {2014},
	note = {00000},
}

@book{amstutz_beyond_2015,
	title = {Beyond {Galaxy}: portable workflows and tool definitions with the {CWL}},
	url = {https://cesgo.genouest.org/resources/129},
	author = {Amstutz, Peter and Tijanić, Nebojša and Soiland-Reyes, Stian and Kern, John and Stojanovic, Luka and Pierce, Tim and Chilton, John and Mikheev, Maxim and Lampa, Samuel and Ménager, Hervé and Frazer, Scott and Malladi, Venkat S. and Crusoe, Michael R.},
	month = jul,
	year = {2015},
	note = {00000},
}

@article{crusoe_khmer_2015,
	title = {The khmer software package: enabling efficient nucleotide sequence analysis [version 1; referees: 2 approved, 1 approved with reservations]},
	url = {http://doi.org/10.12688/f1000research.6924.1},
	doi = {10.12688/f1000research.6924.1},
	author = {Crusoe, Michael R.},
	year = {2015},
	note = {00006},
}

@book{crusoe_common_2016,
	title = {Common workflow language v1.0 \& how it will affect you},
	url = {http://doi.org/10.7490/F1000RESEARCH.1112726.1},
	author = {Crusoe, Michael R. and Crusoe, Michael R.},
	year = {2016},
	doi = {10.7490/F1000RESEARCH.1112726.1},
	note = {00000 },
}

@book{crusoe_common_2016-1,
	title = {Common {Workflow} {Language}, draft 3},
	url = {http://doi.org/10.6084/M9.FIGSHARE.3115156.V1},
	author = {Crusoe, Michael R. and Amstutz, Peter and Andeer, Robin and Chapman, Brad and Chilton, John and Crusoe, Michael R. and Guimerà, Roman Valls and Hernandez, Guillermo Carrasco and Ivkovic, Sinisa and Kartashov, Andrey and Kern, John and Leehr, Dan and Ménager, Hervé and Mikheev, Maxim and Pierce, Tim and Randall, Josh and Soiland-Reyes, Stian and Stojanovic, Luka and Tijanić, Nebojša},
	year = {2016},
	doi = {10.6084/M9.FIGSHARE.3115156.V1},
	note = {00008 },
}

@book{crusoe_khmer_2017,
	title = {The {Khmer} {Project} {V2}.1},
	author = {Crusoe, Michael R. and Standage, Daniel and Aliyari, Ali and Cohen, Lisa J. and Crusoe, Michael R. and Head, Tim and Irber, Luiz and Joslin, Shannon EK and Kingsley, N. B. and Murray, Kevin D. and Neches, Russell and Scott, Camille and Shean, Ryan and Steinbiss, Sascha and Sydney, Cait and Brown, C. Titus},
	year = {2017},
	note = {00000},
}

@book{crusoe_common_2017,
	title = {Common {Workflow} {Language} {User} {Guide}},
	author = {Crusoe, Michael R. and Hodges, Toby and Crusoe, Michael R.},
	year = {2017},
	note = {00000},
}

@article{alterovitz_enabling_2017,
	title = {Enabling {Precision} {Medicine} via standard communication of {NGS} provenance, analysis, and results},
	url = {https://doi.org/10.1101%2F191783},
	doi = {10.1101/191783},
	author = {Alterovitz, Gil and Dean, Dennis A. and Goble, Carole and Crusoe, Michael R. and Soiland-Reyes, Stian and Bell, Amanda and Hayes, Anais and Suresh, Anita and King, Charles Hadley S. and Taylor, Dan and Addepalli, KanakaDurga and Johanson, Elaine and Thompson, Elaine E. and Donaldson, Eric and Morizono, Hiroki and Tsang, Hsinyi and Vora, Jeet K. and Goecks, Jeremy and Yao, Jianchao and Almeida, Jonas S. and Krampis, Konstantinos and Smith, Krista and Guo, Lydia and Walderhaug, Mark and Schito, Marco and Ezewudo, Matthew and Guimera, Nuria and Walsh, Paul and Kahsay, Robel and Gottipati, Srikanth and Rodwell, Timothy C. and Bloom, Toby and Lai, Yuching and Simonyan, Vahan and Mazumder, Raja},
	month = sep,
	year = {2017},
	note = {00002},
}

@article{crusoe_organizing_2018,
	title = {Organizing and running bioinformatics hackathons within {Africa}: {The} {H3ABioNet} cloud computing experience [version 1; referees: awaiting peer review]},
	issn = {2515-9321},
	url = {http://doi.org/10.12688/aasopenres.12847.1},
	doi = {10.12688/aasopenres.12847.1},
	author = {Crusoe, Michael R. and Ahmed, Azza E. and Mpangase, Phelelani T. and Panji, Sumir and Baichoo, Shakuntala and Botha, Gerrit and Fadlelmola, Faisal M. and Hazelhurst, Scott and Heusden, Peter Van and Jongeneel, C. Victor and Joubert, Fourie and Mainzer, Liudmila Sergeevna and Meintjes, Ayton and Armstrong, Don and Crusoe, Michael R. and O'connor, Brian D. and Souilmi, Yassine and Alghali, Mustafa and Aron, Shaun and Bendou, Hocine and Beste, Eugene De and Mbiyavanga, Mamana and Souiai, Oussema and Yi, Long and Zermeno, Jennie and Mulder, Nicola},
	year = {2018},
	note = {00000},
}

@article{crusoe_recommendations_2018,
	title = {Recommendations for the packaging and containerizing of bioinformatics software [version 1; referees: awaiting peer review]},
	issn = {2046-1402},
	url = {http://doi.org/10.12688/f1000research.15140.1},
	doi = {10.12688/f1000research.15140.1},
	author = {Crusoe, Michael R. and Gruening, Bjorn and Sallou, Olivier and Moreno, Pablo and Leprevost, Felipe da Veiga and Ménager, Hervé and Søndergaard, Dan and Röst, Hannes and Sachsenberg, Timo and O'Connor, Brian and Madeira, Fábio and Angel, Victoria Dominguez Del and Crusoe, Michael R. and Varma, Susheel and Blankenberg, Daniel and Jimenez, Rafael C. and Community, BioContainers and Perez-Riverol, Yasset},
	year = {2018},
	note = {00000},
}

@article{baichoo_developing_2018,
	title = {Developing reproducible bioinformatics analysis workflows for heterogeneous computing environments to support {African} genomics},
	volume = {19},
	copyright = {All rights reserved},
	number = {1},
	journal = {BMC bioinformatics},
	author = {Baichoo, Shakuntala and Souilmi, Yassine and Panji, Sumir and Botha, Gerrit and Meintjes, Ayton and Hazelhurst, Scott and Bendou, Hocine and de Beste, Eugene and Mpangase, Phelelani T and Souiai, Oussema and {others}},
	year = {2018},
	note = {00000},
	pages = {457},
}

@inproceedings{simko_search_2018,
	title = {Search for computational workflow synergies in reproducible research data analyses in particle physics and life sciences},
	copyright = {All rights reserved},
	booktitle = {2018 {IEEE} 14th {International} {Conference} on e-{Science} (e-{Science})},
	publisher = {IEEE},
	author = {Šimko, Tibor and Cranmer, Kyle and Crusoe, Michael R and Heinrich, Lukas and Khodak, Anton and Kousidis, Dinos and Rodríguez, Diego},
	year = {2018},
	note = {00000},
	pages = {403--404},
}

@article{gruening_bioinformatics_2018,
	title = {bioinformatics software [version 1; referees: 2 approved with},
	copyright = {All rights reserved},
	author = {Gruening, Bjorn and Sallou, Olivier and Moreno, Pablo and da Veiga Leprevost, Felipe and Ménager, Hervé and Søndergaard, Dan and Röst, Hannes and Sachsenberg, Timo and O'Connor, Brian and Madeira, Fábio and {others}},
	year = {2018},
	note = {00000},
}

@article{gruening_recommendations_2018,
	title = {Recommendations for the packaging and containerizing of bioinformatics software},
	volume = {7},
	copyright = {All rights reserved},
	journal = {F1000Research},
	author = {Gruening, Bjorn and Sallou, Olivier and Moreno, Pablo and da Veiga Leprevost, Felipe and Ménager, Hervé and Søndergaard, Dan and Röst, Hannes and Sachsenberg, Timo and O'Connor, Brian and Madeira, Fábio and {others}},
	year = {2018},
	note = {00002},
}

@article{ahmed_organizing_2018,
	title = {Organizing and running bioinformatics hackathons within {Africa}: {The} {H3ABioNet} cloud computing experience},
	volume = {1},
	copyright = {All rights reserved},
	journal = {AAS Open Research},
	author = {Ahmed, Azza E and Mpangase, Phelelani T and Panji, Sumir and Baichoo, Shakuntala and Botha, Gerrit and Fadlelmola, Faisal M and Hazelhurst, Scott and Van Heusden, Peter and Jongeneel, C Victor and Joubert, Fourie and {others}},
	year = {2018},
	note = {00001},
}

@article{crusoe_khmer_2015-1,
	title = {The khmer software package: enabling efficient nucleotide sequence analysis},
	volume = {4},
	copyright = {All rights reserved},
	journal = {F1000Research},
	author = {Crusoe, Michael R and Alameldin, Hussien F and Awad, Sherine and Boucher, Elmar and Caldwell, Adam and Cartwright, Reed and Charbonneau, Amanda and Constantinides, Bede and Edvenson, Greg and Fay, Scott and {others}},
	year = {2015},
	note = {00127},
}

@inproceedings{austerman_modeling_2009,
	title = {Modeling {Malaria} {Pathogenesis}: {The} {Double}-{Edged} {Sword} of {Nitric} {Oxide}},
	copyright = {All rights reserved},
	booktitle = {Arizona {State} {University} {College} of {Liberal} {Arts} and {Sciences} {Undergraduate} {Research} {Symposium}},
	author = {Austerman, R. J. and Crusoe, Michael R and Malenica, I. and Lindsay, J. I and Salmon, P. and McCaughan, M. and Farahani, T. and Kuang, Y. and Nagy, John D.},
	year = {2009},
	note = {00000},
}

@article{musselman_sid-0002_2006,
	title = {{SID}-0002 {Finger} minutiae-based biometric profile for seafarers' identity documents: {The} standard for the biometric template required by the {Convention} {No}. 185},
	copyright = {All rights reserved},
	author = {Musselman, Cynthia L. and Valencia, Valorie S. and Crusoe, Michael R.},
	year = {2006},
	note = {00000},
}

@inproceedings{crusoe_using_2005,
	title = {Using {Your} {Body} for {Authentication}: {A} {Biometrics} {Guide} for {System} {Administrators}.},
	copyright = {All rights reserved},
	booktitle = {{LISA}},
	author = {Crusoe, Michael R},
	year = {2005},
}

@misc{cromwell,
  doi = {10.7490/f1000research.1114634.1},
  author = {Voss,  Kate and Auwera,  Geraldine Van Der and Gentry,  Jeff},
  title = {Full-stack genomics pipelining with GATK4 + WDL + Cromwell},
  publisher = {F1000Research},
  year = {2017}
}
@Article{debian-med,
author="M{\"o}ller, Steffen
and Prescott, Stuart W.
and Wirzenius, Lars
and Reinholdtsen, Petter
and Chapman, Brad
and Prins, Pjotr
and Soiland-Reyes, Stian
and Kl{\"o}tzl, Fabian
and Bagnacani, Andrea
and Kala{\v{s}}, Mat{\'u}{\v{s}}
and Tille, Andreas
and Crusoe, Michael R.",
title="Robust Cross-Platform Workflows: How Technical and Scientific Communities Collaborate to Develop, Test and Share Best Practices for Data Analysis",
journal="Data Science and Engineering",
year="2017",
month="Sep",
day="01",
volume="2",
number="3",
pages="232--244",
abstract="Information integration and workflow technologies for data analysis have always been major fields of investigation in bioinformatics. A range of popular workflow suites are available to support analyses in computational biology. Commercial providers tend to offer prepared applications remote to their clients. However, for most academic environments with local expertise, novel data collection techniques or novel data analysis, it is essential to have all the flexibility of open-source tools and open-source workflow descriptions. Workflows in data-driven science such as computational biology have considerably gained in complexity. New tools or new releases with additional features arrive at an enormous pace, and new reference data or concepts for quality control are emerging. A well-abstracted workflow and the exchange of the same across work groups have an enormous impact on the efficiency of research and the further development of the field. High-throughput sequencing adds to the avalanche of data available in the field; efficient computation and, in particular, parallel execution motivate the transition from traditional scripts and Makefiles to workflows. We here review the extant software development and distribution model with a focus on the role of integration testing and discuss the effect of common workflow language on distributions of open-source scientific software to swiftly and reliably provide the tools demanded for the execution of such formally described workflows. It is contended that, alleviated from technical differences for the execution on local machines, clusters or the cloud, communities also gain the technical means to test workflow-driven interaction across several software packages.",
issn="2364-1541",
doi="10.1007/s41019-017-0050-4"
}
@article{Gruening2018,
  doi = {10.12688/f1000research.15140.1},
  url = {https://doi.org/10.12688/f1000research.15140.1},
  year  = {2018},
  month = {jun},
  publisher = {F1000 Research,  Ltd.},
  volume = {7},
  pages = {742},
  author = {Bjorn Gruening and Olivier Sallou and Pablo Moreno and Felipe da Veiga Leprevost and Herv{\'{e}} M{\'{e}}nager and Dan S{\o}ndergaard and Hannes R\"{o}st and Timo Sachsenberg and Brian O{\textquotesingle}Connor and F{\'{a}}bio Madeira and Victoria Dominguez Del Angel and Michael R. Crusoe and Susheel Varma and Daniel Blankenberg and Rafael C. Jimenez and Yasset Perez-Riverol and},
  title = {Recommendations for the packaging and containerizing of bioinformatics software},
  journal = {F1000Research}
}

@article{Casati1998,
  doi = {10.1016/s0169-023x(97)00033-5},
  year  = {1998},
  month = {jan},
  publisher = {Elsevier {BV}},
  volume = {24},
  number = {3},
  pages = {211--238},
  author = {F Casati and S Ceri and B Pernici and G Pozzi},
  title = {Workflow evolution},
  journal = {Data {\&} Knowledge Engineering}
}

@article{docker,
 author = {Merkel, Dirk},
 title = {Docker: Lightweight Linux Containers for Consistent Development and Deployment},
 journal = {Linux Journal},
 issue_date = {March 2014},
 volume = {2014},
 number = {239},
 month = mar,
 year = {2014},
 issn = {1075-3583},
 articleno = {2},
 url = {https://www.linuxjournal.com/node/1335702},
 urldate = {2018-11-29},
 acmid = {2600241},
 publisher = {Belltown Media},
 address = {Houston, TX}
} 

@misc{cwl,
  doi = {10.6084/m9.figshare.3115156.v2},
  author = {Amstutz,  Peter and Crusoe,  Michael R. and {Nebojša Tijanić} and Chapman,  Brad and Chilton,  John and Heuer,  Michael and Kartashov,  Andrey and Leehr,  Dan and Ménager,  Hervé and Nedeljkovich,  Maya and Scales,  Matt and Soiland-Reyes,  Stian and Stojanovic,  Luka},
  keywords = {Bioinformatics,  Computational  Biology,  80301 Bioinformatics Software,  Computer Software,  80302 Computer System Architecture,  80501 Distributed and Grid Systems,  Distributed Computing},
  title = {Common Workflow Language,  v1.0},
  publisher = {Figshare},
  year = {2016}
}

@article{Alterovitz2019,
    author = {Alterovitz, Gil AND Dean, Dennis AND Goble, Carole AND Crusoe, Michael R. AND Soiland-Reyes, Stian AND Bell, Amanda AND Hayes, Anais AND Suresh, Anita AND Purkayastha, Anjan AND King, Charles H. AND Taylor, Dan AND Johanson, Elaine AND Thompson, Elaine E. AND Donaldson, Eric AND Morizono, Hiroki AND Tsang, Hsinyi AND Vora, Jeet K. AND Goecks, Jeremy AND Yao, Jianchao AND Almeida, Jonas S. AND Keeney, Jonathon AND Addepalli, KanakaDurga AND Krampis, Konstantinos AND Smith, Krista M. AND Guo, Lydia AND Walderhaug, Mark AND Schito, Marco AND Ezewudo, Matthew AND Guimera, Nuria AND Walsh, Paul AND Kahsay, Robel AND Gottipati, Srikanth AND Rodwell, Timothy C. AND Bloom, Toby AND Lai, Yuching AND Simonyan, Vahan AND Mazumder, Raja},
    journal = {PLOS Biology},
    publisher = {Public Library of Science},
    title = {Enabling precision medicine via standard communication of HTS provenance, analysis, and results},
    year = {2019},
    month = {12},
    volume = {16},
    url = {https://doi.org/10.1371/journal.pbio.3000099},
    pages = {1-14},
    abstract = {This Community Page article presents a communication standard for the provenance of high-throughput sequencing data; a BioCompute Object (BCO) can serve as a history of what was computed, be used as part of a validation process, or provide clarity and transparency of an experimental process to collaborators.},
    number = {12},
    doi = {10.1371/journal.pbio.3000099}
}

@article {Custovic799,
	author = {Custovic, Adnan and Ainsworth, John and Arshad, Hasan and Bishop, Christopher and Buchan, Iain and Cullinan, Paul and Devereux, Graham and Henderson, John and Holloway, John and Roberts, Graham and Turner, Steve and Woodcock, Ashley and Simpson, Angela},
	title = {The Study Team for Early Life Asthma Research (STELAR) consortium {\textquoteleft}Asthma e-lab{\textquoteright}: team science bringing data, methods and investigators together},
	volume = {70},
	number = {8},
	pages = {799--801},
	year = {2015},
	doi = {10.1136/thoraxjnl-2015-206781},
	publisher = {BMJ Publishing Group Ltd},
	abstract = {We created Asthma e-Lab, a secure web-based research environment to support consistent recording, description and sharing of data, computational/statistical methods and emerging findings across the five UK birth cohorts. The e-Lab serves as a data repository for our unified dataset and provides the computational resources and a scientific social network to support collaborative research. All activities are transparent, and emerging findings are shared via the e-Lab, linked to explanations of analytical methods, thus enabling knowledge transfer. eLab facilitates the iterative interdisciplinary dialogue between clinicians, statisticians, computer scientists, mathematicians, geneticists and basic scientists, capturing collective thought behind the interpretations of findings.},
	issn = {0040-6376},
	eprint = {https://thorax.bmj.com/content/70/8/799.full.pdf},
	journal = {Thorax}
}

@inproceedings{Chirigati2016,
  doi = {10.1145/2882903.2899401},
  year  = {2016},
  publisher = {{ACM} Press},
  author = {Fernando Chirigati and R{\'{e}}mi Rampin and Dennis Shasha and Juliana Freire},
  title = {{ReproZip}},
  booktitle = {Proceedings of the 2016 International Conference on Management of Data - {SIGMOD} {\textquotesingle}16}
}

@article{Bergmann2014,
  doi = {10.1186/s12859-014-0369-z},
  year  = {2014},
  month = {dec},
  publisher = {Springer Nature},
  volume = {15},
  number = {1},
  author = {Frank T Bergmann and Richard Adams and Stuart Moodie and Jonathan Cooper and Mihai Glont and Martin Golebiewski and Michael Hucka and Camille Laibe and Andrew K Miller and David P Nickerson and Brett G Olivier and Nicolas Rodriguez and Herbert M Sauro and Martin Scharm and Stian Soiland-Reyes and Dagmar Waltemath and Florent Yvon and Nicolas Le Nov{\`{e}}re},
  title = {{COMBINE} archive and {OMEX} format: one file to share all information to reproduce a modeling project},
  journal = {{BMC} Bioinformatics}
}

@article{Springate2014,
  doi = {10.1371/journal.pone.0099825},
  year  = {2014},
  month = {jun},
  publisher = {Public Library of Science ({PLoS})},
  volume = {9},
  number = {6},
  pages = {e99825},
  author = {David A. Springate and Evangelos Kontopantelis and Darren M. Ashcroft and Ivan Olier and Rosa Parisi and Edmore Chamapiwa and David Reeves},
  editor = {Irene Petersen},
  title = {{ClinicalCodes}: An Online Clinical Codes Repository to Improve the Validity and Reproducibility of Research Using Electronic Medical Records},
  journal = {{PLoS} {ONE}}
}

@incollection{Moreau2008,
  doi = {10.1007/978-3-540-89965-5_31},
  year  = {2008},
  publisher = {Springer Berlin Heidelberg},
  pages = {323--326},
  author = {Luc Moreau and Juliana Freire and Joe Futrelle and Robert E. McGrath and Jim Myers and Patrick Paulson},
  title = {The Open Provenance Model: An Overview},
  booktitle = {Lecture Notes in Computer Science}
}

@article{moreau2009governance,
  title={Governance of the open provenance model},
  author={Moreau, Luc and Freire, Juliana and Futrelle, Joe and Myers, Jim and Paulson, Patrick},
  url = {https://nms.kcl.ac.uk/luc.moreau/papers/governance.pdf},
  note = {Accessed 18 Sep 2018},
  year={2009},
  month={Jun},
  day={15}
}

@article{Moreau2015,
  doi = {10.1016/j.websem.2015.04.001},
  year  = {2015},
  month = {dec},
  publisher = {Elsevier {BV}},
  volume = {35},
  pages = {235--257},
  author = {Luc Moreau and Paul Groth and James Cheney and Timothy Lebo and Simon Miles},
  title = {The rationale of {PROV}},
  journal = {Web Semantics: Science,  Services and Agents on the World Wide Web}
}

@article{Kurtzer2017,
  doi = {10.1371/journal.pone.0177459},
  year  = {2017},
  month = {may},
  publisher = {Public Library of Science ({PLoS})},
  volume = {12},
  number = {5},
  pages = {e0177459},
  author = {Gregory M. Kurtzer and Vanessa Sochat and Michael W. Bauer},
  editor = {Attila Gursoy},
  title = {Singularity: Scientific containers for mobility of compute},
  journal = {{PLOS} {ONE}}
}

@inproceedings{Tolk,
  doi = {10.1109/pads.2006.39},
  publisher = {{IEEE}},
  author = {A. Tolk},
  title = {What Comes After the Semantic Web - {PADS} Implications for the Dynamic Web},
  booktitle = {20th Workshop on Principles of Advanced and Distributed Simulation ({PADS}{\textquotesingle}06)}
}

@article{Grning2018,
  doi = {10.1038/s41592-018-0046-7},
  year  = {2018},
  month = {jul},
  publisher = {Springer Nature},
  volume = {15},
  number = {7},
  pages = {475--476},
  author = {Bj\"{o}rn Gr\"{u}ning and   and Ryan Dale and Andreas Sj\"{o}din and Brad A. Chapman and Jillian Rowe and Christopher H. Tomkins-Tinch and Renan Valieris and Johannes K\"{o}ster},
  title = {Bioconda: sustainable and comprehensive software distribution for the life sciences},
  journal = {Nature Methods}
}

@article{Ivie2018,
 author = {Ivie, Peter and Thain, Douglas},
 title = {Reproducibility in Scientific Computing},
 journal = {ACM Comput. Surv.},
 issue_date = {July 2018},
 volume = {51},
 number = {3},
 month = jul,
 year = {2018},
 issn = {0360-0300},
 pages = {63:1--63:36},
 articleno = {63},
 numpages = {36},
 doi = {10.1145/3186266},
 acmid = {3186266},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Reproducibility, computational science, replicability, reproducible, scientific computing, scientific workflow, scientific workflows, workflow, workflows},
}

@article{sandve_2013,
title = {Ten simple rules for reproducible computational research.},
author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
pages = {e1003285},
year = {2013},
month = {oct},
day = {24},
urldate = {2018-07-13},
journal = {{PLoS} Comput Biol},
volume = {9},
number = {10},
doi = {10.1371/journal.pcbi.1003285},
pmid = {24204232},
pmcid = {PMC3812051},
f1000-projects = {CWLProv}
}

@article{Spjuth2015,
  doi = {10.1186/s13062-015-0071-8},
  year  = {2015},
  month = {aug},
  publisher = {Springer Nature},
  volume = {10},
  number = {1},
  author = {Ola Spjuth and Erik Bongcam-Rudloff and Guillermo Carrasco Hern{\'{a}}ndez and Lukas Forer and Mario Giovacchini and Roman Valls Guimera and Aleksi Kallio and Eija Korpelainen and Maciej M Ka{\'{n}}du{\l}a and Milko Krachunov and David P Kreil and Ognyan Kulev and Pawe{\l} P. {\L}abaj and Samuel Lampa and Luca Pireddu and Sebastian Sch\"{o}nherr and Alexey Siretskiy and Dimitar Vassilev},
  title = {Experiences with workflows for automating data-intensive bioinformatics},
  journal = {Biology Direct}
}

@article{Nekrutenko2012,
  doi = {10.1038/nrg3305},
  year  = {2012},
  month = {sep},
  publisher = {Springer Nature},
  volume = {13},
  number = {9},
  pages = {667--672},
  author = {Anton Nekrutenko and James Taylor},
  title = {Next-generation sequencing data interpretation: enhancing reproducibility and accessibility},
  journal = {Nature Reviews Genetics}
}
@article{Alper2018,
  doi = {10.3390/informatics5010011},
  year  = {2018},
  month = {feb},
  publisher = {{MDPI} {AG}},
  volume = {5},
  number = {1},
  pages = {11},
  author = {Pinar Alper and Khalid Belhajjame and Vasa Curcin and Carole Goble},
  title = {{LabelFlow} Framework for Annotating Workflow Provenance},
  journal = {Informatics}
}
@article{Ison2013,
  doi = {10.1093/bioinformatics/btt113},
  year  = {2013},
  month = {mar},
  publisher = {Oxford University Press ({OUP})},
  volume = {29},
  number = {10},
  pages = {1325--1332},
  author = {J. Ison and M. Kalas and I. Jonassen and D. Bolser and M. Uludag and H. McWilliam and J. Malone and R. Lopez and S. Pettifer and P. Rice},
  title = {{EDAM}: an ontology of bioinformatics operations,  types of data and identifiers,  topics and formats},
  journal = {Bioinformatics}
}

 @article{ludascher2016brief, title={A Brief Tour Through Provenance in Scientific Workflows and Databases}, ISBN={9783319402260}, ISSN={2198-7254}, doi={10.1007/978-3-319-40226-0_7}, journal={Springer Proceedings in Business and Economics}, publisher={Springer International Publishing}, author={Ludäscher, Bertram}, year={2016}, pages={103–126}}


@article{Littauer2012,
  doi = {10.2218/ijdc.v7i2.232},
  year  = {2012},
  month = {oct},
  publisher = {Edinburgh University Library},
  volume = {7},
  number = {2},
  pages = {92--100},
  author = {Richard Littauer and Karthik Ram and Bertram Lud\"{a}scher and William Michener and Rebecca Koskela},
  title = {Trends in Use of Scientific Workflows: Insights from a Public Repository and Recommendations for Best Practice},
  journal = {International Journal of Digital Curation}
}

@article{Gymrek2016,
  doi = {10.1186/s13742-016-0127-4},
  year  = {2016},
  month = {may},
  publisher = {Oxford University Press ({OUP})},
  volume = {5},
  number = {1},
  author = {Melissa Gymrek and Yossi Farjoun},
  title = {Recommendations for open data science},
  journal = {{GigaScience}}
}

@article{Stodden2014,
  doi = {10.5334/jors.ay},
  year  = {2014},
  month = {jul},
  publisher = {Ubiquity Press,  Ltd.},
  volume = {2},
  number = {1},
  author = {Victoria Stodden and Sheila Miguez},
  title = {Best Practices for Computational Science: Software Infrastructure and Environments for Reproducible and Extensible Research},
  journal = {Journal of Open Research Software}
}

@inproceedings{Zhao2012,
  doi = {10.1109/escience.2012.6404482},
  year  = {2012},
  month = {oct},
  publisher = {{IEEE}},
  author = {Jun Zhao and Jose Manuel Gomez-Perez and Khalid Belhajjame and Graham Klyne and Esteban Garcia-Cuesta and Aleix Garrido and Kristina Hettne and Marco Roos and David De Roure and Carole Goble},
  title = {Why workflows break \textendash Understanding and combating decay in Taverna workflows},
  booktitle = {2012 {IEEE} 8th International Conference on E-Science}
}


@article{garijo_2013,
title = {Quantifying reproducibility in computational biology: the case of the tuberculosis drugome.},
author = {Garijo, Daniel and Kinnings, Sarah and Xie, Li and Xie, Lei and Zhang, Yinliang and Bourne, Philip E and Gil, Yolanda},
pages = {e80278},
year = {2013},
month = {nov},
day = {27},
urldate = {2016-04-13},
journal = {{PLoS} {ONE}},
volume = {8},
number = {11},
doi = {10.1371/journal.pone.0080278},
pmid = {24312207},
pmcid = {PMC3842296}
}
@article{stephens_2015,
title = {Big data: astronomical or genomical?},
author = {Stephens, Zachary D and Lee, Skylar Y and Faghri, Faraz and Campbell, Roy H and Zhai, Chengxiang and Efron, Miles J and Iyer, Ravishankar and Schatz, Michael C and Sinha, Saurabh and Robinson, Gene E},
pages = {e1002195},
year = {2015},
month = {jul},
day = {7},
urldate = {2018-07-23},
journal = {{PLoS} Biol},
volume = {13},
number = {7},
issn = {1545-7885},
doi = {10.1371/journal.pbio.1002195},
pmid = {26151137},
pmcid = {PMC4494865},
f1000-projects = {CWLProv},
abstract = {Genomics is a Big Data science and is going to get much bigger, very soon, but it is not known whether the needs of genomics will exceed other Big Data domains. Projecting to the year 2025, we compared genomics with three other major generators of Big Data: astronomy, {YouTube}, and Twitter. Our estimates show that genomics is a "four-headed beast"--it is either on par with or the most demanding of the domains analyzed here in terms of data acquisition, storage, distribution, and analysis. We discuss aspects of new technologies that will need to be developed to rise up and meet the computational challenges that genomics poses for the near future. Now is the time for concerted, community-wide planning for the "genomical" challenges of the next decade.}
}
@article{gonzlezbeltrn_2015,
title = {From Peer-Reviewed to Peer-Reproduced in Scholarly Publishing: The Complementary Roles of Data Models and Workflows in Bioinformatics.},
author = {González-Beltrán, Alejandra and Li, Peter and Zhao, Jun and Avila-Garcia, Maria Susana and Roos, Marco and Thompson, Mark and van der Horst, Eelke and Kaliyaperumal, Rajaram and Luo, Ruibang and Lee, Tin-Lap and Lam, Tak-Wah and Edmunds, Scott C and Sansone, Susanna-Assunta and Rocca-Serra, Philippe},
pages = {e0127612},
url = {http://dx.plos.org/10.1371/journal.pone.0127612},
year = {2015},
month = {jul},
day = {8},
urldate = {2018-07-23},
journal = {{PLoS} {ONE}},
volume = {10},
number = {7},
issn = {1932-6203},
doi = {10.1371/journal.pone.0127612},
pmid = {26154165},
pmcid = {PMC4495984},
f1000-projects = {CWLProv},
abstract = {{MOTIVATION}: Reproducing the results from a scientific paper can be challenging due to the absence of data and the computational tools required for their analysis. In addition, details relating to the procedures used to obtain the published results can be difficult to discern due to the use of natural language when reporting how experiments have been performed. The Investigation/Study/Assay ({ISA}), Nanopublications ({NP}), and Research Objects ({RO}) models are conceptual data modelling frameworks that can structure such information from scientific papers. Computational workflow platforms can also be used to reproduce analyses of data in a principled manner. We assessed the extent by which {ISA}, {NP}, and {RO} models, together with the Galaxy workflow system, can capture the experimental processes and reproduce the findings of a previously published paper reporting on the development of {SOAPdenovo2}, a de novo genome assembler. {RESULTS}: Executable workflows were developed using Galaxy, which reproduced results that were consistent with the published findings. A structured representation of the information in the {SOAPdenovo2} paper was produced by combining the use of {ISA}, {NP}, and {RO} models. By structuring the information in the published paper using these data and scientific workflow modelling frameworks, it was possible to explicitly declare elements of experimental design, variables, and findings. The models served as guides in the curation of scientific information and this led to the identification of inconsistencies in the original published paper, thereby allowing its authors to publish corrections in the form of an errata. {AVAILABILITY}: {SOAPdenovo2} scripts, data, and results are available through the {GigaScience} Database: http://dx.doi.org/10.5524/100044; the workflows are available from {GigaGalaxy}: http://galaxy.cbiit.cuhk.edu.hk; and the representations using the {ISA}, {NP}, and {RO} models are available through the {SOAPdenovo2} case study website http://isa-tools.github.io/soapdenovo2/. {CONTACT}: philippe.rocca-serra@oerc.ox.ac.uk and susanna-assunta.sansone@oerc.ox.ac.uk.}
}
@article{ciccarese_2013,
title = {{PAV} ontology: provenance, authoring and versioning.},
author = {Ciccarese, Paolo and Soiland-Reyes, Stian and Belhajjame, Khalid and Gray, Alasdair Jg and Goble, Carole and Clark, Tim},
pages = {37},
url = {http://dx.doi.org/10.1186/2041-1480-4-37},
year = {2013},
month = {nov},
day = {22},
urldate = {2017-08-16},
journal = {J Biomed Semantics},
volume = {4},
number = {1},
doi = {10.1186/2041-1480-4-37},
pmid = {24267948},
pmcid = {PMC4177195},
f1000-projects = {{CWLProv} and Your publications},
abstract = {{BACKGROUND}: Provenance is a critical ingredient for establishing trust of published scientific content. This is true whether we are considering a data set, a computational workflow, a peer-reviewed publication or a simple scientific claim with supportive evidence. Existing vocabularies such as Dublin Core Terms ({DC} Terms) and the {W3C} Provenance Ontology ({PROV}-O) are domain-independent and general-purpose and they allow and encourage for extensions to cover more specific needs. In particular, to track authoring and versioning information of web resources, {PROV}-O provides a basic methodology but not any specific classes and properties for identifying or distinguishing between the various roles assumed by agents manipulating digital artifacts, such as author, contributor and curator. {RESULTS}: We present the Provenance, Authoring and Versioning ontology ({PAV}, namespace http://purl.org/pav/): a lightweight ontology for capturing "just enough" descriptions essential for tracking the provenance, authoring and versioning of web resources. We argue that such descriptions are essential for digital scientific content. {PAV} distinguishes between contributors, authors and curators of content and creators of representations in addition to the provenance of originating resources that have been accessed, transformed and consumed. We explore five projects (and communities) that have adopted {PAV} illustrating their usage through concrete examples. Moreover, we present mappings that show how {PAV} extends the {W3C} {PROV}-O ontology to support broader interoperability. {METHOD}: The initial design of the {PAV} ontology was driven by requirements from the {AlzSWAN} project with further requirements incorporated later from other projects detailed in this paper. The authors strived to keep {PAV} lightweight and compact by including only those terms that have demonstrated to be pragmatically useful in existing applications, and by recommending terms from existing ontologies when plausible. {DISCU\SSION}: We analyze and compare {PAV} with related approaches, namely Provenance Vocabulary ({PRV}), {DC} Terms and {BIBFRAME}. We identify similarities and analyze differences between those vocabularies and {PAV}, outlining strengths and weaknesses of our proposed model. We specify {SKOS} mappings that align {PAV} with {DC} Terms. We conclude the paper with general remarks on the applicability of {PAV}.}
}
@article{wolstencroft_2013,
title = {The Taverna workflow suite: designing and executing workflows of Web Services on the desktop, web or in the cloud.},
author = {Wolstencroft, Katherine and Haines, Robert and Fellows, Donal and Williams, Alan and Withers, David and Owen, Stuart and Soiland-Reyes, Stian and Dunlop, Ian and Nenadic, Aleksandra and Fisher, Paul and Bhagat, Jiten and Belhajjame, Khalid and Bacall, Finn and Hardisty, Alex and Nieva de la Hidalga, Abraham and Balcazar Vargas, Maria P and Sufi, Shoaib and Goble, Carole},
pages = {W557-61},
year = {2013},
month = {jul},
journal = {Nucleic Acids Res},
volume = {41},
number = {Web Server issue},
doi = {10.1093/nar/gkt328},
pmid = {23640334},
pmcid = {PMC3692062},
f1000-projects = {{CWL} and {CWLProv} and Debianpaper and Your publications},
abstract = {The Taverna workflow tool suite (http://www.taverna.org.uk) is designed to combine distributed Web Services and/or local tools into complex analysis pipelines. These pipelines can be executed on local desktop machines or through larger infrastructure (such as supercomputers, Grids or cloud environments), using the Taverna Server. In bioinformatics, Taverna workflows are typically used in the areas of high-throughput omics analyses (for example, proteomics or transcriptomics), or for evidence gathering methods involving text mining or data mining. Through Taverna, scientists have access to several thousand different tools and resources that are freely available from a large range of life science institutions. Once constructed, the workflows are reusable, executable bioinformatics protocols that can be shared, reused and repurposed. A repository of public workflows is available at http://www.myexperiment.org. This article provides an update to the Taverna tool suite, highlighting new features and developments in the workbench and the Taverna Server.}
}
@article{peng_2011,
title = {Reproducible research in computational science.},
author = {Peng, Roger D},
pages = {1226-1227},
year = {2011},
month = {dec},
day = {2},
journal = {Science},
volume = {334},
number = {6060},
doi = {10.1126/science.1213847},
pmid = {22144613},
pmcid = {PMC3383002},
f1000-projects = {CWLProv},
abstract = {Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.}
}
@article{ison_2013,
title = {{EDAM}: an ontology of bioinformatics operations, types of data and identifiers, topics and formats.},
author = {Ison, Jon and Kalas, Matús and Jonassen, Inge and Bolser, Dan and Uludag, Mahmut and {McWilliam}, Hamish and Malone, James and Lopez, Rodrigo and Pettifer, Steve and Rice, Peter},
pages = {1325-1332},
year = {2013},
month = {may},
day = {15},
journal = {Bioinformatics},
volume = {29},
number = {10},
doi = {10.1093/bioinformatics/btt113},
pmid = {23479348},
pmcid = {PMC3654706},
f1000-projects = {{CWLProv} and Debianpaper},
abstract = {{MOTIVATION}: Advancing the search, publication and integration of bioinformatics tools and resources demands consistent machine-understandable descriptions. A comprehensive ontology allowing such descriptions is therefore required. {RESULTS}: {EDAM} is an ontology of bioinformatics operations (tool or workflow functions), types of data and identifiers, application domains and data formats. {EDAM} supports semantic annotation of diverse entities such as Web services, databases, programmatic libraries, standalone tools, interactive applications, data schemas, datasets and publications within bioinformatics. {EDAM} applies to organizing and finding suitable tools and data and to automating their integration into complex applications or workflows. It includes over 2200 defined concepts and has successfully been used for annotations and implementations. {AVAILABILITY}: The latest stable version of {EDAM} is available in {OWL} format from http://edamontology.org/{EDAM}.owl and in {OBO} format from http://edamontology.org/{EDAM}.obo. It can be viewed online at the {NCBO} {BioPortal} and the {EBI} Ontology Lookup Service. For documentation and license please refer to http://edamontology.org. This article describes version 1.2 available at http://edamontology.org/{EDAM\_1}.2.owl. {CONTACT}: jison@ebi.ac.uk.}
}
@article{muse_1994,
title = {A likelihood approach for comparing synonymous and nonsynonymous nucleotide substitution rates, with application to the chloroplast genome.},
author = {Muse, S V and Gaut, B S},
pages = {715-724},
year = {1994},
month = {sep},
journal = {Mol Biol Evol},
volume = {11},
number = {5},
doi = {10.1093/oxfordjournals.molbev.a040152},
pmid = {7968485},
f1000-projects = {CWLProv},
abstract = {A model of {DNA} sequence evolution applicable to coding regions is presented. This represents the first evolutionary model that accounts for dependencies among nucleotides within a codon. The model uses the codon, as opposed to the nucleotide, as the unit of evolution, and is parameterized in terms of synonymous and nonsynonymous nucleotide substitution rates. One of the model's advantages over those used in methods for estimating synonymous and nonsynonymous substitution rates is that it completely corrects for multiple hits at a codon, rather than taking a parsimony approach and considering only pathways of minimum change between homologous codons. Likelihood-ratio versions of the relative-rate test are constructed and applied to data from the complete chloroplast {DNA} sequences of Oryza sativa, Nicotiana tabacum, and Marchantia polymorpha. Results of these tests confirm previous findings that substitution rates in the chloroplast genome are subject to both lineage-specific and locus-specific effects. Additionally, the new tests suggest tha the rate heterogeneity is due primarily to differences in nonsynonymous substitution rates. Simulations help confirm previous suggestions that silent sites are saturated, leaving no evidence of heterogeneity in synonymous substitution rates.}
}
@article{wilkinson_2016,
title = {The {FAIR} Guiding Principles for scientific data management and stewardship.},
author = {Wilkinson, Mark D and Dumontier, Michel and Aalbersberg, I Jsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and da Silva Santos, Luiz Bonino and Bourne, Philip E and Bouwman, Jildau and Brookes, Anthony J and Clark, Tim and Crosas, Mercè and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T and Finkers, Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J G and Groth, Paul and Goble, Carole and Grethe, Jeffrey S and Heringa, Jaap and 't Hoen, Peter A C and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J and Martone, Maryann E and Mons, Albert and Packer, Abel L and Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and van Schaik, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A and Thompson, Mark and van der Lei, Johan and van Mulligen, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
pages = {160018},
year = {2016},
month = {mar},
day = {15},
urldate = {2018-07-13},
journal = {Sci Data},
volume = {3},
issn = {2052-4463},
doi = {10.1038/sdata.2016.18},
pmid = {26978244},
pmcid = {PMC4792175},
f1000-projects = {CWLProv},
abstract = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders-representing academia, industry, funding agencies, and scholarly publishers-have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the {FAIR} Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the {FAIR} Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the {FAIR} Principles, and includes the rationale behind them, and some exemplar implementations in the community.}
}
@article{freire_2012,
title = {Making Computations and Publications Reproducible with {VisTrails}},
author = {Freire, Juliana and Silva, Claudio T.},
pages = {18-25},
year = {2012},
month = {jul},
journal = {Comput Sci Eng},
volume = {14},
number = {4},
issn = {1521-9615},
doi = {10.1109/{MCSE}.2012.76},
f1000-projects = {CWLProv}
}
@article{afgan_2016,
title = {The Galaxy platform for accessible, reproducible and collaborative biomedical analyses: 2016 update.},
author = {Afgan, Enis and Baker, Dannon and van den Beek, Marius and Blankenberg, Daniel and Bouvier, Dave and Čech, Martin and Chilton, John and Clements, Dave and Coraor, Nate and Eberhard, Carl and Grüning, Björn and Guerler, Aysam and Hillman-Jackson, Jennifer and Von Kuster, Greg and Rasche, Eric and Soranzo, Nicola and Turaga, Nitesh and Taylor, James and Nekrutenko, Anton and Goecks, Jeremy},
pages = {W3-W10},
year = {2016},
month = {jul},
day = {8},
journal = {Nucleic Acids Res},
volume = {44},
number = {W1},
doi = {10.1093/nar/gkw343},
pmid = {27137889},
pmcid = {PMC4987906},
f1000-projects = {{CWL} and {CWLProv} and Debianpaper},
abstract = {High-throughput data production technologies, particularly 'next-generation' {DNA} sequencing, have ushered in widespread and disruptive changes to biomedical research. Making sense of the large datasets produced by these technologies requires sophisticated statistical and computational methods, as well as substantial computational power. This has led to an acute crisis in life sciences, as researchers without informatics training attempt to perform computation-dependent analyses. Since 2005, the Galaxy project has worked to address this problem by providing a framework that makes advanced computational tools usable by non experts. Galaxy seeks to make data-intensive research more accessible, transparent and reproducible by providing a Web-based environment in which users can perform computational analyses and have all of the details automatically tracked for later inspection, publication, or reuse. In this report we highlight recently added features enabling biomedical analyses on a large scale. \copyright The Author(s) 2016. Published by Oxford University Press on behalf of Nucleic Acids Research.}
}
@article{leipzig_2017,
title = {A review of bioinformatic pipeline frameworks.},
author = {Leipzig, Jeremy},
pages = {530-536},
year = {2017},
month = {may},
day = {1},
journal = {Brief Bioinformatics},
volume = {18},
number = {3},
doi = {10.1093/bib/bbw020},
pmid = {27013646},
pmcid = {PMC5429012},
f1000-projects = {{CWL} and {CWLProv} and Debianpaper},
abstract = {High-throughput bioinformatic analyses increasingly rely on pipeline frameworks to process sequence and metadata. Modern implementations of these frameworks differ on three key dimensions: using an implicit or explicit syntax, using a configuration, convention or class-based design paradigm and offering a command line or workbench interface. Here I survey and compare the design philosophies of several current pipeline frameworks. I provide practical recommendations based on analysis requirements and the user base. \copyright The Author 2016. Published by Oxford University Press.}
}
@article{smith_2016,
title = {Software citation principles},
author = {Smith, Arfon M. and Katz, Daniel S. and Niemeyer, Kyle E. and {FORCE11} Software Citation Working Group},
pages = {e86},
url = {https://peerj.com/articles/cs-86},
year = {2016},
month = {sep},
day = {19},
urldate = {2017-09-26},
journal = {{PeerJ} Computer Science},
volume = {2},
issn = {2376-5992},
doi = {10.7717/peerj-cs.86},
f1000-projects = {{CWLProv} and Reproducibility}
}
@article{bergmann_2014,
title = {{COMBINE} archive and {OMEX} format: one file to share all information to reproduce a modeling project.},
author = {Bergmann, Frank T and Adams, Richard and Moodie, Stuart and Cooper, Jonathan and Glont, Mihai and Golebiewski, Martin and Hucka, Michael and Laibe, Camille and Miller, Andrew K and Nickerson, David P and Olivier, Brett G and Rodriguez, Nicolas and Sauro, Herbert M and Scharm, Martin and Soiland-Reyes, Stian and Waltemath, Dagmar and Yvon, Florent and Le Novère, Nicolas},
pages = {369},
url = {http://dx.doi.org/10.1186/s12859-014-0369-z},
year = {2014},
month = {dec},
day = {14},
urldate = {2017-08-16},
journal = {{BMC} Bioinformatics},
volume = {15},
number = {1},
doi = {10.1186/s12859-014-0369-z},
pmid = {25494900},
pmcid = {PMC4272562},
f1000-projects = {{CWLProv} and Your publications},
abstract = {{BACKGROUND}: With the ever increasing use of computational models in the biosciences, the need to share models and reproduce the results of published studies efficiently and easily is becoming more important. To this end, various standards have been proposed that can be used to describe models, simulations, data or other essential information in a consistent fashion. These constitute various separate components required to reproduce a given published scientific result. {RESULTS}: We describe the Open Modeling {EXchange} format ({OMEX}). Together with the use of other standard formats from the Computational Modeling in Biology Network ({COMBINE}), {OMEX} is the basis of the {COMBINE} Archive, a single file that supports the exchange of all the information necessary for a modeling and simulation experiment in biology. An {OMEX} file is a {ZIP} container that includes a manifest file, listing the content of the archive, an optional metadata file adding information about the archive and its content, and the files describing the model. The content of a {COMBINE} Archive consists of files encoded in {COMBINE} standards whenever possible, but may include additional files defined by an Internet Media Type. Several tools that support the {COMBINE} Archive are available, either as independent libraries or embedded in modeling software. {CONCLUSIONS}: The {COMBINE} Archive facilitates the reproduction of modeling and simulation experiments in biology by embedding all the relevant information in one file. Having all the information stored and exchanged at once also helps in building activity logs and audit trails. We anticipate that the {COMBINE} Archive will become a significant help for modellers, as the domain moves to larger, more complex experiments such as multi-scale models of organs, digital organisms, and bioengineering.}
}
@inproceedings{garijo_2011,
title = {A new approach for publishing workflows: Abstractions, standards, and linked data},
author = {Garijo, Daniel and Gil, Yolanda},
pages = {47},
publisher = {{ACM} Press},
url = {http://dl.acm.org/citation.cfm?doid=2110497.2110504},
year = {2011},
month = {nov},
day = {14},
urldate = {2018-07-13},
isbn = {9781450311007},
doi = {10.1145/2110497.2110504},
address = {New York, New York, {USA}},
f1000-projects = {CWLProv},
booktitle = {Proceedings of the 6th workshop on Workflows in support of large-scale science - {WORKS} '11}
}
@article{kaushik_2017,
title = {Rabix: an open-source workflow executor supporting recomputability and interoperability of workflow descriptions.},
author = {Kaushik, Gaurav and Ivkovic, Sinisa and Simonovic, Janko and Tijanic, Nebojsa and Davis-Dusenbery, Brandi and Kural, Deniz},
pages = {154-165},
year = {2017},
urldate = {2017-11-03},
journal = {Pac Symp Biocomput},
volume = {22},
doi = {10.1142/9789813207813\_0016},
pmid = {27896971},
pmcid = {PMC5166558},
f1000-projects = {{CWL} and {CWLProv}},
abstract = {As biomedical data has become increasingly easy to generate in large quantities, the methods used to analyze it have proliferated rapidly. Reproducible and reusable methods are required to learn from large volumes of data reliably. To address this issue, numerous groups have developed workflow specifications or execution engines, which provide a framework with which to perform a sequence of analyses. One such specification is the Common Workflow Language, an emerging standard which provides a robust and flexible framework for describing data analysis tools and workflows. In addition, reproducibility can be furthered by executors or workflow engines which interpret the specification and enable additional features, such as error logging, file organization, optim1izations to computation and job scheduling, and allow for easy computing on large volumes of data. To this end, we have developed the Rabix Executor, an open-source workflow engine for the purposes of improving reproducibility through reusability and interoperability of workflow descriptions.}
}
@article{stodden_2016,
title = {Enhancing reproducibility for computational methods.},
author = {Stodden, Victoria and {McNutt}, Marcia and Bailey, David H and Deelman, Ewa and Gil, Yolanda and Hanson, Brooks and Heroux, Michael A and Ioannidis, John P A and Taufer, Michela},
pages = {1240-1241},
year = {2016},
month = {dec},
day = {9},
urldate = {2018-07-23},
journal = {Science},
volume = {354},
number = {6317},
issn = {0036-8075},
doi = {10.1126/science.aah6168},
pmid = {27940837},
f1000-projects = {CWLProv}
}
@article{oconnor_2017,
title = {The Dockstore: enabling modular, community-focused sharing of Docker-based genomics tools and workflows.},
author = {O'Connor, Brian D and Yuen, Denis and Chung, Vincent and Duncan, Andrew G and Liu, Xiang Kun and Patricia, Janice and Paten, Benedict and Stein, Lincoln and Ferretti, Vincent},
pages = {52},
year = {2017},
month = {jan},
day = {18},
urldate = {2018-07-13},
journal = {F1000Res},
volume = {6},
doi = {10.12688/f1000research.10137.1},
pmid = {28344774},
pmcid = {PMC5333608},
f1000-projects = {{CWLProv} and Debianpaper},
abstract = {As genomic datasets continue to grow, the feasibility of downloading data to a local organization and running analysis on a traditional compute environment is becoming increasingly problematic. Current large-scale projects, such as the {ICGC} {PanCancer} Analysis of Whole Genomes ({PCAWG}), the Data Platform for the U.S. Precision Medicine Initiative, and the {NIH} Big Data to Knowledge Center for Translational Genomics, are using cloud-based infrastructure to both host and perform analysis across large data sets. In {PCAWG}, over 5,800 whole human genomes were aligned and variant called across 14 cloud and {HPC} environments; the processed data was then made available on the cloud for further analysis and sharing. If run locally, an operation at this scale would have monopolized a typical academic data centre for many months, and would have presented major challenges for data storage and distribution. However, this scale is increasingly typical for genomics projects and necessitates a rethink of how analytical tools are packaged and moved to the data. For {PCAWG}, we embraced the use of highly portable Docker images for encapsulating and sharing complex alignment and variant calling workflows across highly variable environments. While successful, this endeavor revealed a limitation in Docker containers, namely the lack of a standardized way to describe and execute the tools encapsulated inside the container. As a result, we created the Dockstore ( https://dockstore.org), a project that brings together Docker images with standardized, machine-readable ways of describing and running the tools contained within. This service greatly improves the sharing and reuse of genomics tools and promotes interoperability with similar projects through emerging web service standards developed by the Global Alliance for Genomics and Health ({GA4GH}).}
}
@article{ditommaso_2017,
title = {Nextflow enables reproducible computational workflows.},
author = {Di Tommaso, Paolo and Chatzou, Maria and Floden, Evan W and Barja, Pablo Prieto and Palumbo, Emilio and Notredame, Cedric},
pages = {316-319},
year = {2017},
month = {apr},
day = {11},
urldate = {2018-07-13},
journal = {Nat Biotechnol},
volume = {35},
number = {4},
doi = {10.1038/nbt.3820},
pmid = {28398311},
f1000-projects = {{CWL} and {CWLProv} and Debianpaper}
}

@online{nextflow_tracing,
 title = {Nextflow: Tracing and visualization },
 author = {{Centre for Genomic Regulation (CRG)}},
 year = 2018,
 url = {https://www.nextflow.io/docs/latest/tracing.html#trace-report},
 urldate = {2018-11-28},
 note = {Accessed 28 Nov 2018}
}

@misc{NogalesPoster2018,
author = "Garriga Nogales, Edgar and Di Tommaso, Paolo and Notredame, Cedric",
title = "{{Nextflow integration for the Research Object Specification}}",
publisher = "Figshare",
year = "2018",
month = "10",
note = { Poster at Workshop on Research Objects (RO2018)},
doi = "10.5281/zenodo.1472384"
}

@proceedings{nogales_abstract_2018,
  title        = {{Nextflow integration for the Research Object 
                   Specification}},
  year         = 2018,
  publisher    = {Zenodo},
  month        = jul,
  note         = {{Poster. At Workshop on Research Objects (RO 2018), 29 Oct 2018, Amsterdam, Netherlands.}},
  doi          = {10.5281/zenodo.1472385}
}

@article{guimera_2012,
title = {bcbio-nextgen: Automated, distributed next-gen sequencing pipeline},
author = {Guimera, Roman Valls},
pages = {30},
year = {2012},
month = {feb},
day = {28},
urldate = {2017-09-07},
journal = {{EMBnet} j.},
volume = {17},
number = {B},
issn = {2226-6089},
doi = {10.14806/ej.17.B.286},
f1000-projects = {{CWL} and {CWLProv} and Debianpaper}
}
@article{hettne_2014,
title = {Structuring research methods and data with the research object model: genomics workflows as a case study.},
author = {Hettne, Kristina M and Dharuri, Harish and Zhao, Jun and Wolstencroft, Katherine and Belhajjame, Khalid and Soiland-Reyes, Stian and Mina, Eleni and Thompson, Mark and Cruickshank, Don and Verdes-Montenegro, Lourdes and Garrido, Julian and de Roure, David and Corcho, Oscar and Klyne, Graham and van Schouwen, Reinout and 't Hoen, Peter A C and Bechhofer, Sean and Goble, Carole and Roos, Marco},
pages = {41},
year = {2014},
month = {sep},
day = {18},
urldate = {2018-07-13},
journal = {J Biomed Semantics},
volume = {5},
number = {1},
doi = {10.1186/2041-1480-5-41},
pmid = {25276335},
pmcid = {PMC4177597},
f1000-projects = {{CWLProv} and Your publications},
abstract = {{BACKGROUND}: One of the main challenges for biomedical research lies in the computer-assisted integrative study of large and increasingly complex combinations of data in order to understand molecular mechanisms. The preservation of the materials and methods of such computational experiments with clear annotations is essential for understanding an experiment, and this is increasingly recognized in the bioinformatics community. Our assumption is that offering means of digital, structured aggregation and annotation of the objects of an experiment will provide necessary meta-data for a scientist to understand and recreate the results of an experiment. To support this we explored a model for the semantic description of a workflow-centric Research Object ({RO}), where an {RO} is defined as a resource that aggregates other resources, e.g., datasets, software, spreadsheets, text, etc. We applied this model to a case study where we analysed human metabolite variation by workflows. {RESULTS}: We present the application of the workflow-centric {RO} model for our bioinformatics case study. Three workflows were produced following recently defined Best Practices for workflow design. By modelling the experiment as an {RO}, we were able to automatically query the experiment and answer questions such as "which particular data was input to a particular workflow to test a particular hypothesis?", and "which particular conclusions were drawn from a particular workflow?". {CONCLUSIONS}: Applying a workflow-centric {RO} model to aggregate and annotate the resources used in a bioinformatics experiment, allowed us to retrieve the conclusions of the experiment in the context of the driving hypothesis, the executed workflows and their input data. The {RO} model is an extendable reference model that can be used by other systems as well. {AVAILABILITY}: The Research Object is available at http://www.myexperiment.org/packs/428 The {Wf4Ever} Research Object Model is available at https://w3id.org/ro/2016-01-28/.}
}
@article{kurtzer_2017,
title = {Singularity: Scientific containers for mobility of compute.},
author = {Kurtzer, Gregory M and Sochat, Vanessa and Bauer, Michael W},
pages = {e0177459},
year = {2017},
month = {may},
day = {11},
urldate = {2017-11-03},
journal = {{PLoS} {ONE}},
volume = {12},
number = {5},
doi = {10.1371/journal.pone.0177459},
pmid = {28494014},
pmcid = {PMC5426675},
f1000-projects = {{CWLProv} and Debianpaper},
abstract = {Here we present Singularity, software developed to bring containers and reproducibility to scientific computing. Using Singularity containers, developers can work in reproducible environments of their choosing and design, and these complete environments can easily be copied and executed on other platforms. Singularity is an open source initiative that harnesses the expertise of system and software engineers and researchers alike, and integrates seamlessly into common workflows for both of these groups. As its primary use case, Singularity brings mobility of computing to both users and {HPC} centers, providing a secure means to capture and distribute software and compute environments. This ability to create and deploy reproducible environments across these centers, a previously unmet need, makes Singularity a game changing development for computational science.}
}
@inproceedings{robinson_2017,
title = {Common Workflow Language Viewer},
author = {Robinson, Mark and Soiland-Reyes, Stian and Crusoe, Michael R. and Goble, Carole},
url = {https://view.commonwl.org/},
year = {2017},
month = {jul},
day = {22},
urldate = {2017-08-16},
f1000-projects = {{CWLProv} and Your publications},
abstract = {The Common Workflow Language ({CWL}) project emerged from the {BOSC} 2014 Codefest as a grassroots, multi-vendor working group to tackle the portability of data analysis workflows. It\textquoterights specification for describing workflows and command line tools aims to make them portable and scalable across a variety of computing platforms. At its heart {CWL} is a set of structured text files ({YAML}) with various extensibility points to the format. However, the {CWL} syntax and multi-file collections are not conducive to workflow browsing, exchange and understanding: for this we need a visualization suite.{CWL} Viewer is a richly featured {CWL} visualization suite that graphically presents and lists the details of {CWL} workflows with their inputs, outputs and steps. It also packages the {CWL} files into a downloadable Research Object Bundle including attribution, versioning and dependency metadata in the manifest, allowing it to be easily shared. The tool operates over any workflow held in a {GitHub} repository. Other features include: path visualization from parents and children nodes; nested workflows support; workflow graph download in a range of image formats; a gallery of previously submitted workflows; and support for private git repositories and public {GitHub} including live updates over versioned workflows. The {CWL} Viewer is the de facto {CWL} visualization suite and has been enthusiastically received by the {CWL} community.Project Website: https://view.commonwl.org/Source Code: https://github.com/common-workflow-language/cwlviewer(https://doi.org/10.5281/zenodo.823535)Software License: Apache License, Version 2.{0Submitted} abstract: {CWL} Viewer: The Common Workflow Language {ViewerPoster}: https://doi.org/10.7490/f1000research.1114375.{1Technical} Report: Reproducible Research using Research Objects(https://doi.org/10.5281/zenodo.823295)}
}
@inproceedings{robinson_2017a,
title = {{CWL} Viewer},
author = {Robinson, Mark and Soiland-Reyes, Stian and Crusoe, Michael R. and Goble, Carole},
url = {https://view.commonwl.org/},
year = {2017},
month = {jul},
day = {6},
urldate = {2017-08-16},
f1000-projects = {{CWLProv} and Your publications},
abstract = {The Common Workflow Language ({CWL}) project emerged from the {BOSC} 2014 Codefest as a grassroots, multi-vendor working group to tackle the portability of data analysis workflows. It\textquoterights specification for describing workflows and command line tools aims to make them portable and scalable across a variety of computing platforms. At its heart {CWL} is a set of structured text files ({YAML}) with various extensibility points to the format. However, the {CWL} syntax and multi-file collections are not conducive to workflow browsing, exchange and understanding: for thiswe need a visualization suite. {CWL} Viewer is a richly featured {CWL} visualization suite that graphically presents and lists the details of {CWL} workflows with their inputs, outputs and steps. It also packages the {CWL} files into a downloadable Research Object Bundle including attribution, versioning and dependency metadata in the manifest, allowing it to be easily shared. The tool operates over any workflow held in a {GitHub} repository. Other features include: path visualization from parents and children nodes; nested workflows support; workflow graph download in a range of image formats; a gallery of previously submitted workflows; and support for private git repositories and public {GitHub} including live updates over versioned workflows. The {CWL} Viewer is the de facto {CWL} visualization suite and has been enthusiastically received by the {CWL} community}
}
@article{mcmurry_2017,
title = {Identifiers for the 21st century: How to design, provision, and reuse persistent identifiers to maximize utility and impact of life science data.},
author = {{McMurry}, Julie A and Juty, Nick and Blomberg, Niklas and Burdett, Tony and Conlin, Tom and Conte, Nathalie and Courtot, Mélanie and Deck, John and Dumontier, Michel and Fellows, Donal K and Gonzalez-Beltran, Alejandra and Gormanns, Philipp and Grethe, Jeffrey and Hastings, Janna and Hériché, Jean-Karim and Hermjakob, Henning and Ison, Jon C and Jimenez, Rafael C and Jupp, Simon and Kunze, John and Laibe, Camille and Le Novère, Nicolas and Malone, James and Martin, Maria Jesus and {McEntyre}, Johanna R and Morris, Chris and Muilu, Juha and Müller, Wolfgang and Rocca-Serra, Philippe and Sansone, Susanna-Assunta and Sariyar, Murat and Snoep, Jacky L and Soiland-Reyes, Stian and Stanford, Natalie J and Swainston, Neil and Washington, Nicole and Williams, Alan R and Wimalaratne, Sarala M and Winfree, Lilly M and Wolstencroft, Katherine and Goble, Carole and Mungall, Christopher J and Haendel, Melissa A and Parkinson, Helen},
pages = {e2001414},
year = {2017},
month = {jun},
day = {29},
urldate = {2018-07-13},
journal = {{PLoS} Biol},
volume = {15},
number = {6},
doi = {10.1371/journal.pbio.2001414},
pmid = {28662064},
pmcid = {PMC5490878},
f1000-projects = {{CWLProv} and Your publications},
abstract = {In many disciplines, data are highly decentralized across thousands of online databases (repositories, registries, and knowledgebases). Wringing value from such databases depends on the discipline of data science and on the humble bricks and mortar that make integration possible; identifiers are a core component of this integration infrastructure. Drawing on our experience and on work by other groups, we outline 10 lessons we have learned about the identifier qualities and best practices that facilitate large-scale data integration. Specifically, we propose actions that identifier practitioners (database providers) should take in the design, provision and reuse of identifiers. We also outline the important considerations for those referencing identifiers in various circumstances, including by authors and data generators. While the importance and relevance of each lesson will vary by context, there is a need for increased awareness about how to avoid and manage common identifier problems, especially those related to persistence and web-accessibility/resolvability. We focus strongly on web-based identifiers in the life sciences; however, the principles are broadly relevant to other disciplines.}
}
@inproceedings{khan_2017,
title = {{CWL}+Research Object == Complete Provenance},
author = {Khan, Farah Zaib and Soiland-Reyes, Stian and Lonie, Andrew and Sinnott, Richard},
url = {https://github.com/common-workflow-language/common-workflow-language/wiki/Research-Object-Proposal},
year = {2017},
month = {jun},
day = {14},
urldate = {2017-08-16},
f1000-projects = {{CWLProv} and Your publications},
abstract = {The term Provenance is referred to as {\textquoteleftThe} beginning of something\textquoterights existence; something\textquoterights origin\textquoteright Or {\textquoteleftA} record of ownership of a work of art or an antique, used as a guide to authenticity or quality\textquoteright. Provenance tracking is crucial in scientific studies where workflows have emerged as an exemplar approach to mechanize data-intensive analyses. Gil et al. analyze challenges of scientific workflows and concluded that formally specified workflow helps\textquoteleftaccelerate the rate of scientific process\textquoteright and facilitates others to reproduce the given experiment provided that provenance of end-to-end process at every level is captured.We have implemented exemplar {GATK} variant calling workflow using three approaches to workflow definition namely Galaxy, {CWL} and Cpipe to identify assumptions implicit in these approaches. These assumptions lead to limited or no understanding of reproducibility requirements due to lack of documentation and comprehensive provenance tracking and resulted in identification of provenance information crucial for genomic workflows.{CWL} provides a declarative approach to workflow declaration making minimal assumptions about precise software environment, base software dependencies, configuration settings, alteration of parameters and software versions. It aims to provide an open source extensible standard to build flexible and customized workflows including intricate details of every process. It facilitates capture of information by supporting declaration of requirements, `cwl:tool` and checksums etc. Currently, there is no mechanism to gather the produced information as a result of a workflow run into one bundle for future use. We propose to demonstrate the implementation of a module for {CWL}.}
}
@inproceedings{chard_2016,
title = {I'll take that to go: Big data bags and minimal identifiers for exchange of large, complex datasets},
author = {Chard, Kyle and D'Arcy, Mike and Heavner, Ben and Foster, Ian and Kesselman, Carl and Madduri, Ravi and Rodriguez, Alexis and Soiland-Reyes, Stian and Goble, Carole and Clark, Kristi and Deutsch, Eric W. and Dinov, Ivo and Price, Nathan and Toga, Arthur},
pages = {319-328},
publisher = {IEEE},
year = {2016},
month = {dec},
day = {5},
urldate = {2018-07-13},
isbn = {978-1-4673-9005-7},
doi = {10.1109/bigdata.2016.7840618},
f1000-projects = {{CWLProv} and Your publications},
abstract = {Big data workflows often require the assembly and exchange of complex, multi-element datasets. For example, in biomedical applications, the input to an analytic pipeline can be a dataset consisting thousands of images and genome sequences assembled from diverse repositories, requiring a description of the contents of the dataset in a concise and unambiguous form. Typical approaches to creating datasets for big data workflows assume that all data reside in a single location, requiring costly data marshaling and permitting errors of omission and commission because dataset members are not explicitly specified. We address these issues by proposing simple methods and tools for assembling, sharing, and analyzing large and complex datasets that scientists can easily integrate into their daily workflows. These tools combine a simple and robust method for describing data collections ({BDBags}), data descriptions (Research Objects), and simple persistent identifiers (Minids) to create a powerful ecosystem of tools and services for big data analysis and sharing. We present these tools and use biomedical case studies to illustrate their use for the rapid assembly, sharing, and analysis of large datasets.},
booktitle = {2016 {IEEE} International Conference on Big Data (Big Data)}
}

@article{tavernaprov,
author = {Soiland-Reyes, Stian and Alper, Pinar and Goble, Carole},
title = {Tracking workflow execution with {TavernaProv}.},
note = {PROV Three Years Later; workshop at Provenance Week 2016, McLean, Virginia, USA.},
doi = {10.5281/zenodo.51314},
year = {2016},
month = {jun},
day = {6},
}
@book{nies_2014,
title = {{PROV}-Dictionary: Modeling Provenance for Dictionary Data Structures},
author = {Nies, Tom De and Coppens, Sam and Missier, Paolo and Moreau, Luc and Cheney, James and Lebo, Timothy and Soiland-Reyes, Stian},
publisher = {W3C},
year = {2014},
month = {apr},
day = {30},
urldate = {2017-08-16},
f1000-projects = {{CWLProv} and Your publications},
abstract = {Provenance is information about entities, activities, and people involved in producing a piece of data or thing, which can be used to form assessments about its quality, reliability or trustworthiness. This document describes extensions to {PROV} to facilitate the modeling of provenance for dictionary data structures. {PROV}-{DM} specifies a Collection as an entity that provides a structure to some constituents, which are themselves entities. However, some applications may need a mechanism to specify more structure to a Collection, in order to accurately describe its provenance. Therefore, in this document, we introduce Dictionary, a specific type of Collection with a logical structure consisting of key-entity pairs.}
}
@article{ciccarese_2013a,
title = {Web Annotation as a First-Class Object},
author = {Ciccarese, Paolo and Soiland-Reyes, Stian and Clark, Tim},
pages = {71-75},
year = {2013},
month = {nov},
urldate = {2018-07-13},
journal = {{IEEE} Internet Comput},
volume = {17},
number = {6},
issn = {1089-7801},
doi = {10.1109/{MIC}.2013.123},
f1000-projects = {{CWLProv} and Your publications}
}
@online{moreau_2013,
title = {{PROV}-N: The Provenance Notation},
author = {Moreau, Luc and Missier, Paolo and Cheney, James and Soiland-Reyes, Stian},
year = {2013},
month = {apr},
day = {30},
    url = {http://www.w3.org/TR/2013/REC-prov-n-20130430/},
    note = {W3C Recommendation 30 April 2013},
f1000-projects = {{CWLProv} and Your publications},
abstract = {Provenance is information about entities, activities, and people involved in producing a piece of data or thing, which can be used to form assessments about its quality, reliability or trustworthiness. {PROV}-{DM} is the conceptual data model that forms a basis for the {W3C} provenance ({PROV}) family of specifications. {PROV}-{DM} distinguishes core structures, forming the essence of provenance information, from extended structures catering for more specific uses of provenance. {PROV}-{DM} is organized in six components, respectively dealing with: (1) entities and activities, and the time at which they were created, used, or ended; (2) derivations of entities from entities; (3) agents bearing responsibility for entities that were generated and activities that happened; (4) a notion of bundle, a mechanism to support provenance of provenance; and, (5) properties to link entities that refer to the same thing; (6) collections forming a logical structure for its members.To provide examples of the {PROV} data model, the {PROV} notation ({PROV}-N) is introduced: aimed at human consumption, {PROV}-N allows serializations of {PROV} instances to be created in a compact manner. {PROV}-N facilitates the mapping of the {PROV} data model to concrete syntax, and is used as the basis for a formal semantics of {PROV}. The purpose of this document is to define the {PROV}-N notation.}
}

@online{PROVN,
author = {Moreau, Luc and Missier, Paolo and Cheney, James and Soiland-Reyes, Stian},
    title = {PROV-N: The Provenance Notation},
    url = {http://www.w3.org/TR/2013/REC-prov-n-20130430/},
    year = {2013},
    month = {apr},
    day = {30},
    urldate = {2018-09-22},
    note = {W3C Recommendation 30 April 2013}
}
 @article{wings2011,
author={Y. {Gil} and V. {Ratnakar} and J. {Kim} and P. {Gonzalez-Calero} and P. {Groth} and J. {Moody} and E. {Deelman}},
journal={IEEE Intelligent Systems},
title={Wings: Intelligent Workflow-Based Design of Computational Experiments},
year={2011},
volume={26},
number={1},
pages={62-72},
keywords={data analysis;natural sciences computing;planning (artificial intelligence);software architecture;workflow management software;workflow system;workflow creation;workflow execution;workflow instance generation and specialization;University of Southern California;Information Sciences Institute;workflow validation;Al planning;data set requirement;Computational intelligence;Tracking;Workflow management software;Design methodology;intelligent systems;workflow management;computational experiments;experiment design;software components;computer-supported discovery},
doi={10.1109/MIS.2010.9},
ISSN={1541-1672},
month={Jan},}

@inproceedings{belhajjame_2013,
title = {A workflow {PROV}-corpus based on Taverna and Wings},
author = {Belhajjame, Khalid and Zhao, Jun and Garijo, Daniel and Garrido, Aleix and Soiland-Reyes, Stian and Alper, Pinar and Corcho, Oscar},
pages = {331},
publisher = {{ACM} Press},
url = {http://dl.acm.org/citation.cfm?doid=2457317.2457376},
year = {2013},
month = {mar},
day = {18},
urldate = {2018-07-13},
isbn = {9781450315999},
doi = {10.1145/2457317.2457376},
address = {New York, New York, {USA}},
f1000-projects = {{CWLProv} and Your publications},
booktitle = {Proceedings of the Joint {EDBT}/{ICDT} 2013 Workshops on - {EDBT} '13}
}
@inproceedings{belhajjame_2012,
title = {Workflow-centric research objects: First class citizens in scholarly discourse},
author = {Belhajjame, Khalid and Corcho, Oscar and Garijo, Daniel and Zhao, Jun and Missier, Paolo and Newman, David and Palma, Raúl and Bechhofer, Sean and García Cuesta, Esteban and Gómez-Pérez, José Manuel and Klyne, Graham and Page, Kevin and Roos, Marco and Ruiz, José Enrique and Soiland-Reyes, Stian and Verdes-Montenegro, Lourdes and De Roure, David and Goble, Carole A.},
pages = {1-12},
year = {2012},
url = {http://ceur-ws.org/Vol-903/paper-01.pdf},
urldate = {2017-08-16},
booktitle = {Proceedings of the 2nd Workshop on Semantic Publishing ({SePublica 2012})},
journal = {{CEUR} Workshop Proceedings},
volume = {903},
issn = {1613-0073},
series = {{CEUR} Workshop Proceedings}
}

@inproceedings{belhajjame_2011,
title = {Fostering Scientific Workflow Preservation through Discovery of Substitute Services},
author = {Belhajjame, Khalid and Goble, Carole and Soiland-Reyes, Stian and De Roure, Davide},
pages = {97-104},
publisher = {IEEE},
year = {2011},
month = {dec},
day = {5},
urldate = {2018-07-13},
isbn = {978-1-4577-2163-2},
doi = {10.1109/eScience.2011.22},
f1000-projects = {{CWLProv} and Your publications},
booktitle = {2011 {IEEE} Seventh International Conference on {eScience}}
}
@incollection{missier_2010,
booktitle = {Scientific and statistical database management},
title = {Taverna, Reloaded},
author = {Missier, Paolo and Soiland-Reyes, Stian and Owen, Stuart and Tan, Wei and Nenadic, Alexandra and Dunlop, Ian and Williams, Alan and Oinn, Tom and Goble, Carole},
editor = {Gertz, Michael and Ludäscher, Bertram and Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
series = {Lecture notes in computer science},
pages = {471-481},
publisher = {Springer Berlin Heidelberg},
year = {2010},
urldate = {2017-08-16},
volume = {6187},
isbn = {978-3-642-13817-1},
issn = {0302-9743},
doi = {10.1007/978-3-642-13818-8\_33},
address = {Berlin, Heidelberg},
f1000-projects = {{CWLProv} and Your publications}
}
@article{garijo_2017,
title = {Abstract, link, publish, exploit: An end to end framework for workflow sharing},
author = {Garijo, D and Gil, Y and Corcho, O},
pages = {271-283},
year = {2017},
month = {oct},
urldate = {2018-07-13},
journal = {Future Generation Computer Systems},
volume = {75},
issn = {0167739X},
doi = {10.1016/j.future.2017.01.008},
f1000-projects = {{CWLProv} and Linked Data},
abstract = {Scientific workflows are increasingly used to manage and share scientific computations and methods to analyze data. A variety of systems have been developed that store the workflows executed and make them part of public repositories However, workflows are published in the idiosyncratic format of the workflow system used for the creation and execution of the workflows. Browsing, linking and using the stored workflows and their results often becomes a challenge for scientists who may only be familiar with one system. In this paper we present an approach for addressing this issue by publishing and exploiting workflows as data on the Web with a representation that is independent from the workflow system used to create them. In order to achieve our goal, we follow the Linked Data Principles to publish workflow inputs, intermediate results, outputs and codes; and we reuse and extend well established standards like {W3C} {PROV}. We illustrate our approach by publishing workflows and consuming them with different tools designed to address common scenarios for workflow exploitation.}
}
@article{cohen2017scientific,
title = {Scientific workflows for computational reproducibility in the life sciences: Status, challenges and opportunities},
author = {Cohen-Boulakia, Sarah and Belhajjame, Khalid and Collin, Olivier and Chopard, Jérôme and Froidevaux, Christine and Gaignard, Alban and Hinsen, Konrad and Larmande, Pierre and Bras, Yvan Le and Lemoine, Frédéric and Mareuil, Fabien and Ménager, Hervé and Pradal, Christophe and Blanchet, Christophe},
pages = {284-298},
year = {2017},
month = {oct},
urldate = {2018-07-13},
journal = {Future Generation Computer Systems},
volume = {75},
issn = {0167739X},
doi = {10.1016/j.future.2017.01.012},
f1000-projects = {{CWL} and {CWLProv} and Workflows},
abstract = {With the development of new experimental technologies, biologists are faced with an avalanche of data to be computationally analyzed for scientific advancements and discoveries to emerge. Faced with the complexity of analysis pipelines, the large number of computational tools, and the enormous amount of data to manage, there is compelling evidence that many if not most scientific discoveries will not stand the test of time: increasing the reproducibility of computed results is of paramount importance. The objective we set out in this paper is to place scientific workflows in the context of reproducibility. To do so, we define several kinds of reproducibility that can be reached when scientific workflows are used to perform experiments. We characterize and define the criteria that need to be catered for by reproducibility-friendly scientific workflow systems, and use such criteria to place several representative and widely used workflow systems and companion tools within such a framework. We also discuss the remaining challenges posed by reproducible scientific workflows in the life sciences. Our study was guided by three use cases from the life science domain involving in silico experiments.}
}
@inproceedings{dahuo_2015,
    title = {Smart Container: an ontology towards conceptualizing Docker},
    author = {Da Huo, and Nabrzyski, Jaroslaw and {II}, Charles},
    series = {{CEUR} Workshop Proceedings},
    url = {http://ceur-ws.org/Vol-1486/paper\_89.pdf},
    year = {2015},
    month = {oct},
    day = {11},
    urldate = {2017-10-04},
    volume = {1486},
    issn = {1613-0073},
    f1000-projects = {{CWLProv} and Linked Data and Reproducibility},
    booktitle = {Proceedings of the {ISWC} 2015 Posters \& Demonstrations Track. Co-located with the 14th International Semantic Web Conference ({ISWC}-2015)}
}
@misc{robinson_other_2017,
    title = {{CWL} Viewer: The Common Workflow Language viewer},
    author = {Robinson, Mark and Soiland-Reyes, Stian and Crusoe, Michael R and Goble, Carole},
    year = {2017},
    month = {jul},
    day = {22},
    urldate = {2017-11-03},
    volume = {6},
    f1000-projects = {{CWL} and {CWLProv} and Debianpaper},
    type = {OTHER}
}
@article{kanwal_2017,
    title = {Investigating reproducibility and tracking provenance - A genomic workflow case study.},
    author = {Kanwal, Sehrish and Khan, Farah Zaib and Lonie, Andrew and Sinnott, Richard O},
    pages = {337},
    year = {2017},
    month = {jul},
    day = {12},
    urldate = {2017-11-03},
    journal = {{BMC} Bioinformatics},
    volume = {18},
    number = {1},
    doi = {10.1186/s12859-017-1747-0},
    pmid = {28701218},
    pmcid = {PMC5508699}
}

@article{belhajjame_2015,
    title = {Using a suite of ontologies for preserving workflow-centric research objects},
    author = {Belhajjame, Khalid and Zhao, Jun and Garijo, Daniel and Gamble, Matthew and Hettne, Kristina and Palma, Raul and Mina, Eleni and Corcho, Oscar and Gómez-Pérez, José Manuel and Bechhofer, Sean and Klyne, Graham and Goble, Carole},
    pages = {16-42},
    year = {2015},
    month = {may},
    urldate = {2018-07-13},
    journal = {Web Semantics: Science, Services and Agents on the World Wide Web},
    volume = {32},
    number = {0},
    issn = {15708268},
    doi = {10.1016/j.websem.2015.01.003},
    f1000-projects = {{CWL} and {CWLProv}},
    abstract = {Scientific workflows are a popular mechanism for specifying and automating data-driven in silico experiments. A significant aspect of their value lies in their potential to be reused. Once shared, workflows become useful building blocks that can be combined or modified for developing new experiments. However, previous studies have shown that storing workflow specifications alone is not sufficient to ensure that they can be successfully reused, without being able to understand what the workflows aim to achieve or to re-enact them. To gain an understanding of the workflow, and how it may be used and repurposed for their needs, scientists require access to additional resources such as annotations describing the workflow, datasets used and produced by the workflow, and provenance traces recording workflow executions.In this article, we present a novel approach to the preservation of scientific workflows through the application of research objects-aggregations of data and metadata that enrich the workflow specifications. Our approach is realised as a suite of ontologies that support the creation of workflow-centric research objects. Their design was guided by requirements elicited from previous empirical analyses of workflow decay and repair. The ontologies developed make use of and extend existing well known ontologies, namely the Object Reuse and Exchange ({ORE}) vocabulary, the Annotation Ontology ({AO}) and the {W3C} {PROV} ontology ({PROVO}). We illustrate the application of the ontologies for building Workflow Research Objects with a case-study that investigates Huntington's disease, performed in collaboration with a team from the Leiden University Medial Centre ({HG}-{LUMC}). Finally we present a number of tools developed for creating and managing workflow-centric research objects.}
}
@article{moller_2017,
title = {Robust Cross-Platform Workflows: How Technical and Scientific Communities Collaborate to Develop, Test and Share Best Practices for Data Analysis},
author = {Möller, Steffen and Prescott, Stuart W. and Wirzenius, Lars and Reinholdtsen, Petter and Chapman, Brad and Prins, Pjotr and Soiland-Reyes, Stian and Klötzl, Fabian and Bagnacani, Andrea and Kalaš, Matúš and Tille, Andreas and Crusoe, Michael R.},
pages = {232-244},
year = {2017},
month = {nov},
day = {16},
urldate = {2018-07-13},
journal = {Data Sci. Eng.},
volume = {2},
number = {3},
issn = {2364-1185},
doi = {10.1007/s41019-017-0050-4},
f1000-projects = {CWLProv},
abstract = {Information integration and workflow technologies for data analysis have always been major fields of investigation in bioinformatics. A range of popular workflow suites are available to support analyses in computational biology. Commercial providers tend to offer prepared applications remote to their clients. However, for most academic environments with local expertise, novel data collection techniques or novel data analysis, it is essential to have all the flexibility of open-source tools and open-source workflow descriptions. Workflows in data-driven science such as computational biology have considerably gained in complexity. New tools or new releases with additional features arrive at an enormous pace, and new reference data or concepts for quality control are emerging. A well-abstracted workflow and the exchange of the same across work groups have an enormous impact on the efficiency of research and the further development of the field. High-throughput sequencing adds to the avalanche of data available in the field; efficient computation and, in particular, parallel execution motivate the transition from traditional scripts and Makefiles to workflows. We here review the extant software development and distribution model with a focus on the role of integration testing and discuss the effect of common workflow language on distributions of open-source scientific software to swiftly and reliably provide the tools demanded for the execution of such formally described workflows. It is contended that, alleviated from technical differences for the execution on local machines, clusters or the cloud, communities also gain the technical means to test workflow-driven interaction across several software packages.}
}
@article{steinbaugh_2017,
title = {{bcbioRNASeq}: R package for bcbio {RNA}-seq analysis},
author = {Steinbaugh, Michael J. and Pantano, Lorena and Kirchner, Rory D. and Barrera, Victor and Chapman, Brad A. and Piper, Mary E. and Mistry, Meeta and Khetani, Radhika S. and Rutherford, Kayleigh D. and Hofmann, Oliver and Hutchinson, John N. and Ho Sui, Shannan},
pages = {1976},
year = {2017},
month = {nov},
day = {8},
urldate = {2018-07-23},
journal = {F1000Res},
volume = {6},
doi = {10.12688/f1000research.12093.1},
f1000-projects = {CWLProv},
abstract = {{RNA}-seq analysis involves multiple steps from processing raw sequencing data to identifying, organizing, annotating, and reporting differentially expressed genes. bcbio is an open source, community-maintained framework providing automated and scalable {RNA}-seq methods for identifying gene abundance counts. We have developed {bcbioRNASeq}, a Bioconductor package that provides ready-to-render templates and wrapper functions to post-process bcbio output data. {bcbioRNASeq} automates the generation of high-level {RNA}-seq reports, including identification of differentially expressed genes, functional enrichment analysis and quality control analysis.}
}
@article{hillion_2017,
title = {Using bio.tools to generate and annotate workbench tool descriptions.},
author = {Hillion, Kenzo-Hugo and Kuzmin, Ivan and Khodak, Anton and Rasche, Eric and Crusoe, Michael and Peterson, Hedi and Ison, Jon and Ménager, Hervé},
year = {2017},
month = {nov},
day = {30},
urldate = {2018-07-13},
journal = {F1000Res},
volume = {6},
doi = {10.12688/f1000research.12974.1},
pmid = {29333231},
pmcid = {PMC5747335},
f1000-projects = {CWLProv},
abstract = {Workbench and workflow systems such as Galaxy, Taverna, Chipster, or Common Workflow Language ({CWL})-based frameworks, facilitate the access to bioinformatics tools in a user-friendly, scalable and reproducible way. Still, the integration of tools in such environments remains a cumbersome, time consuming and error-prone process. A major consequence is the incomplete or outdated description of tools that are often missing important information, including parameters and metadata such as publication or links to documentation. {ToolDog} (Tool {DescriptiOn} Generator) facilitates the integration of tools - which have been registered in the {ELIXIR} tools registry (https://bio.tools) - into workbench environments by generating tool description templates. {ToolDog} includes two modules. The first module analyses the source code of the bioinformatics software with language-specific plugins, and generates a skeleton for a Galaxy {XML} or {CWL} tool description. The second module is dedicated to the enrichment of the generated tool description, using metadata provided by bio.tools. This last module can also be used on its own to complete or correct existing tool descriptions with missing metadata.}
}
@article{atkinson_2017,
title = {Scientific workflows: Past, present and future},
author = {Atkinson, Malcolm and Gesing, Sandra and Montagnat, Johan and Taylor, Ian},
pages = {216-227},
year = {2017},
month = {oct},
urldate = {2018-07-13},
journal = {Future Generation Computer Systems},
volume = {75},
issn = {0167739X},
doi = {10.1016/j.future.2017.05.041},
f1000-projects = {{CWLProv} and Workflows},
abstract = {This special issue and our editorial celebrate 10 years of progress with data-intensive or scientific workflows. There have been very substantial advances in the representation of workflows and in the engineering of workflow management systems ({WMS}). The creation and refinement stages are now well supported, with a significant improvement in usability. Improved abstraction supports cross-fertilisation between different workflow communities and consistent interpretation as {WMS} evolve. Through such re-engineering the {WMS} deliver much improved performance, significantly increased scale and sophisticated reliability mechanisms. Further improvement is anticipated from substantial advances in optimisation. We invited papers from those who have delivered these advances and selected 14 to represent today\textquoterights achievements and representative plans for future progress. This editorial introduces those contributions with an overview and categorisation of the papers. Furthermore, it elucidates responses from a survey of major workflow systems, which provides evidence of substantial progress and a structured index of related papers. We conclude with suggestions on areas where further research and development is needed and offer a vision of future research directions.}
}
@article{cuevasvicenttn_2012,
title = {Scientific workflows and provenance: introduction and research opportunities},
author = {Cuevas-Vicenttín, Víctor and Dey, Saumen and Köhler, Sven and Riddle, Sean and Ludäscher, Bertram},
pages = {193-203},
year = {2012},
month = {nov},
urldate = {2018-03-16},
journal = {Datenbank Spektrum},
volume = {12},
number = {3},
issn = {1618-2162},
doi = {10.1007/s13222-012-0100-z},
f1000-projects = {{CWLProv} and Workflows}
}
@incollection{giesler_2017,
booktitle = {High-Performance Scientific Computing},
title = {Uniprov: A flexible provenance tracking system for {UNICORE}},
author = {Giesler, André and Czekala, Myriam and Hagemeier, Björn and Grunzke, Richard},
editor = {Di Napoli, Edoardo and Hermanns, Marc-André and Iliev, Hristo and Lintermann, Andreas and Peyser, Alexander},
series = {Lecture notes in computer science},
pages = {233-242},
publisher = {Springer International Publishing},
year = {2017},
volume = {10164},
isbn = {978-3-319-53861-7},
issn = {0302-9743},
doi = {10.1007/978-3-319-53862-4\_20},
address = {Cham},
f1000-projects = {{CWLProv} and Research Object},
abstract = {In this paper we present a flexible provenance management system called {UniProv}. {UniProv} is an ongoing development project providing provenance tracking in scientific workflows and data management particularly in the field of neuroscience, thus allowing users to validate and reproduce tasks and results of their experiments.The primary goal is to equip the commonly used Grid middleware {UNICORE} [1] and its incorporated workflow engine with the provenance capturing mechanism of {UniProv}. We also explain an approach for using predefined patterns to ensure compatibility with the {W3C} {PROV} [2] Data Model and to map the provenance information properly to a neo4j graph database.}
}
@inproceedings{gomezperez_2017,
title = {Towards a Human-Machine Scientific Partnership Based on Semantically Rich Research Objects},
author = {Gomez-Perez, Jose Manuel and Palma, Raul and Garcia-Silva, Andres},
pages = {266-275},
publisher = {IEEE},
year = {2017},
month = {oct},
day = {24},
urldate = {2018-04-30},
isbn = {978-1-5386-2686-3},
doi = {10.1109/eScience.2017.40},
f1000-projects = {{CWLProv} and Research Object},
abstract = {A research object is a single information unit encapsulating all the knowledge relevant to a particular scientific investigation, their associated metadata and the context where such resources were produced and came into play. Aimed at enhancing the preservation, reuse and scholarly communication of data-intensive science, research objects are both technical and social artifacts that represent a partnership between scientific communities and the computational support required in nowadays science. In this paper, we explore such partnership, identifying the lack of appropriate machine-readable metadata as one of its main inhibitors, and address the semantic enrichment of research objects as one key aspect towards its establishment. Focused on the specific needs of Earth Science communities, we propose extensions to research object representation models and present novel methods and tools to enrich research object metadata through automatic means. Finally, we validate the approach through the implementation of a recommender system that exploits the resulting metadata to facilitate research object discovery and reuse, enabling humans and machines to work together and accelerate the research life cycle.},
booktitle = {2017 {IEEE} 13th International Conference on e-Science (e-Science)}
}
@article{madduri_2018,
    author = {Madduri, Ravi AND Chard, Kyle AND D’Arcy, Mike AND Jung, Segun C. AND Rodriguez, Alexis AND Sulakhe, Dinanath AND Deutsch, Eric AND Funk, Cory AND Heavner, Ben AND Richards, Matthew AND Shannon, Paul AND Glusman, Gustavo AND Price, Nathan AND Kesselman, Carl AND Foster, Ian},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Reproducible big data science: A case study in continuous FAIRness},
    year = {2019},
    month = {04},
    volume = {14},
    url = {https://doi.org/10.1371/journal.pone.0213013},
    pages = {1-22},
    abstract = {Big biomedical data create exciting opportunities for discovery, but make it difficult to capture analyses and outputs in forms that are findable, accessible, interoperable, and reusable (FAIR). In response, we describe tools that make it easy to capture, and assign identifiers to, data and code throughout the data lifecycle. We illustrate the use of these tools via a case study involving a multi-step analysis that creates an atlas of putative transcription factor binding sites from terabytes of ENCODE DNase I hypersensitive sites sequencing data. We show how the tools automate routine but complex tasks, capture analysis algorithms in understandable and reusable forms, and harness fast networks and powerful cloud computers to process data rapidly, all without sacrificing usability or reproducibility—thus ensuring that big data are not hard-to-(re)use data. We evaluate our approach via a user study, and show that 91\% of participants were able to replicate a complex analysis involving considerable data volumes.},
    number = {4},
    doi = {10.1371/journal.pone.0213013}
}
@article{schloss_2018,
title = {Identifying and overcoming threats to reproducibility, replicability, robustness, and generalizability in microbiome research.},
author = {Schloss, Patrick D},
url = {http://mbio.asm.org/lookup/doi/10.1128/{mBio}.00525-18},
year = {2018},
month = {jun},
day = {5},
urldate = {2018-07-13},
journal = {MBio},
volume = {9},
number = {3},
issn = {2150-7511},
doi = {10.1128/{mBio}.00525-18},
pmid = {29871915},
pmcid = {PMC5989067},
f1000-projects = {CWLProv},
abstract = {The "reproducibility crisis" in science affects microbiology as much as any other area of inquiry, and microbiologists have long struggled to make their research reproducible. We need to respect that ensuring that our methods and results are sufficiently transparent is difficult. This difficulty is compounded in interdisciplinary fields such as microbiome research. There are many reasons why a researcher is unable to reproduce a previous result, and even if a result is reproducible, it may not be correct. Furthermore, failures to reproduce previous results have much to teach us about the scientific process and microbial life itself. This Perspective delineates a framework for identifying and overcoming threats to reproducibility, replicability, robustness, and generalizability of microbiome research. Instead of seeing signs of a crisis in others' work, we need to appreciate the technical and social difficulties that limit reproducibility in the work of others as well as our own. Copyright \copyright 2018 Schloss.}
}
@article{grning_2018,
title = {Bioconda: sustainable and comprehensive software distribution for the life sciences.},
author = {Grüning, Björn and Dale, Ryan and Sjödin, Andreas and Chapman, Brad A and Rowe, Jillian and Tomkins-Tinch, Christopher H and Valieris, Renan and Köster, Johannes and Bioconda Team},
pages = {475-476},
url = {http://www.nature.com/articles/s41592-018-0046-7},
year = {2018},
month = {jul},
urldate = {2018-07-13},
journal = {Nat Methods},
volume = {15},
number = {7},
issn = {1548-7091},
doi = {10.1038/s41592-018-0046-7},
pmid = {29967506},
f1000-projects = {{CWLProv} and Workflows}
}
@article{grning_2018a,
title = {Practical computational reproducibility in the life sciences.},
author = {Grüning, Björn and Chilton, John and Köster, Johannes and Dale, Ryan and Soranzo, Nicola and van den Beek, Marius and Goecks, Jeremy and Backofen, Rolf and Nekrutenko, Anton and Taylor, James},
pages = {631-635},
year = {2018},
month = {jun},
day = {27},
urldate = {2018-07-13},
journal = {Cell Syst},
volume = {6},
number = {6},
issn = {24054712},
doi = {10.1016/j.cels.2018.03.014},
pmid = {29953862},
f1000-projects = {CWLProv},
abstract = {Many areas of research suffer from poor reproducibility, particularly in computationally intensive domains where results rely on a series of complex methodological decisions that are not well captured by traditional publication approaches. Various guidelines have emerged for achieving reproducibility, but implementation of these practices remains difficult due to the challenge of assembling software tools plus associated libraries, connecting tools together into pipelines, and specifying parameters. Here, we discuss a suite of cutting-edge technologies that make computational reproducibility not just possible, but practical in both time and effort. This suite combines three well-tested components-a system for building highly portable packages of bioinformatics software, containerization and virtualization technologies for isolating reusable execution environments for these packages, and workflow systems that automatically orchestrate the composition of these packages for entire pipelines-to achieve an unprecedented level of computational reproducibility. We also provide a practical implementation and five recommendations to help set a typical researcher on the path to performing data analyses reproducibly. Copyright \copyright 2018 Elsevier Inc. All rights reserved.}
}
@article{jeffrey_2014,
title = {{SciCrunch}: A cooperative and collaborative data and resource discovery platform for scientific communities},
author = {Jeffrey, Grethe and Anita, Bandrowski and Davis, Banks and Christopher, Condit and Amarnath, Gupta and Stephen, Larson and Yueling, Li and Ibrahim, Ozyurt and Andrea, Stagg and Patricia, Whetzel and Luis, Marenco and Perry, Miller and Rixin, Wang and Gordon, Shepherd and Maryann, Martone},
url = {http://www.frontiersin.org/Community/{AbstractDetails}.aspx?{ABS\_DOI}=10.3389/conf.fninf.2014.18.00069},
year = {2014},
urldate = {2018-07-13},
journal = {Front Neuroinformatics},
volume = {8},
issn = {1662-5196},
doi = {10.3389/conf.fninf.2014.18.00069},
f1000-projects = {CWLProv}
}

@inproceedings{garijo_2014,
    title = {Towards Workflow Ecosystems through Semantic and Standard Representations},
    author = {Garijo, Daniel and Gil, Yolanda and Corcho, Oscar},
    pages = {94-104},
    publisher = {IEEE},
    year = {2014},
    month = {nov},
    day = {16},
    urldate = {2018-07-13},
    isbn = {978-1-4799-7067-4},
    doi = {10.1109/{WORKS}.2014.13},
    booktitle = {2014 9th Workshop on Workflows in Support of Large-Scale Science}
}

@inproceedings{fernando_2007,
title = {Towards Build-Time Interoperability of Workflow Definition Languages},
author = {Fernando, Sarah D. Induruwa and Creager, Douglas A. and Simpson, Andrew C.},
pages = {525-532},
publisher = {IEEE},
year = {2007},
month = {sep},
day = {26},
urldate = {2018-07-13},
doi = {10.1109/{SYNASC}.2007.18},
f1000-projects = {CWLProv},
booktitle = {Ninth International Symposium on Symbolic and Numeric Algorithms for Scientific Computing ({SYNASC} 2007)}
}
@inproceedings{lynch_2007,
title = {The {OAI}-{ORE} effort: Progress, challenges, synergies},
author = {Lynch, Cliff and Parastatidis, Savas and Jacobs, Neil and Van de Sompel, Herbert and Lagoze, Carl},
pages = {80},
publisher = {{ACM} Press},
url = {http://portal.acm.org/citation.cfm?doid=1255175.1255190},
year = {2007},
month = {jun},
day = {18},
urldate = {2018-07-15},
isbn = {9781595936448},
doi = {10.1145/1255175.1255190},
address = {New York, New York, {USA}},
f1000-projects = {CWLProv},
booktitle = {Proceedings of the 2007 conference on Digital libraries - {JCDL} '07}
}
@techreport{farrell_2013,
title = {Naming Things with Hashes},
author = {Farrell, S. and Kutscher, D. and Dannewitz, C. and Ohlman, B. and Keranen, A. and Hallam-Baker, P.},
publisher = {{RFC} Editor},
url = {https://www.rfc-editor.org/info/rfc6920},
year = {2013},
month = {apr},
urldate = {2018-07-15},
doi = {10.17487/rfc6920},
f1000-projects = {CWLProv}
}
@online{soilandreyes_2014,
title = {Research Object Bundle 1.0},
author = {Soiland-Reyes, Stian and Gamble, Matthew and Haines, Robert},
publisher = {researchobject.org},
doi = {10.5281/zenodo.12586},
publisher = {Zenodo},
year = {2014},
month = {nov},
day = {5},
urldate = {2018-07-13},
f1000-projects = {CWLProv},
note = {researchobject.org Specification}
}

@article{cwlprov-preprint,
    title = {{CWLProv} - Interoperable Retrospective Provenance capture and its challenges},
    author = {Khan, Farah Zaib and Soiland-Reyes, Stian and Crusoe, Michael R. and Lonie, Andrew and Sinnott, Richard},
    publisher = {Zenodo},
    doi = {10.5281/zenodo.1215611},
    year = {2018},
    note = {Zenodo preprint},
    month = {mar},
    day = {27},
    urldate = {2018-07-13},
    f1000-projects = {CWLProv},
}
@article{soilandreyes_2018,
    title = {The Archive and Package (arcp) {URI} scheme},
    author = {Soiland-Reyes, Stian and Cáceres, Marcos},
    year = {2018},
    month = {jan},
    day = {17},
    url = {https://tools.ietf.org/id/draft-soilandreyes-arcp-03.html},
    urldate = {2018-07-13},
    journal = {Internet-Draft soilandreyes-arcp },
    f1000-projects = {CWLProv},
    abstract = {This specification define the Archive and Package {URI} scheme arcp.arcp {URIs} can be used to consume or reference hypermedia resources bundled inside a file archive or an application package, as well as to resolve {URIs} for archive resources within a programmatic framework.This {URI} scheme provides mechanisms to generate a unique base {URI} to represent the root of the archive, so that relative {URI} references in a bundled resource can be resolved within the archive without having to extract the archive content on the local file system.An arcp {URI} can be used for purposes of isolation (e.g. when consuming multiple archives), security constraints (avoiding out\textquotedblrightfrom the archive), or for externally identiyfing sub-resources referenced by hypermedia formats.}
}

@misc{belhajjame_film_2014,
title = {A workflow {PROV}-corpus based on Taverna and Wings (Datasets)},
author = {Belhajjame, Khalid and Zhao, Jun and Garijo, Daniel and Garrido, Aleix and Soiland-Reyes, Stian and Alper, Pinar and Corcho, Óscar},
url = {https://github.com/provbench/{Wf4Ever}-{PROV}},
year = {2014},
month = {sep},
day = {1},
urldate = {2018-07-13},
f1000-projects = {CWLProv},
abstract = {This release contains provenance traces that have been obtained by running Taverna and Wing workflows. As well as the traces, the release contains the specifications of the workflows that were ran.},
type = {FILM}
}
@article{robinson_2017b,
title = {{CWL} Viewer: the common workflow language viewer},
author = {Robinson, Mark and Soiland-Reyes, Stian and Crusoe, Michael R and Goble, Carole},
pages = {1075 (poster)},
url = {https://doi.org/10.7490/f1000research.1114375.1},
year = {2017},
month = {jul},
day = {6},
urldate = {2018-07-13},
journal = {F1000Research},
volume = {6},
number = {{ISCB} Comm J},
f1000-projects = {CWLProv}
}
@inproceedings{garijo_2012,
title = {Augmenting {PROV} with Plans in P-{PLAN}: Scientific Processes as Linked Data},
author = {Garijo, Daniel and Gil, Yolanda},
series = {{CEUR} Workshop Proceedings},
url = {http://ceur-ws.org/Vol-951/paper6.pdf},
year = {2012},
month = {nov},
day = {12},
urldate = {2018-07-13},
volume = {951},
issn = {1613-0073},
series = {{CEUR} Workshop Proceedings},
f1000-projects = {CWLProv},
booktitle = {Proceedings of the Second International Workshop on Linked Science 2012 - Tackling Big Data}
}
@techreport{kunze_2018,
title = {The {BagIt} File Packaging Format (V1.0)},
author = {Kunze, John A. and Littman, Justin and Madden, Liz and Scancella, John and Adams, Chris},
publisher = {Internet Engineering Task Force},
url = {https://datatracker.ietf.org/doc/html/draft-kunze-bagit-16},
year = {2018},
month = {jun},
day = {4},
urldate = {2018-07-13},
f1000-projects = {CWLProv},
abstract = {This document describes {BagIt}, a set of hierarchical file layout conventions for storage and transfer of arbitrary digital content. A "bag" has just enough structure to enclose descriptive metadata "tags" and a file "payload" but does not require knowledge of the payload\textquoterights internal semantics. This {BagIt} format is suitable for reliable storage and transfer.},
type = {Internet-Draft}
}
@techreport{researchdatarepositoryinteroperabilitywg_2018,
title = {Research Data Repository Interoperability {WG} Final Recommendations},
author = {Research Data Repository Interoperability {WG},},
publisher = {Research Data Alliance},
url = {https://doi.org/10.15497/{RDA00025}},
year = {2018},
month = {jun},
day = {21},
urldate = {2018-07-13},
f1000-projects = {CWLProv}
}
@techreport{sanderson_2017,
title = {Web Annotation Data Model},
author = {Sanderson, Robert and Ciccarese, Paolo and Young, Benjamin},
series = {{W3C} Recommendation},
publisher = {World Wide Web Consortium},
url = {https://www.w3.org/{TR}/2017/{REC}-annotation-model-20170223/},
year = {2017},
month = {feb},
day = {23},
urldate = {2018-07-13},
f1000-projects = {CWLProv},
type = {{W3C} Recommendation}
}

@online{huynh_2013,
title = {The {PROV}-{JSON} Serialization. A {JSON} Representation for the {PROV} Data Model},
author = {Huynh, Trung Dong and Jewell, Michael and Keshavarz, Amir Sezavar and Michaelides, Danius and Yang, Huanjia and Moreau, Luc},
year = {2013},
month = {Apr},
day = {24},
note = {W3C Member Submission 24 April 2013},
url = {http://www.w3.org/Submission/2013/SUBM-prov-json-20130424/},
urldate = {2018-07-23},
f1000-projects = {CWLProv}
}
@article{garijo_2017a,
title = {Abstract, link, publish, exploit: An end to end framework for workflow sharing},
author = {Garijo, Daniel and Gil, Yolanda and Corcho, Oscar},
pages = {271-283},
year = {2017},
urldate = {2018-07-23},
journal = {Future Gener. Comput. Syst.},
volume = {75},
f1000-projects = {CWLProv}
}
@article{clifford_2008,
title = {Tracking provenance in a virtual data grid},
author = {Clifford, Ben and Foster, Ian and Voeckler, Jens-S and Wilde, Michael and Zhao, Yong},
pages = {565-575},
doi = {10.1002/cpe.1256},
publisher = {John Wiley \& Sons, Ltd.},
year = {2008},
month = {apr},
journal = {Concurrency and Computation: Practice and Experience},
volume = {20},
number = {5},
f1000-projects = {CWLProv},
abstract = {The virtual data model allows data sets to be described prior to, and separately from, their physical materialization. We have implemented this model in a Virtual Data Language ({VDL}) and associated supporting tools, which provide for both the storage, query, and retrieval of virtual data set descriptions, and the automated, on-demand materialization of virtual data sets. We use a standardized data provenance challenge exercise to illustrate the powerful queries that can be performed on the data maintained by these tools, which for a single virtual data set can include three elements: the computational procedure(s) that must be executed to materialize the data set, the runtime log(s) produced by the execution of the computation(s), and optional metadata annotation(s) that associate application semantics with data and procedures. Copyright \copyright 2007 John Wiley \& Sons, Ltd.}
}
@article{leipzig_2017a,
title = {A review of bioinformatic pipeline frameworks},
author = {Leipzig, Jeremy},
pages = {530-536},
year = {2017},
month = {may},
urldate = {2018-07-23},
journal = {Brief. Bioinform.},
volume = {18},
number = {3},
f1000-projects = {CWLProv},
abstract = {High-throughput bioinformatic analyses increasingly rely on pipeline frameworks to process sequence and metadata. Modern implementations of these frameworks differ on three key dimensions: using an implicit or explicit syntax, using a configuration, convention or class-based design paradigm and offering a command line or workbench interface. Here I survey and compare the design philosophies of several current pipeline frameworks. I provide practical recommendations based on analysis requirements and the user base.}
}
@inproceedings{chirigati_2013,
title = {{VisTrails} Provenance Traces for Benchmarking},
author = {Chirigati, Fernando and Freire, Juliana and Koop, David and Silva, Cláudio},
series = {{EDBT} '13},
pages = {323-324},
publisher = {ACM},
year = {2013},
urldate = {2018-07-23},
address = {New York, {NY}, {USA}},
f1000-projects = {CWLProv},
booktitle = {Proceedings of the Joint {EDBT}/{ICDT} 2013 Workshops}
}
@inproceedings{missier_2013,
title = {The {W3C} {PROV} Family of Specifications for Modelling Provenance Metadata},
author = {Missier, Paolo and Belhajjame, Khalid and Cheney, James},
series = {{EDBT} '13},
pages = {773-776},
publisher = {ACM},
year = {2013},
urldate = {2018-07-23},
address = {New York, {NY}, {USA}},
f1000-projects = {CWLProv},
booktitle = {Proceedings of the 16th International Conference on Extending Database Technology}
}
@misc{,
title = {{Wf4Ever} Research Object Model},
urldate = {2018-07-23},
f1000-projects = {CWLProv},
type = {OTHER}
}
@article{a,
title = {Service Management:: Operations, Strategy and Information Technology, 2nd edition},
pages = {3-4},
year = {1999},
urldate = {2018-07-23},
journal = {International Journal of Service Industry Management},
volume = {10},
number = {2},
f1000-projects = {CWLProv}
}
@article{benabdelkader_2015,
title = {{PROV}-man: A {PROV}-compliant toolkit for provenance management},
author = {Benabdelkader, Ammar and {VanKampen}, Antoine Ahc and Olabarriaga, Silvia D},
publisher = {{PeerJ}, Inc.},
year = {2015},
volume = {3},
number = {e1102v1},
 doi={10.7287/peerj.preprints.1102v1},
journal = {{PeerJ} {PrePrints}},
f1000-projects = {CWLProv}
}
@inproceedings{pasquier_2017,
title = {Practical whole-system provenance capture},
author = {Pasquier, Thomas and Han, Xueyuan and Goldstein, Mark and Moyer, Thomas and Eyers, David and Seltzer, Margo and Bacon, Jean},
pages = {405-418},
publisher = {{ACM} Press},
year = {2017},
month = {sep},
day = {24},
urldate = {2018-07-23},
isbn = {9781450350280},
doi = {10.1145/3127479.3129249},
address = {New York, New York, {USA}},
f1000-projects = {CWLProv},
abstract = {Data provenance describes how data came to be in its present form. It includes data sources and the transformations that have been applied to them. Data provenance has many uses, from forensics and security to aiding the reproducibility of scientific experiments. We present {CamFlow}, a whole-system provenance capture mechanism that integrates easily into a {PaaS} offering. While there have been several prior whole-system provenance systems that captured a comprehensive, systemic and ubiquitous record of a system's behavior, none have been widely adopted. They either A) impose too much overhead, B) are designed for long-outdated kernel releases and are hard to port to current systems, C) generate too much data, or D) are designed for a single system. {CamFlow} addresses these shortcoming by: 1) leveraging the latest kernel design advances to achieve efficiency; 2) using a self-contained, easily maintainable implementation relying on a Linux Security Module, {NetFilter}, and other existing kernel facilities; 3) providing a mechanism to tailor the captured provenance data to the needs of the application; and 4) making it easy to integrate provenance across distributed systems. The provenance we capture is streamed and consumed by tenant-built auditor applications. We illustrate the usability of our implementation by describing three such applications: demonstrating compliance with data regulations; performing fault/intrusion detection; and implementing data loss prevention. We also show how {CamFlow} can be leveraged to capture meaningful provenance without modifying existing applications.},
booktitle = {Proceedings of the 2017 Symposium on Cloud Computing - {SoCC} '17}
}
@article{merkel_2014,
title = {Docker: Lightweight Linux Containers for Consistent Development and Deployment},
author = {Merkel, Dirk},
publisher = {Belltown Media},
year = {2014},
month = {mar},
urldate = {2018-07-23},
journal = {Linux J.},
volume = {2014},
number = {239},
address = {Houston, {TX}},
f1000-projects = {CWLProv},
abstract = {\textellipsis The client libraries are great for accessing the d\{\textbackslashae\}mon programmatically, but the more common use case is to issue \textellipsis Third, the Docker binary functions as a client to remote repositories of images. Tagged images that make up the filesystem for a container are called repositories \textellipsis}
}

@article{william_2017,
title = {Distributing Data and Analysis Software Containers For Better Data Sharing in Clinical Research},
author = {William, A and Furmanek, Stephen and Sinclair, Christopher M and Timothy, L},
pages = {6},
year = {2017},
urldate = {2018-07-23},
journal = {The University of Louisville Journal of Respiratory Infections},
volume = {1},
number = {4},
f1000-projects = {CWLProv}
}
@inproceedings{deroure_2011,
title = {Towards the preservation of scientific workflows},
author = {De Roure, David and Belhajjame, Khalid and Missier, Paolo and Gómez-Pérez, José Manuel and Palma, Raúl and Ruiz, José Enrique and Hettne, Kristina and Roos, Marco and Klyne, Graham and Goble, Carole},
year = {2011},
urldate = {2018-07-23},
f1000-projects = {CWLProv},
booktitle = {Procs. of the 8th International Conference on Preservation of Digital Objects ({iPRES} 2011). {ACM}}
}

@incollection{michaelides_2016,
booktitle = {Provenance and annotation of data and processes},
title = {Intermediate notation for provenance and workflow reproducibility},
author = {Michaelides, Danius T. and Parker, Richard and Charlton, Chris and Browne, William J. and Moreau, Luc},
editor = {Mattoso, Marta and Glavic, Boris},
series = {Lecture notes in computer science},
pages = {83-94},
publisher = {Springer International Publishing},
year = {2016},
urldate = {2018-07-23},
volume = {9672},
isbn = {978-3-319-40592-6},
issn = {0302-9743},
doi = {10.1007/978-3-319-40593-3\_7},
address = {Cham},
f1000-projects = {CWLProv},
abstract = {We present a technique to capture retrospective provenance across a number of tools in a statistical software suite. Our goal is to facilitate portability of processes between the tools to enhance usability and to support reproducibility. We describe an intermediate notation to aid runtime capture of provenance and demonstrate conversion to an executable and editable workflow. The notation is amenable to conversion to {PROV} via a template expansion mechanism. We discuss the impact on our system of recording this intermediate notation in terms of runtime performance and also the benefits it brings.}
}
@article{herschel_2017,
title = {A survey on provenance: What for? What form? What from?},
author = {Herschel, Melanie and Diestelkämper, Ralf and Ben Lahmar, Houssem},
pages = {881-906},
year = {2017},
month = {dec},
urldate = {2018-07-23},
journal = {{VLDB} J.},
volume = {26},
number = {6},
f1000-projects = {CWLProv},
abstract = {Provenance refers to any information describing the production process of an end product, which can be anything from a piece of digital data to a physical object. While this survey focuses on the former type of end product, this definition still leaves room for many different interpretations of and approaches to provenance. These are typically motivated by different application domains for provenance (e.g., accountability, reproducibility, process debugging) and varying technical requirements such as runtime, scalability, or privacy. As a result, we observe a wide variety of provenance types and provenance-generating methods. This survey provides an overview of the research field of provenance, focusing on what provenance is used for (what for?), what types of provenance have been defined and captured for the different applications (what form?), and which resources and system requirements impact the choice of deploying a particular provenance solution (what from?). For each of these three key questions, we provide a classification and review the state of the art for each class. We conclude with a summary and possible future research challenges.}
}
@article{bechhofer_2013,
title = {Why linked data is not enough for scientists},
author = {Bechhofer, Sean and Buchan, Iain and De Roure, David and Missier, Paolo and Ainsworth, John and Bhagat, Jiten and Couch, Philip and Cruickshank, Don and Delderfield, Mark and Dunlop, Ian and Gamble, Matthew and Michaelides, Danius and Owen, Stuart and Newman, David and Sufi, Shoaib and Goble, Carole},
pages = {599-611},
year = {2013},
urldate = {2018-07-23},
journal = {Future Gener. Comput. Syst.},
volume = {29},
number = {2},
f1000-projects = {CWLProv}
}

@book{services_2012,
    title = {Information Storage and Management: Storing, Managing, and Protecting Digital Information in Classic, Virtualized, and Cloud Environments, Second Edition},
    editor = {Somasundaram Gnanasundaram and Alok Shrivastava},
    author = {{EMC Education Services}},
    publisher = {John Wiley \& Sons},
    year = {2012},
    month = {apr},
    note = {ISBN 978-1-118-09483-9},
    isbn={9781118094839}
}


@Book{Anderberg83,
   author =   {Anderberg, M. R.},
   title =    {Cluster Analysis for Applications},
   publisher =    {New York: Academic Press},
   year =     {1983},
 }

  @Manual{R:2010,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Development Core Team}},
    organization = {Vienna, Austria: R Foundation for Statistical Computing},
    address = {},
    year = {2012},
    note = {{ISBN} 3-900051-07-0, http://www.R-project.org},
    url = {http://www.R-project.org},
  }

  @Manual{R:2008,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Development Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2008},
    note = {{ISBN} 3-900051-07-0},
    url = {http://www.R-project.org},
  }

  @Article{Arabie80,
   author =   {Arabie, P. and Carroll, J. D.},
   title =    {MAPCLUS: A mathematical programming approach to fitting the ADCLUS models},
   journal =      {Psychometrika},
   year =     {1980 },
   volume =   {445},
   pages =    {211-35},
 }


@book{Tufte:1990,
	Address = {Cheshire, Connecticut},
	Author = {E. R. Tufte},
	Publisher = {Graphics Press},
	Title = {{Envisioning Information}},
	Year = {1990}}


@book{Tufte:1983,
	Address = {Cheshire},
	Author = {E. R. Tufte},
	Publisher = {Graphics Press},
	Title = {{The Visual Display of Quantitative Information}},
	Year = {1983}}

@book{Cleveland:1994,
	Address = {Summit},
	Author = {W. S. Cleveland},
	Edition = {Revised},
	Publisher = {Hobart Press},
	Title = {{The Elements of Graphing Data}},
	Year = {1994}}

@book{Cleveland:1993,
	Address = {Summit},
	Author = {W. S. Cleveland},
	Publisher = {Hobart Press},
	Title = {{Vizualizing Data}},
	Year = {1993}}

 @TECHREPORT{Ball65 ,
   AUTHOR =       {Ball, G. H. and Hall, D. J. },
   TITLE =        {A novel method of data analysis and pattern classification },
   INSTITUTION =  {Stanford Research Institute, California },
   YEAR =         {1965 },
 }

 @Article{Banfield93,
   author =   {Banfield, J. D. and Raftery, A. E.},
   title =    {Model-Based Gaussian and Non-Gaussian Clustering },
   journal =      {Biometrics },
   year =     {1993 },
   volume =   {49},
   pages =    {803--21},
 }

 @Article{Beale69,
   author =   {Beale, E. M. L.},
   title =    {Euclidean cluster analysis },
   journal =      {Bulletin of the International Statistical Institute },
   year =     {1969 },
   volume =   {43},
   pages =    {92-94},
 }

 @Article{Bensmail,
   author =   {Bensmail, H. },
   title =    {Model-based Clustering with Noise: Bayesian inference and estimation },
 }

@Article{Bezdek74,
   author =   {Bezdek, J. C.},
   title =    {Numerical taxonomy with fuzzy sets},
   journal =      {Journal of Methematical Biology},
   year =     {1974},
   volume =   {1},
   pages =    {57-71},
 }

@article{Cox:1972,
	Author = {D. R. Cox},
	Journal = {J. R. Statist. Soc. {\rm B}},
	Pages = {187--220},
	Title = {{Regression models and life tables (with Discussion)}},
	Volume = {34},
	Year = {1972}}

@article{Hear:Holm:Step:quan:2006,
	Author = {Heard, Nicholas A. and Holmes, Christopher C. and Stephens, David A.},
	Journal = {J. Am. Statist. Assoc.},
	Keywords = {bayesian hierarchical clustering; gene expression profiles; Microarrays},
	Pages = {18--29},
	Title = {A Quantitative Study of Gene Regulation Involved in the Immune Response of {A}nopheline Mosquitoes: {A}n Application of {B}ayesian Hierarchical Clustering of Curves},
	Volume = {101},
	Year = {2006}}

@article{Fan:2004,
	Author = {Fan, J. and Peng, H.},
	Journal = {Ann. Statist.},
	Pages = {928--61},
	Title = {{Nonconcave penalized likelihood with a diverging number of parameters}},
	Volume = {32},
	Year = {2004}}
	
@article{ELMROTH2010245,
    title = "Three fundamental dimensions of scientific workflow interoperability: Model of computation, language, and execution environment",
    journal = "Future Generation Computer Systems",
    volume = "26",
    number = "2",
    pages = "245 - 256",
    year = "2010",
    issn = "0167-739X",
    doi = "https://doi.org/10.1016/j.future.2009.08.011",
    url = "http://www.sciencedirect.com/science/article/pii/S0167739X09001174",
    author = "Erik Elmroth and Francisco Hernández and Johan Tordsson",
    keywords = "Scientific workflows, Workflow interoperability, Workflow languages, Model of computation, Grid interoperability",
    abstract = "We investigate interoperability aspects of scientific workflow systems and argue that the workflow execution environment, the model of computation (MoC), and the workflow language form three dimensions that must be considered depending on the type of interoperability sought: at the activity, sub-workflow, or workflow levels. With a focus on the problems that affect interoperability, we illustrate how these issues are tackled by current scientific workflows as well as how similar problems have been addressed in related areas. Our long-term objective is to achieve (logical) interoperability between workflow systems operating under different MoCs, using distinct language features, and sharing activities running on different execution environments."
}	
@inproceedings{Mohan2014,
  doi = {10.1109/scc.2014.53},
  year  = {2014},
  month = {jun},
  publisher = {{IEEE}},
  author = {Aravind Mohan and Shiyong Lu and Alexander Kotov},
  title = {Addressing the Shimming Problem in Big Data Scientific Workflows},
  booktitle = {2014 {IEEE} International Conference on Services Computing}
}
@article{caoprovone,
  title={ProvONE: Extending PROV to support the DataONE scientific community},
  author={Cao, Yang and Jones, Christopher and Cuevas-Vicentt{\i}n, V{\i}ctor and Jones, Matthew B and Lud{\"a}scher, Bertram and McPhillips, Timothy and Missier, Paolo and Schwalm, Christopher and Slaughter, Peter and Vieglais, Dave and Walker, Lauren
and Wei, Yaxing},
  year = {2016},
  month = {jun},
  day = {6},
  booktitle = {PROV Three Years Later; Workshop at Provenance Week 2016},
  url = {http://homepages.cs.ncl.ac.uk/paolo.missier/doc/dataone-prov-3-years-later.pdf},
  urldate = {2018-11-29}
}

@article{datacrate_2018,
  author = {Peter Sefton and Michael Lynch and Gerard Devine and Duncan Loxton},
 title = {DataCrate: a method of packaging, distributing, displaying and archiving Research Objects},
  month        = jul,
  year         = 2018,
  note         = {{Workshop on Research Objects (RO2018) at IEEE eScience 2018}},
  doi          = {10.5281/zenodo.1312323},
}

@misc{cwltool,
  author       = {Peter Amstutz and
                  Michael R. Crusoe and
                  Farah Zaib Khan and
                  Stian Soiland-Reyes and
                  Manvendra Singh and
                  Kapil kumar and
                  Anton Khodak and
                  Pau Ruiz Safont and
                  John Chilton and
                  Thomas Hickman and
                  boysha and
                  Josh Holland and
                  Brad Chapman and
                  Michael Kotliar and
                  Dan Leehr and
                  Guillermo Carrasco and
                  Andrey Kartashov and
                  Joshua C. Randall and
                  Nebojsa Tijanic and
                  Hervé Ménager and
                  Tomoya Tanjo and
                  James J Porter and
                  Gijs Molenaar and
                  Michael Tong and
                  Giacomo Tagliabue and
                  Denis Yuen and
                  Alejandro Barrera and
                  Sinisa Ivkovic and
                  Ryan Spangler and
                  psaffrey-illumina},
  title        = {{common-workflow-language/cwltool: 
                   1.0.20181012180214}},
  month        = oct,
  year         = 2018,
  doi          = {10.5281/zenodo.1471589},
  publisher    = {Zenodo},
}


@article{Conesa2016,
  doi = {10.1186/s13059-016-0881-8},
  year  = {2016},
  month = {jan},
  publisher = {Springer Nature},
  volume = {17},
  number = {1},
  author = {Ana Conesa and Pedro Madrigal and Sonia Tarazona and David Gomez-Cabrero and Alejandra Cervera and Andrew McPherson and Micha{\l} Wojciech Szcze{\'{s}}niak and Daniel J. Gaffney and Laura L. Elo and Xuegong Zhang and Ali Mortazavi},
  title = {A survey of best practices for {RNA}-seq data analysis},
  journal = {Genome Biology}
}

@article{seo2012transcriptional,
  title={The transcriptional landscape and mutational profile of lung adenocarcinoma},
  author={Seo, Jeong-Sun and Ju, Young Seok and Lee, Won-Chul and Shin, Jong-Yeon and Lee, June Koo and Bleazard, Thomas and Lee, Junho and Jung, Yoo Jin and Kim, Jung-Oh and Shin, Jung-Young and others},
  journal={Genome Research},
  year={2012},
  publisher={Cold Spring Harbor Lab},
  doi={10.1101/gr.145144.112}
}

@article{robinson_2017b,
title = {{CWL} Viewer: the common workflow language viewer},
author = {Robinson, Mark and Soiland-Reyes, Stian and Crusoe, Michael R and Goble, Carole},
pages = {1075 (poster)},
url = {https://doi.org/10.7490/f1000research.1114375.1},
year = {2017},
month = {jul},
day = {6},
urldate = {2018-07-13},
journal = {F1000Research},
volume = {6},
number = {{ISCB} Comm J},
f1000-projects = {CWLProv}
}

@article{xu2018review,
  title={A review of somatic single nucleotide variant calling algorithms for next-generation sequencing data},
  author={Xu, Chang},
  journal={Computational and structural biotechnology journal},
  year={2018},
  publisher={Elsevier},
  doi = {10.1016/j.csbj.2018.01.003}
}

@article{vivian2017toil,
  title={Toil enables reproducible, open source, big biomedical data analyses},
  author={Vivian, John and Rao, Arjun Arkal and Nothaft, Frank Austin and Ketchum, Christopher and Armstrong, Joel and Novak, Adam and Pfeil, Jacob and Narkizian, Jake and Deran, Alden D and Musselman-Brown, Audrey and others},
  journal={Nature biotechnology},
  volume={35},
  number={4},
  pages={314},
  year={2017},
  doi={10.1038/nbt.3772},
  publisher={Nature Publishing Group}
}

@article {cwlairflow2018,
	author = {Kotliar, Michael and Kartashov, Andrey and Barski, Artem},
	title = {CWL-Airflow: a lightweight pipeline manager supporting Common Workflow Language},
	year = {2018},
	doi = {10.1101/249243},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Abstract: Massive growth in the amount of research data and computational analysis has led to increased utilization of pipeline managers in biomedical computational research. However, each of more than 100 such managers uses its own way to describe pipelines, leading to difficulty porting workflows to different environments and therefore poor reproducibility of computational studies. For this reason, the Common Workflow Language (CWL) was recently introduced as a specification for platform-independent workflow description, and work began to transition existing pipelines and workflow managers to CWL. Here, we present CWL-Airflow, an extension for the Apache Airflow pipeline manager supporting CWL. CWL-Airflow utilizes CWL v1.0 specification and can be used to run workflows on standalone MacOS/Linux servers, on clusters, or on variety cloud platforms. A sample CWL pipeline for processing of ChIP-Seq data is provided. CWL-Airflow is available under Apache license v.2 and can be downloaded from https://github.com/Barski-lab/cwl-airflow.},
	journal = {bioRxiv}
}

@inproceedings{cristofori2013usage,
  title={Usage Record--Format Recommendation},
  author={Cristofori, A and Bologna, IGI and Gordon, J and London, STFC RAL and Kennedy, JA and Munich, RZG and M{\"u}ller-Pfefferkorn, R},
  booktitle={Open Grid Forum},
  year={2013},
  note = {GFD-R-P.204},
  url = {https://www.ogf.org/documents/GFD.204.pdf}
  
}

@inproceedings{alper2018labelflow,
  title={LabelFlow Framework for Annotating Workflow Provenance},
  author={Alper, Pinar and Belhajjame, Khalid and Curcin, Vasa and Goble, Carole A},
  booktitle={Informatics},
  volume={5},
  number={1},
  pages={11},
  year={2018},
  doi={10.3390/informatics5010011},
  organization={Multidisciplinary Digital Publishing Institute} }

@inproceedings{kanwal2017digital,
  title={Digital reproducibility requirements of computational genomic workflows},
  author={Kanwal, Sehrish and Lonie, Andrew and Sinnott, Richard O},
  booktitle={Bioinformatics and Biomedicine (BIBM), 2017 IEEE International Conference on},
  pages={1522--1529},
  year={2017},
  organization={IEEE},
  doi={10.1109/bibm.2017.8217887}
}

@inproceedings{chen2011partitioning,
  title={Partitioning and scheduling workflows across multiple sites with storage constraints},
  author={Chen, Weiwei and Deelman, Ewa},
  booktitle={International Conference on Parallel Processing and Applied Mathematics},
  pages={11--20},
  year={2011},
  organization={Springer},
  doi={10.1007/978-3-642-31500-8_2}
}

@article{Dobin2012,
  doi = {10.1093/bioinformatics/bts635},
  url = {https://doi.org/10.1093/bioinformatics/bts635},
  year  = {2012},
  month = {oct},
  publisher = {Oxford University Press ({OUP})},
  volume = {29},
  number = {1},
  pages = {15--21},
  author = {Alexander Dobin and Carrie A. Davis and Felix Schlesinger and Jorg Drenkow and Chris Zaleski and Sonali Jha and Philippe Batut and Mark Chaisson and Thomas R. Gingeras},
  title = {{STAR}: ultrafast universal {RNA}-seq aligner},
  journal = {Bioinformatics}
}

@article{li2009sequence,
  title={The sequence alignment/map format and SAMtools},
  author={Li, Heng and Handsaker, Bob and Wysoker, Alec and Fennell, Tim and Ruan, Jue and Homer, Nils and Marth, Gabor and Abecasis, Goncalo and Durbin, Richard},
  journal={Bioinformatics},
  volume={25},
  number={16},
  pages={2078--2079},
  year={2009},
  publisher={Oxford University Press},
  doi={10.1093/bioinformatics/btp352},
}

@article{dobin2015mapping,
  title={Mapping RNA-seq reads with STAR},
  author={Dobin, Alexander and Gingeras, Thomas R},
  journal={Current Protocols in Bioinformatics},
  volume={51},
  number={1},
  pages={11--14},
  year={2015},
  publisher={Wiley Online Library},
  doi={10.1002/0471250953.bi1114s51}
}

@article{DeLuca2012,
  doi = {10.1093/bioinformatics/bts196},
  year  = {2012},
  month = {apr},
  publisher = {Oxford University Press ({OUP})},
  volume = {28},
  number = {11},
  pages = {1530--1532},
  author = {David S. DeLuca and Joshua Z. Levin and Andrey Sivachenko and Timothy Fennell and Marc-Danie Nazaire and Chris Williams and Michael Reich and Wendy Winckler and Gad Getz},
  title = {{RNA}-{SeQC}: {RNA}-seq metrics for quality control and process optimization},
  journal = {Bioinformatics}
}

@article{li2011rsem,
  title={RSEM: accurate transcript quantification from RNA-Seq data with or without a reference genome},
  author={Li, Bo and Dewey, Colin N},
  journal={BMC Bioinformatics},
  volume={12},
  number={1},
  pages={323},
  year={2011},
  publisher={BioMed Central},
  doi={10.1186/1471-2105-12-323}
}

@inproceedings{arcp_ro2018,
  title        = {The Archive and Package (arcp) URI scheme},
  author       = {Soiland-Reyes, Stian and Cáceres, Marcos},  
  booktitle    = {2018 IEEE 13th International Conference on e-Science (e-Science)},
  year         = {2018},
  month        = jul,
  note         = {In Print, arXiv:1809.06935},
  url          = {http://s11.no/2018/arcp.html}
}


@article{li2013aligning,
  title={Aligning sequence reads, clone sequences and assembly contigs with BWA-MEM},
  author={Li, Heng},
  journal={arXiv preprint arXiv:1303.3997},
  year={2013},
  url={https://arxiv.org/abs/1303.3997}
}

@article{Saunders2012,
  doi = {10.1093/bioinformatics/bts271},
  year  = {2012},
  month = {may},
  publisher = {Oxford University Press ({OUP})},
  volume = {28},
  number = {14},
  pages = {1811--1817},
  author = {Christopher T. Saunders and Wendy S. W. Wong and Sajani Swamy and Jennifer Becq and Lisa J. Murray and R. Keira Cheetham},
  title = {Strelka: accurate somatic small-variant calling from sequenced tumor{\textendash}normal sample pairs},
  journal = {Bioinformatics}
}


	
@online{cwl-existing-workflow-systems,
  publisher = {{\relax Common Workflow Language project}},
  author = {{\relax Common Workflow Language project}},
  title = {Existing Workflow Systems},
  editor = {CWL Community},
  year = 2018,
  url = {https://s.apache.org/existing-workflow-systems},
  urldate = {2018-09-18},
  note = {Accessed 12 Sep 2018}
}	

@online{bagit17,
author = {{\relax Network Working Group}},
title = {draft-kunze-bagit-17 - The BagIt File Packaging Format (V1.0)},
url = {https://tools.ietf.org/html/draft-kunze-bagit-17},
month = {},
year = {2017},
urldate = {2018-09-22},
note = {Accessed 22 Sep 2018}
}

@inproceedings{Missier2013,
 author = {Missier, Paolo and Belhajjame, Khalid and Cheney, James},
 title = {The W3C PROV Family of Specifications for Modelling Provenance Metadata},
 booktitle = {Proceedings of the 16th International Conference on Extending Database Technology},
 series = {EDBT '13},
 year = {2013},
 isbn = {978-1-4503-1597-5},
 location = {Genoa, Italy},
 pages = {773--776},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/2452376.2452478},
 doi = {10.1145/2452376.2452478},
 acmid = {2452478},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@online{PROVDM,
author = {
    Luc Moreau and
    Paolo Missier and
    Khalid Belhajjame and
    Reza B'Far and
    James Cheney and
    Sam Coppens and
    Stephen Cresswell and
    Yolanda Gil and
    Paul Groth and
    Graham Klyne and
    Timothy Lebo and
    Jim McCusker and
    Simon Miles and
    James Myers and
    Satya Sahoo and
    Curt Tilmes
},
  title       = "{PROV}-DM: The {PROV} Data Model",
  month       = apr,
  note        = "http://www.w3.org/TR/2013/REC-prov-dm-20130430/",
  year        = "2013",
  bibsource   = "https://w2.syronex.com/jmr/w3c-biblio",
  type        = "{W3C} Recommendation",
  institution = "W3C",
}

@online{PROVXML,
author = {
    Hook Hua and
    Curt Tilmes and
    Stephan Zednik and
    Luc Moreau
},
title = {PROV-XML: The PROV XML Schema},
note = {W3C Working Group Note 30 April 2013},
url = {http://www.w3.org/TR/2013/NOTE-prov-xml-20130430/},
day = {30},
month = {apr},
year = {2013},
}

@online{PROVO,
author = {
    Timothy Lebo and
    Satya Sahoo and
    Deborah McGuinness and
    Khalid Belhajjame and
    James Cheney and
    David Corsar and
    Daniel Garijo and
    Stian Soiland-Reyes and
    Stephan Zednik and
    Jun Zhao
},
title = {PROV-O: The PROV Ontology},
note = {W3C Recommendation Note 30 April 2013},
url = {http://www.w3.org/TR/2013/REC-prov-o-20130430/},
day = {30},
month = {apr},
year = {2013},
}


@online{interope55:online,
author = {},
title = {interoperability | Definition of interoperability in English by Oxford Dictionaries},
url = {https://en.oxforddictionaries.com/definition/interoperability},
month = {},
year = {},
urldate = {2018-09-22},
note = {Accessed 22 Sep 2018}
}

@online{portabil95:online,
author = {},
title = {portability | Definition of portability in English by Oxford Dictionaries},
url = {https://en.oxforddictionaries.com/definition/portability},
month = {},
year = {},
urldate = {2018-09-22},
note = {Accessed 22 Sep 2018}
}

@online{DockerHu74:online,
author = {},
title = {Docker Hub},
url = {https://hub.docker.com/},
month = {},
year = {},
urldate = {2018-09-22},
note = {Accessed 22 Sep 2018}
}

@online{Conda,
author = {},
title = {Conda \textendash Conda documentation},
url = {https://conda.io/docs/},
month = {},
year = {},
urldate = {2018-09-22},
note = {Accessed 22 Sep 2018}
}
@online{Zenodo,
author = {},
title = {Zenodo - Research. Shared.},
url = {https://zenodo.org/},
month = {},
year = {},
urldate = {2018-09-22},
note = {Accessed 22 Sep 2018}
}

@online{GitHub,
author = {},
title = {GitHub},
url = {https://github.com/},
month = {},
year = {},
urldate = {2018-09-22},
note = {Accessed 22 Sep 2018}
}

@online{figshare,
author = {},
title = {figshare - credit for all your research},
url = {https://figshare.com/},
month = {},
year = {},
urldate = {2018-09-22},
note = {Accessed 22 Sep 2018}
}

@online{CodeasaR,
author = {},
title = {Code as a Research Object},
url = {http://mozillascience.github.io/code-research-object/},
month = {},
year = {},
urldate = {2018-09-22},
note = {Accessed 22 Sep 2018}
}

@online{W3CProvWorkingGroup,
author = {},
title = {W3C Provenance Incubator Group Wiki - XG Provenance Wiki},
url = {https://www.w3.org/2005/Incubator/prov/wiki/W3C\_Provenance\_Incubator\_Group\_Wiki},
month = {},
year = {},
urldate = {2018-09-22},
note = {Accessed 22 Sep 2018}
}

@online{GATKBP,
author = {},
title = {GATK | BP Doc \#11165 | Data pre-processing for variant discovery},
url = {https://software.broadinstitute.org/gatk/best-practices/workflow?id=11165},
year = {2018},
urldate = {2018-09-22},
note = {Accessed 22 Sep 2018}
}

@misc{bioschemas,
  doi = {10.7490/f1000research.1114493.1},
  author = {Goble,  Carole and Jimenez,  Rafael and Gray,  Alasdair and Beard,  Niall and Profiti,  Giuseppe and Morrison,  Norman and {Bioschemas Consortium}},
  note = {Poster},
  title = {Bioschemas.org},
  publisher = {F1000Research},
  year = {2017}
}

@article{Michel_2018, title={Bioschemas \& Schema.org: a Lightweight Semantic Layer for Life Sciences Websites}, volume={2}, ISSN={2535-0897}, DOI={10.3897/biss.2.25836}, journal={Biodiversity Information Science and Standards}, publisher={Pensoft Publishers}, author={Michel, Franck and The Bioschemas Community}, year={2018}, month={May}, pages={e25836}}


@online{reusevocab,
author = {
    Bernadette Farias Lóscio and 
    Caroline Burle and
    Newton Calegari and
    Annette Greiner
    Antoine Isaac and
    Carlos Iglesias and
    Carlos Laufer and 
    Christophe Guéret and 
    Deirdre Lee and
    Doug Schepers and
    Eric G. Stephan and
    Eric Kauz and
    Ghislain A. Atemezing and
    Hadley Beeman and
    Ig Ibert Bittencourt and
    João Paulo Almeida and
    Makx Dekkers and 
    Peter Winstanley and
    Phil Archer and 
    Riccardo Albertoni and 
    Sumit Purohit and 
    Yasodara Córdova},
title = {Data on the Web Best Practices},
url = {https://www.w3.org/TR/2017/REC-dwbp-20170131/#ReuseVocabularies},
day = {31},
month = {jan},
year = {2017},
urldate = {2018-09-22},
note = {W3C Recommendation 31 January 2017}
}

@online{roprinciples,
author = {},
title = {researchobject.org},
url = {http://www.researchobject.org/overview/},
month = {},
year = {},
urldate = {2018-09-22},
note = {Accessed 22 Sep 2018}
}

@online{PROVModel,
author = {
    Yolanda Gil and
    Simon Miles and
    Khalid Belhajjame and
    Helena Deus and
    Daniel Garijo and
    Graham Klyne and
    Paolo Missier and
    Stian Soiland-Reyes and
    Stephan Zednik
},
title = {PROV Model Primer},
url = {http://www.w3.org/TR/2013/NOTE-prov-primer-20130430/},
year = {2013},
month = {apr},
day = {30},
note = {W3C Working Group Note 30 April 2013},
}

@online{wf4ever1,
author = {
    Stian Soiland-Reyes and
    Sean Bechhofer and
    Oscar Corcho and
    Graham Klyne and
    Khalid Belhajjame and
    Daniel Garijo and
    Esteban García Cuesta and
    Raul Palma
},
title = {The Wfdesc Ontology},
url = {https://w3id.org/ro/2016-01-28/wfdesc},
year = {2016},
month = {Jan},
day = {28},
urldate = {2018-09-22},
note = {Accessed 22 Sep 2018}
}

@online{wf4ever2,
author = {
    Stian Soiland-Reyes and
    Sean Bechhofer and
    Oscar Corcho and
    Graham Klyne and
    Khalid Belhajjame and
    Daniel Garijo and
    Esteban García Cuesta and
    Raul Palma
},
title = {The Wfprov Ontology},
url = {https://w3id.org/ro/2016-01-28/wfprov/},
year = {2016},
urldate = {2018-09-22},
note = {Accessed 22 Sep 2018}
}

@online{JSONLD,
author = {
    Manu Sporny and
    Dave Longley and
    Gregg Kellogg and
    Markus Lanthaler and
    Niklas Lindström 
},
title = {JSON-LD 1.0: A JSON-based Serialization for Linked Data},
url = {http://www.w3.org/TR/2014/REC-json-ld-20140116/},
year = {2014},
month = {jan},
day = {16},
urldate = {2018-09-22},
note = {W3C Recommendation 16 January 2014}
}

@misc{cwlprov,
  author       = {Stian Soiland-Reyes and
                  Farah Zaib Khan and
                  Michael R Crusoe},
  title        = {common-workflow-language/cwlprov: CWLProv 0.6.0},
  month        = oct,
  year         = 2018,
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.1471585},
}


@online{provpython,
title = {prov 1.5.2},
url = {https://pypi.org/project/prov/},
year = {2018},
urldate = {2018-09-23},
note = {Accessed 22 Sep 2018}
}

@online{cwltool-controlflow,
title = {common-workflow-language/cwltool:Common Workflow Language reference implementation},
url = {https://github.com/common-workflow-language/cwltool#cwl-tool-control-flow},
year = {2016},
urldate = {2018-09-23}, 
note = {Accessed 23 Sep 2018}
}

@online{rnaseq,
title = {TopMed RNA-seq workflow},
url = {https://w3id.org/cwl/view/git/027e8af41b906173aafdb791351fb29efc044120/topmed-workflows/TOPMed_RNAseq_pipeline/rnaseq_pipeline_fastq.cwl},
year = {2018},
urldate = {2018-09-23},
note = {Accessed 23 Sep 2018}
}

@online{heliumda,
title = {heliumdatacommons},
url = {https://github.com/heliumdatacommons},
year = {2017},
urldate = {2018-09-23},
note = {Accessed 23 Sep 2018}
}

@online{gtexpipe,
author = {},
title = {Gtex RNA-seq pipeline},
url = {https://github.com/broadinstitute/gtex-pipeline/tree/master/rnaseq},
year = {2017},
urldate = {2018-09-23},
note = {Accessed 23 Sep 2018}
}

@online{MarkDuplicates,
author = {},
title = {Tool documentation: MarkDuplicates},
url = {http://broadinstitute.github.io/picard/command-line-overview.html#MarkDuplicates},
year = {},
urldate = {2018-09-23}, 
note = {Accessed 23 Sep 2018}
}

@online{alignment-wf,
author = {},
title = {topmed-workflows/topmed-alignment.cwl at cwlprov\_testing · FarahZKhan/topmed-workflows},
url = {https://github.com/FarahZKhan/topmed-workflows/blob/2fcea0b9469b572399755b6828ff87a40d865e43/aligner/sbg-alignment-cwl/topmed-alignment.cwl},
month = {},
year = {},
urldate = {2018-09-23},
note = {Accessed 23 Sep 2018}
}

@online{docker-alignment,
author = {},
title = {statgen/docker-alignment: Dockerfile for Alignment},
url = {https://github.com/statgen/docker-alignment},
year = {2017},
urldate = {2018-09-23},
note = {Accessed 23 Sep 2018}
}

@online{Abecasis14,
title = {Abecasis Lab - Genome Analysis Wiki},
url = {https://genome.sph.umich.edu/wiki/Abecasis_Lab},
year = {2017},
urldate = {2018-09-23},
note = {Accessed 23 Sep 2018}
}

@online{DataBios21,
title = {Data Biosphere},
url = {https://github.com/DataBioSphere},
year = {2018},
urldate = {2018-09-23},
note = {Accessed 23 Sep 2018}
}

@online{DataComm86,
author = {},
title = {Data Commons | NIH Common Fund},
url = {https://commonfund.nih.gov/commons},
month = {},
year = {2018},
urldate = {2018-09-23},
note = {Accessed 23 Sep 2018}
}

@online{TransOmi24,
title = {Trans-Omics for Precision Medicine (TOPMed) Program | National Heart, Lung, and Blood Institute (NHLBI)},
url = {https://www.nhlbi.nih.gov/science/trans-omics-precision-medicine-topmed-program},
year = {2014},
urldate = {2018-09-23},
note = {Accessed 23 Sep 2018}
}

@online{variantcallinglecture,
title = {Introduction to Variant Calling},
url = {https://bioconductor.org/help/course-materials/2014/CSAMA2014/3_Wednesday/lectures/VariantCallingLecture.pdf},
year = {2014},
urldate = {2018-09-23},
note = {Accessed 23 Sep 2018}
}

@online{bcbio,
title = {Blue Collar Bioinformatics},
url = {http://bcb.io/},
year = {},
urldate = {2018-09-23},
note = {Accessed 23 Sep 2018}
}

@online{bcbiowf,
title = {Somatic Variant Calling Workflow},
url = {https://github.com/FarahZKhan/bcbio_test_cwlprov/blob/master/somatic/somatic-workflow/main-somatic.cwl},
year = {2018},
urldate = {2018-09-23},
note = {Accessed 23 Sep 2018}
}

@online{bcbiocwl,
title = {Common Workflow Language (CWL) \textendash bcbio-nextgen 1.1.0 documentation},
url = {https://bcbio-nextgen.readthedocs.io/en/latest/contents/cwl.html#current-status},
year = {2017},
urldate = {2018-09-23},
note = {Accessed 23 Sep 2018}
}

@online{Nectar,
title = {Nectar Cloud - Nectar},
url = {https://nectar.org.au/research-cloud/},
year = {},
urldate = {2018-09-23},
note = {Accessed 23 Sep 2018}
}

@article{Carata2014,
  doi = {10.1145/2596628},
  url = {https://doi.org/10.1145/2596628},
  year  = {2014},
  month = {may},
  publisher = {Association for Computing Machinery ({ACM})},
  volume = {57},
  number = {5},
  pages = {52--60},
  author = {Lucian Carata and Sherif Akoush and Nikilesh Balakrishnan and Thomas Bytheway and Ripduman Sohan and Margo Selter and Andy Hopper},
  title = {A primer on provenance},
  journal = {Communications of the {ACM}}
}

@misc{cwlprov-py,
        author = {Stian Soiland-Reyes and Farah Zaib Khan},
        title  = {{common-workflow-language/cwlprov-py: cwlprov-py
                0.1.1}},
        month  = oct,
        year   = 2018,
        publisher    = {Zenodo},
        doi    = {10.5281/zenodo.1471376},
        url    = {https://doi.org/10.5281/zenodo.1471376}
}

@online{OBFTravel,
title = {OBF Travel Fellowship Program | OBF News},
url = {https://news.open-bio.org/2016/03/01/obf-travel-fellowship-program/},
year = {2016},
urldate = {2018-09-26},
note = {Accessed 26 Sep 2018}
}

@article{Goble2010,
  doi = {10.1093/nar/gkq429},
  url = {https://doi.org/10.1093/nar/gkq429},
  year  = {2010},
  month = {may},
  publisher = {Oxford University Press ({OUP})},
  volume = {38},
  number = {suppl{\_}2},
  pages = {W677--W682},
  author = {Carole A. Goble and Jiten Bhagat and Sergejs Aleksejevs and Don Cruickshank and Danius Michaelides and David Newman and Mark Borkum and Sean Bechhofer and Marco Roos and Peter Li and David De Roure},
  title = {{myExperiment}: a repository and social network for the sharing of bioinformatics workflows},
  journal = {Nucleic Acids Research}
}

@article{kim2016assessing,
  title={Assessing Run-time Overhead of Securing Kepler},
  author={Kim, Donghoon and Vouk, Mladen A},
  journal={Procedia Computer Science},
  volume={80},
  pages={2281--2286},
  year={2016},
  publisher={Elsevier},
  DOI={10.1016/j.procs.2016.05.412},  
}


@incollection{Missier2016,
  doi = {10.1007/978-3-319-40226-0_8},
  url = {https://doi.org/10.1007/978-3-319-40226-0_8},
  year  = {2016},
  publisher = {Springer International Publishing},
  pages = {127--137},
  author = {Paolo Missier},
  title = {The Lifecycle of Provenance Metadata and Its Associated Challenges and Opportunities},
  booktitle = {Building Trust in Information}
}

@article{Goble2014,
  doi = {10.1109/mic.2014.88},
  url = {https://doi.org/10.1109/mic.2014.88},
  year  = {2014},
  month = {sep},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {18},
  number = {5},
  pages = {4--8},
  author = {Carole Goble},
  title = {Better Software,  Better Research},
  journal = {{IEEE} Internet Computing}
}

@online{CodeIsSc,
    title = {Code Is Science},
    url = {http://www.codeisscience.com/},
    urldate = {2018-10-04},
    lastvisited = {2018-10-04},
    note = {Accessed 03 Oct 2018}
}

@online{softwarecarpentry,
    title = {Software Carpentry},
    url = {https://software-carpentry.org/},
    urldate = {2018-10-03},
    lastvisited = {2018-10-03},
    note = {Accessed 03 Oct 2018}  
}


@online{arvados,
    title = {Arvados - Open Source Big Data Processing and Bioinformatics},
    url = {https://arvados.org/},
    urldate = {2018-11-08},
    note = {Accessed 08 Nov 2018}  
}


@article{Cochrane2012,
  doi = {10.1093/nar/gks1175},
  url = {https://doi.org/10.1093/nar/gks1175},
  year  = {2012},
  month = {nov},
  publisher = {Oxford University Press ({OUP})},
  volume = {41},
  number = {D1},
  pages = {D30--D35},
  author = {Guy Cochrane and Blaise Alako and Clara Amid and Lawrence Bower and Ana Cerde{\~{n}}o-T{\'{a}}rraga and Iain Cleland and Richard Gibson and Neil Goodgame and Mikyung Jang and Simon Kay and Rasko Leinonen and Xiu Lin and Rodrigo Lopez and Hamish McWilliam and Arnaud Oisel and Nima Pakseresht and Swapna Pallreddy and Youngmi Park and Sheila Plaister and Rajesh Radhakrishnan and Stephane Rivi{\`{e}}re and Marc Rossello and Alexander Senf and Nicole Silvester and Dmitriy Smirnov and Petra ten Hoopen and Ana Toribio and Daniel Vaughan and Vadim Zalunin},
  title = {Facing growth in the European Nucleotide Archive},
  journal = {Nucleic Acids Research}
}

@online{NIH-PILOT,
    title = {heliumdatacommons/cwl\_workflows: Example CWL Workflows that run on team Helium PIVOT architecture.},
    url = {https://github.com/heliumdatacommons/cwl_workflows},
    note = {Accessed 03 Oct 2018},
    urldate = {2018-10-11}
}

@article{RObundle,
      doi = {10.5281/zenodo.12586},
      author = {Soiland-Reyes,  Stian and Gamble,  Matthew and Haines,  Robert},
      keywords = {research object,  bundle,  zip,  linked data,  json-ld},
      title = {Research Object Bundle 1.0},
      publisher = {Zenodo},
      year = {2014}
}

@article{howe,
  doi = {10.1109/mcse.2012.62},
  url = {https://doi.org/10.1109/mcse.2012.62},
  year  = {2012},
  month = {jul},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume = {14},
  number = {4},
  pages = {36--41},
  author = {Bill Howe},
  title = {Virtual Appliances,  Cloud Computing,  and Reproducible Research},
  journal = {Computing in Science {\&} Engineering}
}

@article {tazro2018,
	author = {Ohta, Tazro and Tanjo, Tomoya and Ogasawara, Osamu},
	title = {Accumulating computational resource usage of genomic data analysis workflow to optimize cloud computing instance selection},
	year = {2018},
	doi = {10.1101/456756},
	abstract = {Background: Container virtualization technologies such as Docker became popular in the bioinformatics domain as they improve portability and reproducibility of software deployment. Along with software packaged in containers, the workflow description standards Common Workflow Language also enabled to perform data analysis on multiple different computing environments with ease. These technologies accelerate the use of on-demand cloud computing platform which can scale out according to the amount of data. However, to optimize the time and the budget on a use of cloud, users need to select a suitable instance type corresponding to the resource requirements of their workflows. Results: We developed CWL-metrics, a system to collect runtime metrics of Docker containers and workflow metadata to analyze resource requirement of workflows. We demonstrated the analysis by using seven transcriptome quantification workflows on six instance types. The result showed instance type options of lower financial cost and faster execution time with required amount of computational resources. Conclusions: The summary of resource requirements of workflow executions provided by CWL-metrics can help users to optimize the selection of cloud computing instance. The runtime metrics data also accelerate to share workflows among different workflow management frameworks.},
	comment = {Submitted to GigaScience},
	journal = {bioRxiv}
}

@online {biosimspacewebinar,
  title = {BioExcel Webinar \#28: BioSimSpace – filling the gaps between molecular simulation codes},
  author = {Christopher Woods},
  publisher = {YouTube, BioExcel},
  year = {2018},
  month = {Jun},
  abstract = { 
  BioSimSpace (https://biosimspace.org) is a flagship software project from the CCP-BioSim (https://ccpbiosim.ac.uk) and HEC-BioSim (https://hecbiosim.ac.uk) biomolecular modelling communities. The project aims to make it easier for researchers to develop, share and re-use biomolecular simulation workflow nodes. The software problem in our community is that we have lots of different pieces of software that are all incompatible and are not interoperable. This forces the community to hand-write small scripts for converting between different file formats, or to generate different software input files. The resulting scripts are specific for the software and task performed, leading to the community generating lots of bespoke and brittle workflows. BioSimSpace solves this problem by providing an interoperability layer between the major molecular simulation packages. In effect, BioSimSpace provides the shims that fill in the gaps between codes. BioSimSpace python scripts can be run from the command line, used as workflow nodes in packages such as Knime, or run on the cloud as Jupyter notebooks. In this talk, we will show how BioSimSpace has been implemented, will talk about its capabilities and our future plans, and will demonstrate some BioSimSpace workflow nodes running on the cloud.
  },
  url = {https://youtu.be/pD8mhj3WElE?t=1599},
  urldate = {2018-11-29}
}


@online{biosimspace,
  title = {BioSimSpace},
  note = {Accessed 29 Nov 2018},
  year = {2018},
  url = {https://biosimspace.org/},
  urldate = {2018-11-29}
}

 @article{Angiuoli_2011, 
   author={Angiuoli, Samuel V. and White, James R. and Matalka, Malcolm and White, Owen and Fricke, W. Florian}, editor={Highlander, Sarah K.Editor}, 
   title={Resources and Costs for Microbial Sequence Analysis Evaluated Using Virtual Machines and Cloud Computing}, 
   ISSN={1932-6203}, 
   DOI={10.1371/journal.pone.0026624}, 
   journal={PLoS ONE}, 
   publisher={Public Library of Science (PLoS)},
   volume={6}, 
   number={10}, 
   pages={e26624},
   year={2011}, 
   month={Oct}
}


 @article{Bubak_2013, title={Evaluation of Cloud Providers for VPH Applications}, 
   DOI={10.1109/ccgrid.2013.54}, journal={2013 13th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing}, publisher={IEEE}, author={Bubak, M. and Kasztelnik, M. and Malawski, M. and Meizner, J. and Nowakowski, P. and Varma, S.}, year={2013}, month={May}}

 @article{Malawski_2013, title={Cost minimization for computational applications on hybrid cloud infrastructures}, volume={29}, ISSN={0167-739X}, DOI={10.1016/j.future.2013.01.004}, number={7}, journal={Future Generation Computer Systems}, publisher={Elsevier BV}, author={Malawski, Maciej and Figiela, Kamil and Nabrzyski, Jarek}, year={2013}, month={Sep}, pages={1786–1794}}

@article{mitchell_2017,
author = {Mitchell, Alex L and Scheremetjew, Maxim and Denise, Hubert and Potter, Simon and Tarkowska, Aleksandra and Qureshi, Matloob and Salazar, Gustavo A and Pesseat, Sebastien and Boland, Miguel A and Hunter, Fiona M I and ten Hoopen, Petra and Alako, Blaise and Amid, Clara and Wilkinson, Darren J and Curtis, Thomas P and Cochrane, Guy and Finn, Robert D},
title = {EBI Metagenomics in 2017: enriching the analysis of microbial communities, from sequence reads to assemblies},
journal = {Nucleic Acids Research},
volume = {46},
number = {D1},
pages = {D726-D735},
year = {2018},
doi = {10.1093/nar/gkx967},
}

@article{Kaur_2012, title={An Efficient Approach to Genetic Algorithm for Task Scheduling in Cloud Computing Environment}, volume={4}, ISSN={2074-9015}, DOI={10.5815/ijitcs.2012.10.09}, number={10}, journal={International Journal of Information Technology and Computer Science}, publisher={MECS Publisher}, author={Kaur, Shaminder and Verma, Amandeep}, year={2012}, month={Sep}, pages={74–79}}

@article{samblaster,
  doi = {10.1093/bioinformatics/btu314},
  url = {https://doi.org/10.1093/bioinformatics/btu314},
  year  = {2014},
  month = {may},
  publisher = {Oxford University Press ({OUP})},
  volume = {30},
  number = {17},
  pages = {2503--2505},
  author = {G. G. Faust and I. M. Hall},
  title = {{SAMBLASTER}: fast duplicate marking and structural variant read extraction},
  journal = {Bioinformatics}
}

@online{Mozilla,
    title = {Mozilla Science},
    url = {https://science.mozilla.org/},
    note = {Accessed 01 Dec 2018},
    urldate = {2018-12-01}
}

@online{DigitalOcean,
    title = {DigitalOcean - Cloud Computing, Simplicity at Scale},
    url = {https://www.digitalocean.com/},
    note = {Accessed 01 Dec 2018},
    urldate = {2018-12-01}
}

@online{AmazonEC,
    title = {Amazon EC2},
    url = {https://aws.amazon.com/ec2/},
    note = {Accessed 01 Dec 2018},
    urldate = {2018-12-01}
}

@online{GoogleCl,
    title = {Google Cloud including GCP \& G Suite
    url = {https://cloud.google.com/},
    note = {Accessed 01 Dec 2018},
    urldate = {2018-12-01}
}}

@online{Microsof,
    title = {Microsoft Azure Cloud Computing Platform \& Services},
    url = {https://azure.microsoft.com/en-us/},
    note = {Accessed 01 Dec 2018},
    urldate = {2018-12-01}
}

@misc{somatic_mendeley,
  doi = {10.17632/97hj93mkfd.3},
  author = {Khan, Farah Zaib and Soiland-Reyes,  Stian},
  title = {CWL run of Somatic Variant Calling Workflow (CWLProv 0.5.0 Research Object)},
  note = {v3},
  publisher = {Mendeley Data},
  year = {2018}
}

@misc{alignment_mendeley,
  doi = {10.17632/6wtpgr3kbj.1},
  author = {Khan, Farah Zaib and Soiland-Reyes, Stian},
  title = {CWL run of Alignment Workflow (CWLProv 0.6.0 Research Object)},
  note = {v1},
  publisher = {Mendeley Data},
  year = {2018}
}

@misc{rnaseq_mendeley,
  doi = {10.17632/xnwncxpw42.1},
  author = {Khan, Farah Zaib and Soiland-Reyes, Stian},
  title = {CWL run of RNA-seq Analysis Workflow (CWLProv 0.5.0 Research Object))},
  note = {v1},
  publisher = {Mendeley Data},
  year = {2018}
}

@article{Gaignard2014,
  doi = {10.1016/j.websem.2014.07.001},
  url = {https://doi.org/10.1016/j.websem.2014.07.001},
  year = {2014},
  month = dec,
  publisher = {Elsevier {BV}},
  volume = {29},
  pages = {19--30},
  author = {Alban Gaignard and Johan Montagnat and Bernard Gibaud and Germain Forestier and Tristan Glatard},
  title = {Domain-specific summarization of Life-Science e-experiments from provenance traces},
  journal = {Journal of Web Semantics}
}

@inproceedings {PoeM,
  title={From Scientific Workflow Patterns to 5-star Linked Open Data.},
  author={Gaignard, Alban and Skaf-Molli, Hala and Bihou{\'e}e, Audrey},
  booktitle={TaPP},
  year={2016}
}

@article{10.1093/gigascience/giz052,
    author = {Ohta, Tazro and Tanjo, Tomoya and Ogasawara, Osamu},
    title = "{Accumulating computational resource usage of genomic data analysis workflow to optimize cloud computing instance selection}",
    journal = {GigaScience},
    volume = {8},
    number = {4},
    year = {2019},
    month = {04},
    abstract = "{Container virtualization technologies such as Docker are popular in the bioinformatics domain because they improve the portability and reproducibility of software deployment. Along with software packaged in containers, the standardized workflow descriptors Common Workflow Language (CWL) enable data to be easily analyzed on multiple computing environments. These technologies accelerate the use of on-demand cloud computing platforms, which can be scaled according to the quantity of data. However, to optimize the time and budgetary restraints of cloud usage, users must select a suitable instance type that corresponds to the resource requirements of their workflows.We developed CWL-metrics, a utility tool for cwltool (the reference implementation of CWL), to collect runtime metrics of Docker containers and workflow metadata to analyze workflow resource requirements. To demonstrate the use of this tool, we analyzed 7 transcriptome quantification workflows on 6 instance types. The results revealed that choice of instance type can deliver lower financial costs and faster execution times using the required amount of computational resources.CWL-metrics can generate a summary of resource requirements for workflow executions, which can help users to optimize their use of cloud computing by selecting appropriate instances. The runtime metrics data generated by CWL-metrics can also help users to share workflows between different workflow management frameworks.}",
    issn = {2047-217X},
    doi = {10.1093/gigascience/giz052}
}


@article{Clark2014,
  doi = {10.1186/2041-1480-5-28},
  url = {https://doi.org/10.1186/2041-1480-5-28},
  year = {2014},
  publisher = {Springer Nature},
  volume = {5},
  number = {1},
  pages = {28},
  author = {Tim Clark and Paolo N Ciccarese and Carole A Goble},
  title = {Micropublications: a semantic model for claims,  evidence,  arguments and annotations in biomedical communications},
  journal = {Journal of Biomedical Semantics}
}


@online{5star,
    author = {James G. Kim and Michael Hausenblas et al.},
    title = {5 Star Open Data},
    url = {https://5stardata.info/},
    urldate = {2019-05-21},
    note = {Accessed 21 May 2019}  
}

@misc{GigaScienceData,
    author= {Khan, Farah Zaib and Soiland-Reyes, Stian and Sinnott, Richard O and Lonie,, Andrew and Goble Carole and Crusoe, Michael R},
    title = {Supporting data for ``Sharing interoperable workflow provenance: A review of best practices and their practical application in CWLProv''}, 
    publisher = {GigaScience Database},
    doi = {10.5524/100625},
    year = {2019}
    }
